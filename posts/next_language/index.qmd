---
title: "The Data Science Language of the Future"
author: "Ryan Heslin"
date: "2022-09-08"
categories: ["R"]
urlcolor: "blue"
---

R, for all its warts, has most of the features I want from a data science 
language. It's powerful, and typically fun to use. But, like all languages, it is neither perfect nor likely to be widely used forever. (I doubt it will enjoy \em if that is the right word \em the endless afterlife of COBOL and its ilk). So I hope the (distant!) future will see statistical languages that replicate and refine R's strengths while improving its weaknesses. What should those languages look 
like?

To discuss a next-generation language, we need to establish what makes R so great to begin with. 
On reflection, I identified three key ingredients:

1. _Vector types and vectorized functions_. As John Chambers says, if it exists in R, it's a vector. 
R doesn't have any true scalar types; there are only vectors of varying lengths. 
The rationale is obvious: converting between scalar and vector types would add 
complexity for little gain,
make analysis and data tidying a pain. Anyone who's ever spent an hour deriving the ordinary 
least squares estimators by elementary algebra and calculus, and then done it in a few lines 
with linear algebra, will know what I mean. 

But vectorization has benefits beyond mathematical convenience. (For now, let's use 
Hadley Wickham's working definition of a vectorized function: $f(x[[i]]) = f(x)[[i]]$).
It abstracts away the iteration involved in operations, freeing you to think of functions 
as acting on each element independently. This results in compact, readable code:

```{r}
paste0(letters, " is letter #", seq_along(letters), " of the alphabet")
```

In base Python or most other languages, this would require a `for` loop that 
kept track of letters and indices, resulting in less readable code and a greater 
likelihood of mistakes. Better still, R features convenience functions like `colMeans` that operate at a higher 
level of abstraction: data frames or arrays, which are versatile generalizations 
of simple atomic vectors. These capabilities let you ignore implementation 
details of iteration and write nicely abstract code.

Vectorization is hardly unique to R, but I don't know of another language  
as fundamentally vector-oriented. Our ideal successor language should emulate 
R in this area.

2. _Expressive data manipulation_ 

Too often, the actual "science" of data science, like dessert after a big feast, 
is dwarfed by what came before:
data tidying, missing value imputation, transformation, and  
everything else required to get messy input into a form that can be analyzed. If 
a data pipeline doesn't exist, this can become far more daunting than the analysis 
itself. No language is better suited for the job than R. A skilled 
user can achieve even elaborate transformations in ten or twenty lines. 
With practice, the feeling of power becomes almost addictive. Using another 
language feels like putting on heavy gloves before tying your shoelaces.

R's expressive, powerful data manipulation interface grants it this power. It also makes R hard to learn. You can often find five or six obvious,
correct ways to do even a simple task, like obtaining the fourth element of the `mtcars`
columns `cyl`.

```{r}
mtcars$cyl[[4]]
mtcars[[c(2, 4)]]
mtcars[4, "cyl"]
mtcars[["cyl"]][[4]]
mtcars[rownames(mtcars)[[4]], "cyl"]
```

A successor to R might develop a smaller set of operators, and smooth 
out some oddities (like `drop = FALSE`). 
But it should not go too far in this
Emphasizing readability and separating tasks 
into different functions, 
as `dplyr` has 
done, would make code more readable and easier to debug, but also more verbose. 
Too radical a departure from R's approach would fail to replicate what 
makes it special.

3. _Metaprogramming_ 

The other two areas I identify are widely cited as strengths of R. This one, though, is esoteric. 
While almost all R users take advantage of the features that power metaprogramming, many without knowing it, few use them extensively. It's easy (and sometimes advisable) even for experienced users to avoid invoking it directly. Still, it distinguishes R from most other languages, and rests on bold design decisions made long before the language's inception.

"Metaprogramming", as used in the R community, means writing programs that treat R code as data - programming on programs, in other words. It utilizes R's highly developed capabilities for partial expression substitution, controlled evaluation, and environment manipulation. Books could be written about this topic, and [Advanced R](https://adv-r.hadley.nz/meta-big-picture.html) covers it in detail.

As a basic example, have you ever wondered why most calls to `library` in R scripts look like `library(package)`, not `library("package")`? The latter is legal, but seldom used. Most functions will throw an error if passed the name of a nonexistent object: 

```{r, error = TRUE }
c("a", "b", "c", d)
```

But certain functions capture their inputs directly, _without_ evaluating them, and then evaluate them in a different context. This is called "quoting", since it captures the syntax of code 
while ignoring the semantics the way quoting natural language does. The implementation, known as non-standard evaluation, powers much 
of R's interface. One prominent example is formulas: a compact mini-language for 
specifying a statistical relationship to modeling functions. 
Because the formula is quoted and evaluated in the context of a data frame, 
the user can provide bare variable names, making for a clean, simple interface: 

```{r}
lm(mpg ~ wt + cyl * disp, data = mtcars)
```

The `tidyverse` takes this idea much further. Its functions rely on 
[tidy evaluation](https://dplyr.tidyverse.org/articles/programming.html), an elaborate framework 
for selecting and modifying variables within "data masks." 
In the end, R is really a statistics-oriented descendant of Lisp with more conventional syntax. Many of these ideas - expressions as data, expression substitution, and even optional prefix syntax - come from that immortal language.

```{r}
`+`(2, 2)
```

All this power comes with serious drawbacks - serious enough that it can be 
reasonably argued that non-standard evaluation is a bad paradigm. 
Manipulating expressions means code loses referential transparency (evaluating 
the same if variable names are changed). Controlled evaluation requires 
programmers to think about environment inheritance, creating the potential for a host 
of subtle bugs. Functions that quote some of their arguments but not all, or 
accept quoted and nonquoted forms of the same argument (like `library`), are 
harder to use. In the end, all this indirection makes code harder to 
write and reason about (hence the need for a vignette on simply programming with `dplyr`). 
I think the tradeoff is worthwhile; the convenience and flexibility of non-standard evaluation 
are too valuable to abandon. But unlike the other two characteristics I outlined 
above, a strong case can be made otherwise.

In short, a successor to R should contain R's most powerful features: vector types 
and vectorized functions, a terse but expressive subsetting syntax, and 
support for expression manipulation and controlled evaluation.

# Improving on R's Weaknesses

R is not without faults. The problems listed below are more annoying than 
serious, but they stem from design decisions made long ago that can no longer 
be easily reversed. A successor language should avoid those mistakes. 

## Finicky Interface

R's user interface, in places, in harder to learn and use than necessary. 
It uses conventions inconsistently, exposes too much detail to the user, 
and contains too many "gotchas" that cause confusing errors you can 
only avoid with experience.

One of the unwritten rules of programming 
is that inconsistency should not exist without reason. If you write a class `Foo` with methods called 
`bar_bar`, `baz_baz`, and `quxQux`, your users will wonder why you used camelCase for just one method every time they try to call the logically expected but nonexistent  `qux_qux`. If you 
put a data frame argument at the head of one function's argument list but the 
tail of another's, they will wonder why every time they forget which is which. Only
constant attention in design can avoid inconsistencies like these, but the best designs do so.

R violates the principle in many places. 
One trivial but well-known example is the way S3 methods are written `generic.class` (e.g., `mean.default`), yet dots are used all 
the time in the names of functions, including 
S3 generics. The many exceptions (`t.test`, `all.vars`, ...) thwart a potentially useful convention.
Unlike the other functionals, `mapply` has the function as the first argument, not the second, and the `simplify` and `use.names` arguments are  
actually `SIMPLIFY` and `USE.NAMES` (not without reason, but good luck remembering).
`ave` and `tapply` do similar things, but 
`ave` uses `...` for grouping factors, while `tapply` reserves it for arguments to the `FUN` argument. 
Once you notice one of these seams in the design, you can't unsee it.

R sometimes contains unnecessary complexity. 
Interfaces often have complicated semantics, and functions sometimes feature multiple operating modes.
For instance, there are two slightly different functions 
for doing principal components analysis, differing in the algorithm used. 
The function `diag` has four distinct uses 
(five, if you count `diag<-` as part of the same interface). Most troubling to me are 
the heavily overloaded arguments of certain 
functions. Consider this passage from the help for `get`: 

```
The ‘pos’ argument can specify the environment in which to look
     for the object in any of several ways: as a positive integer (the
     position in the ‘search’ list); as the character string name of an
     element in the search list; or as an ‘environment’ (including
     using ‘sys.frame’ to access the currently active function calls).
     The default of ‘-1’ indicates the current environment of the call
     to ‘get’. The ‘envir’ argument is an alternative way to specify an
     environment.
```
I count three possible types for `pos`, all with different meanings, a default value with a special meaning, and another argument that does exactly the same thing for one type. (Plus a suggestion to use call stack introspection, which I'll leave to braver programmers than me). 

Trying to memorize the intricacies of an interface like this is a fool's errand: at some point, you'll get it wrong and cause a nasty bug. That leaves no recourse but referring to 
the documentation each time you use the function, and nothing makes an interface more annoying to use.

Another offender is factors. Factors represent categorical variables by mapping integer codes to levels. Simple idea, but so many potential errors come from this fact. Something as simple as naively concatenating a factor causes disaster: 

```{r}
x <- factor(c("a", "b", "c"))
y <- factor(c("x", "y", "z"))
c(x, "d")
```

Attempting to do factor arithmetic only triggers a warning, despite being nonsense
(Note also that the factor warning preempts the "mismatched object lengths" warning this would normally trigger):
```{r}
x + 3:6
```

Worst of all, and not widely known: R's lexical sort order differs by system locale. (See [here](https://stackoverflow.com/questions/31772185/language-dependent-sorting-with-r) for an example). When creating a factor, R defaults to ordering the levels lexically. Good luck with that reproducible research! 

Individually, these criticisms are trivial. 
I don't mean to cast them as evidence of incompetence or carelessness by the language designers. I have written much worse interfaces to far simpler programs, so I know from experience how hard it is to implement and maintain a good one. But our successor language can do better by following the `tidyverse` and making "design for human users" a 
core principle. 

## Very Weak Typing

Our new language should have dynamic typing. Static typing makes code easier to reason about and debug, especially in large applications, but it would be awkward to explore or transform data without quick, easy type conversions that can be done interactively. 
In its present form, I think R makes these conversions a little _too_ easy. R is a weakly typed language: instead of disallowing operations with objects of disparate types, it casts them to a common type. Sometimes the result is predictable: 

```{r}
c(TRUE, "abc")
TRUE + 3
```

But sometimes R will allow operations that have no sensible result: 

```{r}
paste0(mtcars, "abc")
```

Moreover, R has no equivalent of Python's type hinting system. If you want to enforce 
a specific type for function arguments, you have to do it manually: 


```{r, eval = FALSE}
foo <- function(x, y, z)
if(!is.character(x)){
        stop("x must be character")
    }
}
```

Many of the type-checking helpers like `is.character` have surprisingly 
complex behaviors that make them dangerous to rely on.

R functions also do not always have stable return types. `sapply`, for example, can return a list, an array, an atomic vector, or even an empty list, depending on the input. Programming guides often recommend `lapply` or `vapply` instead, since they enforce stable return types, but many unwary users (including me, at various times) who did not know this have written subtly buggy code. 

R's very weak typing accounts for much of the 
unpredictable behavior that makes it challenging to use in large applications. I think strict typing like Python's would be excessive; operations like `paste(1:10, letters[1:10])` are too convenient to part with. But our successor language will dispense with some of the crazier implicit coercions R allows.

## String Manipulation

R's string manipulation facilities leave something to be 
desired. In other languages, strings are array types or feature array-like 
subsetting. R, however, handles strings (i.e., the raw character data that make up the elements of character vectors) with an [internal type](https://cran.r-project.org/doc/manuals/r-release/R-ints.html#SEXPTYPEs). You can't extract string elements the way you can in Python: 

```{py}
x = "A typical string"
x[0]
```

You have to use `substr` or `substring` (barely distinguishable functions again!)

```{r}
x <- "A typical string"
substr(x, 1, 1)
```

The rationale is obvious - the unpalatable alternative would be to implement character 
vectors as list-like recursive vectors - but it has annoying consequences for the 
interface, such as `strsplit` returning a list:

```{r}
x <- c("This is a typical", "character vector")
strsplit(x, split = "\\s")
```

But these are quibbles. The real problem is the regular expression interface. 
This is the only part of base R I actively dislike.
There are too many functions with terse, barely distinguishable names. (If you can remember the difference between `gregexpr` and `regexec` without looking it up, please teach me your secrets). Functions don't use [PCRE](https://en.wikipedia.org/wiki/Perl_Compatible_Regular_Expressions) by default, a fact I never remember until it causes an error. They return match data 
in awkward formats; `gregexpr`, for instance, returns a list of match start positions 
and lengths, making it difficult to extract the actual match data. 

Put together, these issues make working with regular expressions much more verbose and painful than necessary. The convoluted snippet below, copied from the documentation, does nothing more than create a matrix with the text from two capture groups. 
For comparison, Python's `re` module contains a `groupdict` method that stores matches in an appropriate data structure automatically.

```{r}
notables <- c("  Ben Franklin and Jefferson Davis",
              "\tMillard Fillmore")
# name groups 'first' and 'last'
name.rex <- "(?<first>[[:upper:]][[:lower:]]+) (?<last>[[:upper:]][[:lower:]]+)"
(parsed <- regexpr(name.rex, notables, perl = TRUE))
gregexpr(name.rex, notables, perl = TRUE)[[2]]
parse.one <- function(res, result) {
  m <- do.call(rbind, lapply(seq_along(res), function(i) {
    if(result[i] == -1) return("")
    st <- attr(result, "capture.start")[i, ]
    substring(res[i], st, st + attr(result, "capture.length")[i, ] - 1)
  }))
  colnames(m) <- attr(result, "capture.names")
  m
}
parse.one(notables, parsed)
```

```{py}
import re 

notables <- ["Ben Franklin and Jefferson Davis",  "\tMillard Fillmore"]
[re.match("(?P<first>[[:upper:]][[:lower:]]+) (?P<last>[[:upper:]][[:lower:]]+)", x).groupdict() for x in notables]
```

(Is Fillmore's inclusion a sly joke? He is chiefly notable for being a bottom-tier president).

The excellent `stringr` package provides functions that fix all of these problems. But R users shouldn't have to choose between taking a major dependency and foregoing easy string processing.

# Summing Up

You should have a clear idea by now of the language I want. It relies on vector types and makes it easy to manipulate data. It uses some form of non-standard evaluation and 
offers powerful metaprogramming tools to interested users. Its interface judiciously hides complexity and contains few discrepancies and special cases. With an easy-to-use package system and thorough documentation, it will rapidly gain users and establish a productive, long-lasting community.

That language sounds a lot like what the people behind `tidyverse` have 
already created. `tidyverse` expands and enhances R's data manipulation 
capabilities, with particular attention to ease of use and rigorous 
implementation of non-standard evaluation. Perhaps most importantly,
its developers update aggressively; they have made several complete 
overhauls of `dplyr`'s interface over the past few years. This 
means lots of breaking changes that make `tidyverse` infamously dangerous to use 
in production, but `tidyverse` advances and develops new ideas much 
more quickly than R itself. I think the tradeoff is worthwhile.

It also sounds a little like [Julia](https://julialang.org/), a newer statistical language with metaprogramming support, vector types, and an emphasis on performance that is lacking in R.That emphasis, [some have observed](https://www.reddit.com/r/datascience/comments/m8wcnl/julia_vs_rpython/), gives it the potential to eliminate the "prototype in R/Python, program in C/C++" cycle that 
plagues machine learning research today. It has nowhere near R's 
popularity or anything like its mature ecosystem, but users I've encountered speak highly of it. Will I be writing Julia ten years from now? Perhaps. But for now, R reigns supreme.

\# TODO: Update this in `r 2027 - as.integer(format(Sys.Date(), "%Y"))` years to see how things shook out

