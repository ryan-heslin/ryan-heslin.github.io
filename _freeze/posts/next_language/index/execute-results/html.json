{
  "hash": "df81f76786c6e8cbd3bad41b5585e3ac",
  "result": {
    "markdown": "---\ntitle: \"The Data Science Language of the Future\"\nauthor: \"Ryan Heslin\"\ndate: \"2022-09-08\"\ncategories: [\"R\"]\nurlcolor: \"blue\"\n---\n\n\nR, for all its warts, has most of the features I want from a data science \nlanguage. It's powerful, surprisingly versatile, and usually fun to use. But, like all languages, it is neither perfect nor likely to be widely used forever. (I doubt it will enjoy - if that is the right word - the endless afterlife of COBOL and its ilk). So I hope the (distant!) future will see statistical languages that replicate and refine R's strengths while improving its weaknesses. What should those languages look \nlike?\n\nTo discuss a next-generation language, we need to establish what makes R so great to begin with. \nOn reflection, I identified three key ingredients:\n\n1. _Vector types and vectorized functions_. As John Chambers says, if it exists in R, it's a vector. \nR doesn't have any true scalar types; there are only vectors of varying lengths. \nThe rationale is obvious: converting between scalar and vector types would add \ncomplexity for little gain,\nmake analysis and data tidying a pain. Anyone who's ever spent an hour deriving the ordinary \nleast squares estimators by elementary algebra and calculus, and then done it in a few lines \nwith linear algebra, will know what I mean. \n\nBut vectorization has benefits beyond mathematical convenience. (For now, let's use \nHadley Wickham's working definition of a vectorized function: $f(x[[i]]) = f(x)[[i]]$).\nIt abstracts away the iteration involved in operations, freeing you to think of functions \nas acting on each element independently. This results in compact, readable code:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\npaste0(letters, \" is letter #\", seq_along(letters), \" of the alphabet\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n--  [1] \"a is letter #1 of the alphabet\" \n--  [2] \"b is letter #2 of the alphabet\" \n--  [3] \"c is letter #3 of the alphabet\" \n--  [4] \"d is letter #4 of the alphabet\" \n--  [5] \"e is letter #5 of the alphabet\" \n--  [6] \"f is letter #6 of the alphabet\" \n--  [7] \"g is letter #7 of the alphabet\" \n--  [8] \"h is letter #8 of the alphabet\" \n--  [9] \"i is letter #9 of the alphabet\" \n-- [10] \"j is letter #10 of the alphabet\"\n-- [11] \"k is letter #11 of the alphabet\"\n-- [12] \"l is letter #12 of the alphabet\"\n-- [13] \"m is letter #13 of the alphabet\"\n-- [14] \"n is letter #14 of the alphabet\"\n-- [15] \"o is letter #15 of the alphabet\"\n-- [16] \"p is letter #16 of the alphabet\"\n-- [17] \"q is letter #17 of the alphabet\"\n-- [18] \"r is letter #18 of the alphabet\"\n-- [19] \"s is letter #19 of the alphabet\"\n-- [20] \"t is letter #20 of the alphabet\"\n-- [21] \"u is letter #21 of the alphabet\"\n-- [22] \"v is letter #22 of the alphabet\"\n-- [23] \"w is letter #23 of the alphabet\"\n-- [24] \"x is letter #24 of the alphabet\"\n-- [25] \"y is letter #25 of the alphabet\"\n-- [26] \"z is letter #26 of the alphabet\"\n```\n:::\n:::\n\n\nIn base Python or most other languages, this would require a `for` loop that \nkept track of letters and indices, resulting in less readable code and a greater \nlikelihood of mistakes. Better still, R features convenience functions like `colMeans` that operate at a higher \nlevel of abstraction: data frames or arrays, which are versatile generalizations \nof simple atomic vectors. These capabilities let you ignore implementation \ndetails of iteration and write nicely abstract code.\n\nVectorization is hardly unique to R, but I don't know of another language  \nas fundamentally vector-oriented. Our ideal successor language should emulate \nR in this area.\n\n2. _Expressive data manipulation_ \n\nToo often, the actual \"science\" of data science, like dessert after a big feast, \nis dwarfed by what came before:\ndata tidying, missing value imputation, transformation, and  \neverything else required to get messy input into a form that can be analyzed. If \na data pipeline doesn't exist, this can become far more daunting than the analysis \nitself. No language is better suited for the job than R. A skilled \nuser can achieve even elaborate transformations in ten or twenty lines. \nWith practice, the feeling of power becomes almost addictive. Using another \nlanguage feels like putting on heavy gloves before tying your shoelaces.\n\nR's expressive, powerful data manipulation interface grants it this power. It also makes R hard to learn. You can often find five or six obvious,\ncorrect ways to do even a simple task, like obtaining the fourth element of the `mtcars`\ncolumns `cyl`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nmtcars$cyl[[4]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] 6\n```\n:::\n\n```{.r .source .cell-code}\nmtcars[[c(2, 4)]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] 6\n```\n:::\n\n```{.r .source .cell-code}\nmtcars[4, \"cyl\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] 6\n```\n:::\n\n```{.r .source .cell-code}\nmtcars[[\"cyl\"]][[4]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] 6\n```\n:::\n\n```{.r .source .cell-code}\nmtcars[rownames(mtcars)[[4]], \"cyl\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] 6\n```\n:::\n:::\n\n\nA successor to R might develop a smaller set of operators, and smooth \nout some oddities (like `drop = FALSE`). \nBut it should not go too far in this\nEmphasizing readability and separating tasks \ninto different functions, \nas `dplyr` has \ndone, would make code more readable and easier to debug, but also more verbose. \nToo radical a departure from R's approach would fail to replicate what \nmakes it special.\n\n3. _Metaprogramming_ \n\nThe other two areas I identify are widely cited as strengths of R. This one, though, is esoteric. \nWhile almost all R users take advantage of the features that power metaprogramming, many without knowing it, few use them extensively. It's easy (and sometimes advisable) even for experienced users to avoid invoking it directly. Still, it distinguishes R from most other languages, and rests on bold design decisions made long before the language's inception.\n\n\"Metaprogramming\", as used in the R community, means writing programs that treat R code as data - programming on programs, in other words. It utilizes R's highly developed capabilities for partial expression substitution, controlled evaluation, and environment manipulation. Books could be written about this topic, and [Advanced R](https://adv-r.hadley.nz/meta-big-picture.html) covers it in detail.\n\nAs a basic example, have you ever wondered why most calls to `library` in R scripts look like `library(package)`, not `library(\"package\")`? The latter is legal, but seldom used. Most functions will throw an error if passed the name of a nonexistent object: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nc(\"a\", \"b\", \"c\", d)\n```\n\n::: {.cell-output .cell-output-error}\n```\n-- Error in eval(expr, envir, enclos): object 'd' not found\n```\n:::\n:::\n\n\nBut certain functions capture their inputs directly, _without_ evaluating them, and then evaluate them in a different context. This is called \"quoting\", since it captures the syntax of code \nwhile ignoring the semantics the way quoting natural language does. The implementation, known as non-standard evaluation, powers much \nof R's interface. One prominent example is formulas: a compact mini-language for \nspecifying a statistical relationship to modeling functions. \nBecause the formula is quoted and evaluated in the context of a data frame, \nthe user can provide bare variable names, making for a clean, simple interface: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nlm(mpg ~ wt + cyl * disp, data = mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- \n-- Call:\n-- lm(formula = mpg ~ wt + cyl * disp, data = mtcars)\n-- \n-- Coefficients:\n-- (Intercept)           wt          cyl  \n--    49.55195     -2.73695     -3.00543  \n--        disp     cyl:disp  \n--    -0.08670      0.01107\n```\n:::\n:::\n\n\nThe `tidyverse` takes this idea much further. Its functions rely on \n[tidy evaluation](https://dplyr.tidyverse.org/articles/programming.html), an elaborate framework \nfor selecting and modifying variables within \"data masks.\" \nIn the end, R is really a statistics-oriented descendant of Lisp with more conventional syntax. Many of these ideas - expressions as data, expression substitution, and even optional prefix syntax - come from that immortal language.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\n`+`(2, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] 4\n```\n:::\n:::\n\n\nAll this power comes with serious drawbacks - serious enough that it can be \nreasonably argued that non-standard evaluation is a bad paradigm. \nManipulating expressions means code loses referential transparency (evaluating \nthe same if variable names are changed). Controlled evaluation requires \nprogrammers to think about environment inheritance, creating the potential for a host \nof subtle bugs. Functions that quote some of their arguments but not all, or \naccept quoted and nonquoted forms of the same argument (like `library`), are \nharder to use. In the end, all this indirection makes code harder to \nwrite and reason about (hence the need for a vignette on simply programming with `dplyr`). \nI think the tradeoff is worthwhile; the convenience and flexibility of non-standard evaluation \nare too valuable to abandon. But unlike the other two characteristics I outlined \nabove, a strong case can be made otherwise.\n\nIn short, a successor to R should contain R's most powerful features: vector types \nand vectorized functions, a terse but expressive subsetting syntax, and \nsupport for expression manipulation and controlled evaluation.\n\n# Improving on R's Weaknesses\n\nR is not without faults. The problems listed below are more annoying than \nserious, but they stem from design decisions made long ago that can no longer \nbe easily reversed. A successor language should avoid those mistakes. \n\n## Finicky Interface\n\nR's user interface, in places, in harder to learn and use than necessary. \nIt uses conventions inconsistently, exposes too much detail to the user, \nand contains too many \"gotchas\" that cause confusing errors you can \nonly avoid with experience.\n\nOne of the unwritten rules of programming \nis that inconsistency should not exist without reason. If you write a class `Foo` with methods called \n`bar_bar`, `baz_baz`, and `quxQux`, your users will wonder why you used camelCase for just one method every time they try to call the logically expected but nonexistent  `qux_qux`. If you \nput a data frame argument at the head of one function's argument list but the \ntail of another's, they will wonder why every time they forget which is which. Only\nconstant attention in design can avoid inconsistencies like these, but the best designs do so.\n\nR violates the principle in many places. \nOne trivial but well-known example is the way S3 methods are written `generic.class` (e.g., `mean.default`), yet dots are used all \nthe time in the names of functions, including \nS3 generics. The many exceptions (`t.test`, `all.vars`, ...) thwart a potentially useful convention.\nUnlike the other functionals, `mapply` has the function as the first argument, not the second, and the `simplify` and `use.names` arguments are  \nactually `SIMPLIFY` and `USE.NAMES` (not without reason, but good luck remembering).\n`ave` and `tapply` do similar things, but \n`ave` uses `...` for grouping factors, while `tapply` reserves it for arguments to the `FUN` argument. \nOnce you notice one of these seams in the design, you can't unsee it.\n\nR sometimes contains unnecessary complexity. \nInterfaces often have complicated semantics, and functions sometimes feature multiple operating modes.\nFor instance, there are two slightly different functions \nfor doing principal components analysis, differing in the algorithm used. \nThe function `diag` has four distinct uses \n(five, if you count `diag<-` as part of the same interface). Most troubling to me are \nthe heavily overloaded arguments of certain \nfunctions. Consider this passage from the help for `get`: \n\n```\nThe ‘pos’ argument can specify the environment in which to look\n     for the object in any of several ways: as a positive integer (the\n     position in the ‘search’ list); as the character string name of an\n     element in the search list; or as an ‘environment’ (including\n     using ‘sys.frame’ to access the currently active function calls).\n     The default of ‘-1’ indicates the current environment of the call\n     to ‘get’. The ‘envir’ argument is an alternative way to specify an\n     environment.\n```\nI count three possible types for `pos`, all with different meanings, a default value with a special meaning, and another argument that does exactly the same thing for one type. (Plus a suggestion to use call stack introspection, which I'll leave to braver programmers than me). \n\nTrying to memorize the intricacies of an interface like this is a fool's errand: at some point, you'll get it wrong and cause a nasty bug. That leaves no recourse but referring to \nthe documentation each time you use the function, and nothing makes an interface more annoying to use.\n\nAnother offender is factors. Factors represent categorical variables by mapping integer codes to levels. Simple idea, but so many potential errors come from this fact. Something as simple as naively concatenating a factor causes disaster: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nx <- factor(c(\"a\", \"b\", \"c\"))\ny <- factor(c(\"x\", \"y\", \"z\"))\nc(x, \"d\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] \"1\" \"2\" \"3\" \"d\"\n```\n:::\n:::\n\n\nAttempting to do factor arithmetic only triggers a warning, despite being nonsense\n(Note also that the factor warning preempts the \"mismatched object lengths\" warning this would normally trigger):\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nx + 3:6\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] NA NA NA NA\n```\n:::\n:::\n\n\nWorst of all, and not widely known: R's lexical sort order differs by system locale. (See [here](https://stackoverflow.com/questions/31772185/language-dependent-sorting-with-r) for an example). When creating a factor, R defaults to ordering the levels lexically. Good luck with that reproducible research! \n\nIndividually, these criticisms are trivial. \nI don't mean to cast them as evidence of incompetence or carelessness by the language designers. I have written much worse interfaces to far simpler programs, so I know from experience how hard it is to implement and maintain a good one. But our successor language can do better by following the `tidyverse` and making \"design for human users\" a \ncore principle. \n\n## Very Weak Typing\n\nOur new language should have dynamic typing. Static typing makes code easier to reason about and debug, especially in large applications, but it would be awkward to explore or transform data without quick, easy type conversions that can be done interactively. \nIn its present form, I think R makes these conversions a little _too_ easy. R is a weakly typed language: instead of disallowing operations with objects of disparate types, it casts them to a common type. Sometimes the result is predictable: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nc(TRUE, \"abc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] \"TRUE\" \"abc\"\n```\n:::\n\n```{.r .source .cell-code}\nTRUE + 3\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] 4\n```\n:::\n:::\n\n\nBut sometimes R will allow operations that have no sensible result: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\npaste0(mtcars, \"abc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n--  [1] \"c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26, 30.4, 15.8, 19.7, 15, 21.4)abc\"                    \n--  [2] \"c(6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4)abc\"                                                                                                            \n--  [3] \"c(160, 160, 108, 258, 360, 225, 360, 146.7, 140.8, 167.6, 167.6, 275.8, 275.8, 275.8, 472, 460, 440, 78.7, 75.7, 71.1, 120.1, 318, 304, 350, 400, 79, 120.3, 95.1, 351, 145, 301, 121)abc\"                       \n--  [4] \"c(110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 205, 215, 230, 66, 52, 65, 97, 150, 150, 245, 175, 66, 91, 113, 264, 175, 335, 109)abc\"                                                     \n--  [5] \"c(3.9, 3.9, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92, 3.07, 3.07, 3.07, 2.93, 3, 3.23, 4.08, 4.93, 4.22, 3.7, 2.76, 3.15, 3.73, 3.08, 4.08, 4.43, 3.77, 4.22, 3.62, 3.54, 4.11)abc\"                  \n--  [6] \"c(2.62, 2.875, 2.32, 3.215, 3.44, 3.46, 3.57, 3.19, 3.15, 3.44, 3.44, 4.07, 3.73, 3.78, 5.25, 5.424, 5.345, 2.2, 1.615, 1.835, 2.465, 3.52, 3.435, 3.84, 3.845, 1.935, 2.14, 1.513, 3.17, 2.77, 3.57, 2.78)abc\"  \n--  [7] \"c(16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20, 22.9, 18.3, 18.9, 17.4, 17.6, 18, 17.98, 17.82, 17.42, 19.47, 18.52, 19.9, 20.01, 16.87, 17.3, 15.41, 17.05, 18.9, 16.7, 16.9, 14.5, 15.5, 14.6, 18.6)abc\"\n--  [8] \"c(0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1)abc\"                                                                                                            \n--  [9] \"c(1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1)abc\"                                                                                                            \n-- [10] \"c(4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 5, 4)abc\"                                                                                                            \n-- [11] \"c(4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2, 2, 4, 2, 1, 2, 2, 4, 6, 8, 2)abc\"\n```\n:::\n:::\n\n\nMoreover, R has no equivalent of Python's type hinting system. If you want to enforce \na specific type for function arguments, you have to do it manually: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nfoo <- function(x, y, z) {\n  if (!is.character(x)) {\n    stop(\"x must be character\")\n  }\n}\n```\n:::\n\n\nMany of the type-checking helpers like `is.character` have surprisingly \ncomplex behaviors that make them dangerous to rely on.\n\nR functions also do not always have stable return types. `sapply`, for example, can return a list, an array, an atomic vector, or even an empty list, depending on the input. Programming guides often recommend `lapply` or `vapply` instead, since they enforce stable return types, but many unwary users (including me, at various times) who did not know this have written subtly buggy code. \n\nR's very weak typing accounts for much of the \nunpredictable behavior that makes it challenging to use in large applications. I think strict typing like Python's would be excessive; operations like `paste(1:10, letters[1:10])` are too convenient to part with. But our successor language will dispense with some of the crazier implicit coercions R allows.\n\n## String Manipulation\n\nR's string manipulation facilities leave something to be \ndesired. In other languages, strings are array types or feature array-like \nsubsetting. R, however, handles strings (i.e., the raw character data that make up the elements of character vectors) with an [internal type](https://cran.r-project.org/doc/manuals/r-release/R-ints.html#SEXPTYPEs). You can't extract string elements the way you can in Python: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .source .cell-code}\nx = \"A typical string\"\nx[0]\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- 'A'\n```\n:::\n:::\n\n\nYou have to use `substr` or `substring` (barely distinguishable functions again!)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nx <- \"A typical string\"\nsubstr(x, 1, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] \"A\"\n```\n:::\n:::\n\n\nThe rationale is obvious - the unpalatable alternative would be to implement character \nvectors as list-like recursive vectors - but it has annoying consequences for the \ninterface, such as `strsplit` returning a list:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nx <- c(\"This is a typical\", \"character vector\")\nstrsplit(x, split = \"\\\\s\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [[1]]\n-- [1] \"This\"    \"is\"      \"a\"       \"typical\"\n-- \n-- [[2]]\n-- [1] \"character\" \"vector\"\n```\n:::\n:::\n\n\nBut these are quibbles. The real problem is the regular expression interface. \nThis is the only part of base R I actively dislike.\nThere are too many functions with terse, barely distinguishable names. (If you can remember the difference between `gregexpr` and `regexec` without looking it up, please teach me your secrets). Functions don't use [PCRE](https://en.wikipedia.org/wiki/Perl_Compatible_Regular_Expressions) by default, a fact I never remember until it causes an error. They return match data \nin awkward formats; `gregexpr`, for instance, returns a list of match start positions \nand lengths, making it difficult to extract the actual match data. \n\nPut together, these issues make working with regular expressions much more verbose and painful than necessary. The convoluted snippet below, copied from the documentation, does nothing more than create a matrix with the text from two capture groups. \nFor comparison, Python's `re` module contains a `groupdict` method that stores matches in an appropriate data structure automatically.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nnotables <- c(\n  \"  Ben Franklin and Jefferson Davis\",\n  \"\\tMillard Fillmore\"\n)\n# name groups 'first' and 'last'\nname.rex <- \"(?<first>[[:upper:]][[:lower:]]+) (?<last>[[:upper:]][[:lower:]]+)\"\n(parsed <- regexpr(name.rex, notables, perl = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] 3 2\n-- attr(,\"match.length\")\n-- [1] 12 16\n-- attr(,\"index.type\")\n-- [1] \"chars\"\n-- attr(,\"useBytes\")\n-- [1] TRUE\n-- attr(,\"capture.start\")\n--      first last\n-- [1,]     3    7\n-- [2,]     2   10\n-- attr(,\"capture.length\")\n--      first last\n-- [1,]     3    8\n-- [2,]     7    8\n-- attr(,\"capture.names\")\n-- [1] \"first\" \"last\"\n```\n:::\n\n```{.r .source .cell-code}\ngregexpr(name.rex, notables, perl = TRUE)[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [1] 2\n-- attr(,\"match.length\")\n-- [1] 16\n-- attr(,\"index.type\")\n-- [1] \"chars\"\n-- attr(,\"useBytes\")\n-- [1] TRUE\n-- attr(,\"capture.start\")\n--      first last\n-- [1,]     2   10\n-- attr(,\"capture.length\")\n--      first last\n-- [1,]     7    8\n-- attr(,\"capture.names\")\n-- [1] \"first\" \"last\"\n```\n:::\n\n```{.r .source .cell-code}\nparse.one <- function(res, result) {\n  m <- do.call(rbind, lapply(seq_along(res), function(i) {\n    if (result[i] == -1) {\n      return(\"\")\n    }\n    st <- attr(result, \"capture.start\")[i, ]\n    substring(res[i], st, st + attr(result, \"capture.length\")[i, ] - 1)\n  }))\n  colnames(m) <- attr(result, \"capture.names\")\n  m\n}\nparse.one(notables, parsed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n--      first     last      \n-- [1,] \"Ben\"     \"Franklin\"\n-- [2,] \"Millard\" \"Fillmore\"\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .source .cell-code}\nimport re \n\nnotables = [\"Ben Franklin and Jefferson Davis\",  \"\\tMillard Fillmore\"]\n[re.match(\".*(?P<first>[A-Z][a-z]+).*(?P<last>[A-Z][a-z]+)\", x).groupdict() for x in notables]\n```\n\n::: {.cell-output .cell-output-stdout}\n```{.output}\n-- [{'first': 'Jefferson', 'last': 'Davis'}, {'first': 'Millard', 'last': 'Fillmore'}]\n```\n:::\n:::\n\n\n(Is Fillmore's inclusion a sly joke? He is chiefly notable for being a bottom-tier president).\n\nThe excellent `stringr` package provides functions that fix all of these problems. But R users shouldn't have to choose between taking a major dependency and foregoing easy string processing.\n\n# Summing Up\n\nYou should have a clear idea by now of the language I want. It relies on vector types and makes it easy to manipulate data. It uses some form of non-standard evaluation and \noffers powerful metaprogramming tools to interested users. Its interface judiciously hides complexity and contains few discrepancies and special cases. With an easy-to-use package system and thorough documentation, it will rapidly gain users and establish a productive, long-lasting community.\n\nThat language sounds a lot like what the people behind `tidyverse` have \nalready created. `tidyverse` expands and enhances R's data manipulation \ncapabilities, with particular attention to ease of use and rigorous \nimplementation of non-standard evaluation. Perhaps most importantly,\nits developers update aggressively; they have made several complete \noverhauls of `dplyr`'s interface over the past few years. This \nmeans lots of breaking changes that make `tidyverse` infamously dangerous to use \nin production, but `tidyverse` advances and develops new ideas much \nmore quickly than R itself. I think the tradeoff is worthwhile.\n\nIt also sounds a little like [Julia](https://julialang.org/), a newer statistical language with metaprogramming support, vector types, and an emphasis on performance that is lacking in R.That emphasis, [some have observed](https://www.reddit.com/r/datascience/comments/m8wcnl/julia_vs_rpython/), gives it the potential to eliminate the \"prototype in R/Python, program in C/C++\" cycle that \nplagues machine learning research today. It has nowhere near R's \npopularity or anything like its mature ecosystem, but users I've encountered speak highly of it. Will I be writing Julia ten years from now? Perhaps. But for now, R reigns supreme.\n\n\\# TODO: Update this in 5 years to see how things shook out\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}