{
  "hash": "91faf03d804cb9f6449c9f74b2ce7be7",
  "result": {
    "markdown": "---\ntitle: \"Why Linear Algebra is Useful\"\nauthor: \"Ryan Heslin\"\ndate: \"2023-09-11\"\ncategories: [\"Miscellaneous\", \"Math\"]\nurlcolor: \"blue\"\n---\n\n\nLinear algebra has a bad reputation. The subject has a tough \ncombination of fiddly computation and abstract reasoning,\none that isn't always taught well.\nSTEM majors often have bitter memories \nof spending hours on [row reduction](https://en.wikipedia.org/wiki/Gaussian_elimination) and [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition). \nBut sometimes linear algebra can make your life much, much easier. One such \nsituation is expressing the formula for computing linear regression coefficients.\n\n# What does Linear Regression Actually Do? \n\nFormally, [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares) \nregression takes a data matrix $X$ and a response vector $Y$, and \nfinds a vector of coefficients $\\beta$ that minimize the equation $\\sum_{i=1}^n (Y_i - \\hat Y)^2$, \nwhere $\\hat Y = X \\beta$. This is called the sum of squared errors ($SSE$). It can also be viewed as the sum of the squares of \nthe Euclidean distance between each response value $Y_i$ and the corresponding fitted value $\\hat Y_i$. \n(Why squared distance instead of absolute value? See below). The individual errors are called residuals.\n\nSimple linear regression is the special case where there is \nonly one variable, $X_1$, and hence two coefficients: the constant term $\\beta_0$ and \n$\\beta_1$, the coefficient for $X_1$. ($\\beta_0$ isn't strictly necessary, but it's rarely \na good idea to omit it). In simple regression, $\\hat Y_i = \\beta_0 + \\beta_1 X_{i1}$ \n\nThis is equivalent to plotting the data and finding the line that minimizes the sum of \nsquared errors. $\\beta_0$ provides the intercept of the line.\n\nHere is a model with the residuals shown:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nlibrary(ggplot2)\nset.seed(1)\n\nX <- rnorm(100, 10, 5)\nY <- X * 7 + 4 + rnorm(100, 20, 4)\nmodel <- lm(Y ~ X)\nggplot(mapping = aes(x = X, y = Y)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\", linewidth = 0.7, se = FALSE) +\n  geom_point(alpha = .3) +\n  geom_segment(aes(x = X, y = model$fitted.values, xend = X, yend = Y), color = \"grey\", linewidth = 0.5) +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n-- `geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nNow we have to know how to find these coefficients $\\beta$. There the trouble starts.\n\n\n# Elementary Algebra\n\nStatistics textbooks usually introduce the equations for the estimators \nin simple linear regression using these formulae (or algebraically equivalent expressions):\n$$\n\\displaylines{\\beta_0 = \\bar Y -\\beta_1 \\bar X \\\\\n\\beta_1 = \\frac{\\sum_{i=1}^n(X_i - \\bar X)(Y_i - \\bar Y)}{\\sum_{i = 1}^n (X_i - \\bar X)^2}}\n$$\n\nProfessors often throw these complicated formulae at unsuspecting students \nin the last weeks of intro stats courses. I first encountered them that way, and I remember the shock of fear I felt. Nobody with mathematical maturity is scared of a \nsummation symbol, but back then I had no clue what I was doing, and many \nother students are in the same position.\n\nThere's also the classic assignment of _deriving_ these formulae by minimizing the \nnormal equation $SSE = \\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1 x_i)^2$. You have to take first derivatives, solve the equations for each estimator simultaneously, \ndo some algebra tricks, and finish off with a second-derivative test to make sure \nthe estimators really do minimize the loss function. It's one of those tough \nbut brutally satisfying assignments, like computing a median with standard SQL \nor implementing iterative least squares to estimate a generalized linear model.\n(I appreciate the professor who suggested doing it as an exercise; it made me \na lot less mathematically naive!).\n\n# Linear Algebra\n\nIn linear algebra terms, linear regression [orthogonally projects](https://textbooks.math.gatech.edu/ila/projections.html) the response vector $Y$ into the linear subspace formed by the data matrix $X$.\n(Linear algebra texts usually call a generic matrix $A$, but statistics \ntexts use $X$ for a data matrix, so I will do so here).\n In other words, it finds the closest \nvector to $Y$ by Euclidean distance that can be formed by a [linear combination](https://en.wikipedia.org/wiki/Linear_combination) of $X$. \nIn matrix algebra, the normal equation is $SSE = (Y-\\beta X)^T(Y- \\beta_ X)$. \n(Multiplying a row vector by its transpose just sums the squares of each element).\nSome basic calculus and rearrangement turns this into $X^TX \\beta = X^Ty$.\nBy inverting $X^TX$ (a matrix operation roughly analogous to turning a number $n$ into $1/n$), we get a simple formula for $\\beta$: \n\n$$ \n\\beta = (X^TX)^{-1}X^Ty\n$$\n\nThis is a variation on the formula for a projection matrix, $P = X(X^TX)^{-1}X^T$. \nFor any vector $y$, $Py$ is the closest vector to $y$ by Euclidean distance that is a \nlinear combination of the vectors in $X$.\nThis is much more revealing than the elementary-algebra version of the \nformula. For one thing, it's a lot simpler. It's fully general, too: it holds for all coefficients\nfor a data matrix of any size, not just simple linear regression. (What about \n$\\beta_0$, the constant term? Just add a column of ones to your data matrix, and you have it!).\nAnd it implies the important fact that no unique least-squares solution exists \nif $(X^TX)$ fails to be invertible, which happens if the [rank](https://en.wikipedia.org/wiki/Rank_(linear_algebra)) of $X$ is less \nthan $p$, the number of variables. In that case, there are infinitely many \nleast-squares solutions.\n\n# Conclusion\n\nAs this example shows, matrix notation is more abstract and concise than scalar notation. It makes \nthe underlying concepts of an expression like this far easier to see and understand. \nIt's the difference between implementing a function in Python and implementing it in C, except without the loss in efficiency.\n\nThis is just one example of how linear algebra makes it easy to express \nideas that would be hard to convey with elementary algebra.\nThose weeks of row-reducing matrices really were worth it.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}