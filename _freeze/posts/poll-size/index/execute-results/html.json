{
  "hash": "14dee986fd878c63c0b2f80cad6caec0",
  "result": {
    "markdown": "---\ntitle: \"How Do Polls Even Work?\"\nauthor: \"Ryan Heslin\"\ndate: \"2024-04-24\"\ncategories: [\"math\"]\n---\n\n\nSome mathematical claims seem to defy intuition. How is it that every system of linear equations \nhas exactly zero, one, or infinitely many solutions? How can it be that `0.999... = 1`? \nHow can it be that $P(x = 0) = 0$ for all continuous distributions?\nBut few defy common sense as much as poll sample sizes. Presidential election polls routinely \nclaim to estimate the fraction of support for each candidate with only hundreds or \na few thousand respondents, out of an electorate of more than a hundred million. How can \nan accurate result be obtained from such a tiny fraction of the population? As it turns out, \nthe math all but guarantees it \\em on certain conditions.\n\n# The Central Limit Theorem\n\nThe justification comes from the ever-popular [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).\nFormally, the CLT states that a sum of independent and identically distributed \nvariables with finite mean and variance is a normally distributed random variable. Let $X_i$ \nbe the $i$th such random variable, $\\mu$ the common mean of the $X_i$, and $\\sigma^2$ their \ncommon variance. Let $Z_n = \\frac{\\sum_{i=1}^n X_i - n \\mu}{\\sigma \\sqrt n}$, or the centered and \nstandardized sum of $n$ samples of $X$.  Then for any two real numbers $a$ and $b$ that satisfy a < Z_n < b:\n\n$$\n\\lim_{n \\to \\infty} P(a < Z_n < b) = \\Phi(b) - \\Phi(a)\n$$\n\nwhere $\\Phi$ is the cumulative density function of the standard normal distribution.\n\nIn other words, $Z_n$, representing the sample's deviation from the true sum, follows a \nnormal distribution, even if the $X_i$ don't! This means we can precisely state the uncertainty \nof estimates, since we can compute the probability of an observation of $Z_n$ falling within some interval. Furthermore, if we divide the three numbers in the definition of $Z_n$ by $n$, \neverything stated above applies to the sample mean just as to the sample sum.\n\nThis follows from the fact that the variance of a sum of independent random variables is equal to \nthe sum of their variances, and the sum equal to the sum of their means (since $n \\overline X = \\sum_{i=1}^n X_i$). Since we assume the variables to be identically distributed with \ncommon variance $\\sigma^2$, $\\text{Var}(\\sum_{i=1}^nX_i) = \\sum_{i=1}^n\\sigma_2 = n \\sigma^2$.\nSo the true mean and variance can be reliably estimated from the sample.\n\n(See [here](https://www.probabilitycourse.com/chapter7/7_1_2_central_limit_theorem.php) and [this video](https://www.youtube.com/watch?v=zeJD6dqJ5lo) for more information on CLT theory).\n\n# Application to Public Opinion Polls\n\nIn a survey measuring support for candidates in a two-candidate election, each respondent ($X_i$) can be modeled as a Bernoulli variable with probability of support $p$ for one candidate and $1-p$ for the other.  (If we were modeling voters' choice among more than two candidates, we would \nhave to use the [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution) instead. I'll spare you this, since it's annoying to work with, and third-party and independent candidates are usually not important in American presidential elections). Plugging in the Bernoulli expected value and variance, we have this expression for the standardized population proportion:\n\n$$\nZ_n = \\frac{\\sum_{i=1}^n X_i - n p}{\\sqrt {np(1-p)}}\n$$\n\nThe term $1 / \\sqrt n$ is often called the \"standard error multiplier\", since it determines the \nsize of the standard error of the sample mean. The inverse square root function shrinks quickly enough that even reasonably small samples cut down most of the standard error:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nFor example, suppose  $p = 0.5$, so the standard deviation is $\\sqrt{0.5(1 -0.5)} = 0.25$ (the highest Bernoulli variance possible).\nA poll of 100 respondents would have a standard error of 0.05,\nwhile one of 1000 would have a standard error of 0.0158114. \nSince a  95% confidence interval covers about 1.96 standard errors, this reduces the margin of error to a few percentage points, enough to make good predictions even in tight elections.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nsuppressPackageStartupMessages(library(ggplot2))\n\nggplot() +\n  geom_function(fun = \\(x) 1 / sqrt(x)) +\n  scale_y_continuous(labels = scales::label_percent(), limits = c(0, 1)) +\n  xlim(c(0, 1000)) +\n  labs(x = \"n\", y = \"SE Multiplier\", title = \"SE Multiplier over Sample Sizes\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nHere are simulations of this scenario, using larger and larger samples. The plots\nshow how the sampling distribution approaches normality for sufficiently large $n$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .source .cell-code}\nsuppressPackageStartupMessages(library(dplyr, quietly = TRUE))\nset.seed(1)\n\nn <- 500\np <- 0.5\nEX <- 0.5 * n\nsizes <- c(5, 10, 30, 50, 500, 1000)\ntrials <- rep(sizes, each = n)\ndata <- data.frame(draw = rbinom(n * length(sizes), trials, p), size = trials)\ndata$z <- (data$draw - (p * data$size)) / sqrt(data$size * p * (1 - p))\n\ngroup_by(data, size, z) |>\n  summarize(count = n(), .groups = \"drop\") |>\n  ggplot(aes(x = z, y = count)) +\n  geom_col() +\n  facet_wrap(~size, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n# Polling in Practice\n\nFrom the Central Limit Theorem, we know that the distribution of $\\overline X$ is $\\mathcal N(\\mu, \\sigma / \\sqrt n)$. Using our estimate $\\overline X$ of the population proportion $\\mu$ and $\\hat{\\sigma}$ of the population standard deviation $\\sigma$, we can compute quantiles for this normal distribution. For a typical 95% two-sided confidence interval, we would calculate the quantiles for the interval endpoints 0.025 and 0.975. In frequentist inference, this does not mean the true proportion $mu$ has a 95% probability of falling within the interval, but that an interval constructed this way would be expected to contain the true parameter on 95% of samples. \n\nSo the above is certainly true provided these conditions are fulfilled and the sample is random. But are they \nfulfilled in real-world polls? The answer is, \"it depends.\" Pollsters can't use simple random samples, as we assumed in our discussion above. Since different demographics (by race, gender, class, region, etc.) vote in different ways, polls try to make their samples represent the \nnational population as well as possible. They often use complicated demographic weights to correct biases in samples, but this isn't an exact science. Nonresponse error is another problem; very few people actually agree to \ntalk to pollsters cold-calling them, and if some demographic segments do so less often than \nothers, the results are biased. All of these factors contribute to total survey error: the overall deviation of a survey result from the true value from all causes.\n\nIn short, the simple CLT margin of error is smaller than the true margin. It may not even account for most of the true uncertainty.\nAs [one review](https://5harad.com/papers/polling-errors.pdf) of these problems (from which the above is drawn) noted (p. 608):\n\n```\nIn contrast to errors due to sample variance, it is difficult—\nand perhaps impossible—to build a useful and general statistical\ntheory for the remaining components of total survey error.\nMoreover, even empirically measuring total survey error can\nbe difficult, as it involves comparing the results of repeated\nsurveys to a ground truth obtained, for example, via a census.\nFor these reasons, it is not surprising that many survey organi-\nzations continue to use estimates of error based on theoretical\nsampling variation, simply acknowledging the limitations of\nthe approach. Indeed, Gallup (2007) explicitly states that their\nmethodology assumes “other sources of error, such as nonre-\nsponse, by some members of the targeted sample are equal,” and\nfurther notes that “other errors that can affect survey validity\ninclude measurement error associated with the questionnaire,\nsuch as translation issues and coverage error, where a part or\nparts of the target population ... have a zero probability of being\nselected for the survey.”\n```\n\nIn the 2020 election, national polls performed terribly. The average error was 4.5 percentage \npoints, about as big as Joe Biden's margin of victory in the popular vote. \n[Investigation failed to uncover why](https://www.politico.com/news/2021/07/18/pollsters-2020-polls-all-wrong-500050), despite much speculation about the COVID-19 pandemic, higher nonresponse rates among Trump voters, and other possible causes. We will see whether pollsters will do better in 2024.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}