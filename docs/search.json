[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Why the Name?\nNaming things really is as hard as cache invalidation. I spent a long time casting about for something to call the site before hitting on Verso. It means the reverse side of a physical document, such as the left-hand page of a book. The word attracted me because I aim to make this blog like a good anthology: you can visit any page and find something short but worth reading. As a publishing term derived from Italian, it has a pleasingly archaic resonance similar to “quarto”, the namesake of the file format this site’s documents are written in. Most important, it’s got an R.\nVerso was built with Quarto and deployed using Github Pages with a GitHub workflow developed by pommevilla. A previous version of this site was built with rendered Rmarkdown and the blogdown R package."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Verso",
    "section": "",
    "text": "Advent of Code\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvent of Code\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/aoc_advice/index.html",
    "href": "posts/aoc_advice/index.html",
    "title": "What I Wish I’d Known Going into Advent of Code",
    "section": "",
    "text": "With a hundred-odd stars across three years, I thought I’d offer some advice.\nTo have an enjoyable experience, I don’t think you need anything more than familiarity with at least one programming language. You certainly don’t need formal computer science education, knowledge of math theory, or any experience with competitive programming, though all those things are helpful. You just need to be effective at taking a problem statement, coming up with a plan to solve it, and translating that plan into code. None of that is easy, but Advent of Code is a great way to practice.\nHere are some less obvious pointers.\nBe absolutely sure you understand what is being asked. Puzzles often\ncontain complicated rules with subtle but important edge cases. The text is usually careful to note these; if not, they often appear in the examples provided. Still, Wastl has remarked that there is always at least one player who ignores each sentence of the puzzle text. More times than I can count, I’ve wasted time on unworkable solutions after missing some crucial nuance. Don’t be like me. Don’t start solving a puzzle until you’ve resolved all your questions about how you should interpret the input, and remember that an extra minute spent skimming the puzzle text could save you an hour of fruitless coding.\nResist the temptation to pre-solve the second part.\nI’d better make this code robust to handle the second part, you tell yourself. I’ll look so clever when I solve it just by tweaking a parameter in my solution to the first part! Stop right there. The whole point of the second part is that you can’t plan for it. It could take any one of dozens of forms - asking for the highest of something instead of the lowest, removing a constraint that made the problem simpler, imposing a new constraint that makes it more complicated, or even asking you to use the input in a completely different way. Beyond wrapping your code in functions or classes you can reuse, you can’t prepare for all these scenarios.\nThe exception is puzzles whose first parts tell you to ignore some part of the input. In that case, the second part is sure to use the full input, and the increase in scale could well break a naive but inefficient algorithm that works on the first part.\nLook out for some recurring themes 175 puzzles in, Wastl still manages to avoid merely recycling old puzzles. But naturally, the same kinds of problem crop up again and again, albeit always with a fresh variation or two. Expect to see at least one appearance each of the following:\n\nEvaluating and parsing a made-up assembly language\nFinding the shortest path on a graph\nString manipulation, possibly involving regular expressions\nSome application of combinatorics\nA variation on Conway’s game of life\n\nPuzzles become a lot less daunting when you can relate them to familiar concepts. Moreover, these are all well-studied problems in computer science, so you can easily find advice or pseudocode for useful algorithms if you get stuck.\nBe lazy. Efficient code does as little work as possible. Advent of Code doesn’t expect you to aggressively optimize your code, but it does reward finding clever ways to avoid unnecessary computations. If you’re trying combinations of values to look for the answer, for instance, you can greatly speed up your code if you find an easy way to filter combinations that can’t possibly yield the right answer. In a similar vein, resist the urge to just simulate a puzzle that asks for the outcome of a game or process. If you can think of a way to compute the answer directly, your code will probably run much faster.\nAdvent of Code isn’t intended to require dirty optimization hacks, but it does test your ability to think of subtler approaches than brute force.\nDon’t make it into work. Advent of Code is, above all, fun - a chance to write code that solves silly problems about made-up Yuletide adventures. Of course, it’s also an opportunity to practice writing good code to diverse specifications, but that’s secondary. Don’t stress about efficiency, style, or refactoring; nobody is going to hound you for breaking code quality guidelines on Christmas puzzles.\nAnd if you get stuck, do something else for awhile - you’ll get that star eventually. It’s not a sign you’re dumb or bad at coding; the puzzles are devised by a clever software engineer to challenge an audience largely composed of software engineers, after all. (That doesn’t mean I’m not dumb, mind you - you just have to look elsewhere for proof).\nIf you’re unsure whether to take part, just give it a try. I did, and the experience spurred me to learn new languages and start thinking seriously about the code I wrote. The first puzzle of each year is usually simple, and you’re not obliged to continue if you don’t enjoy it. I hope you found this advice useful, or at least entertaining. See you in December!"
  },
  {
    "objectID": "posts/a_star/index.html",
    "href": "posts/a_star/index.html",
    "title": "Chasing A*: Completing Advent of Code 2021, Once and For All",
    "section": "",
    "text": "In an earlier post, I related my long but successful effort to obtain every last star in Advent of Code’s 2021 edition. It ended in surprising anticlimax when I solved day 23, a puzzle that looked daunting to tackle with code, using paper, pen, and some cut-up sticky notes. I could have stopped there. I should have stopped there. But good stories don’t end with an anticlimax, and the feeling that I had somehow cheated nagged at me. With plenty of free time as I hunted for a job, I decided to solve the puzzle the right way.\n\nThe Problem\nDay 23’s puzzle is a variant of the classic Towers of Hanoi. Instead of disks, the puzzle has players move different types of amphipod, in keeping with the year’s ocean theme. More importantly, the goal is to move each amphipod of each type into the correct “side room” connected to the main board, or “hall”, as efficiently as possible. The second part of the puzzle doubles the number of amphipods, making it much tougher to solve by hand.\nA post on the subreddit suggested using the A* (“A star”) algorithm. A*https://en.wikipedia.org/wiki/A*_search_algorithm) is a classic pathfinding algorithm that finds the shortest path between two given nodes of a graph. Implementations use a function called d to measure distances between nodes, and a heuristic function called h to estimate the distance between a node and the goal. The algorithm is mathematically certain to return the correct path if h never overestimates the distance to the goal.\nIn the problem at hand, the nodes were clearly game states (legal configurations of the board). Two nodes shared a connection if one could be reached from the other by a legal move (luckily, no legal moves are reversible, so the graph is acyclic — no loops are possible). The puzzle required the only the minimal cost of completing the game, not the actual sequence of moves, which further simplified things.\nStill, I faced several hard tasks:\n\nCreate a data structure capable of representing any valid game state\nImplement d (to measure distances between nodes) and h (to conservatively estimate any node’s distance from the goal).\nWrite an A_star function that used these routines to find the minimal cost\n\n\n\nInto the Fray\nAs is usual with Advent of Code, the first task was parsing the input. This was my raw input:\n#############\n#...........#\n###A#D#A#B###\n  #B#C#D#C#\n  #########\nI made the crucial decision to represent positions on the board as tuples of (x, y) coordinates. Since I was using Python, I decided to use a zero-based index, with the leftmost hall space as the x origin and the bottom side room spaces as the y origin. So the leftmost A amphipod on the board above would be located at (2, 2). I would have made my life much easier if I had used complex numbers instead of tuples of real numbers. In any case, I wrote a crude function to map each amphipod type to a set containing its positions:\ndef parse(inp, xmax=10):\n    stripped = inp[2 : (len(inp) - 1)]\n    ymax = len(stripped)\n    stripped.reverse()\n    stripped = list(zip(*stripped))\n    stripped = stripped[1 : (xmax + 1)]\n    mapping = {i: set() for i in range(4)}\n    for i in range(min(ends) + 2, max(ends) - 1, 2):\n        this = stripped[i]\n        for j in range(ymax):\n            val = values_map[this[j]]\n            mapping[val].add((i, j))  # Add position to set\n\n    return mapping, ymax\nThen I wrote a complicated function I’ll spare you. It computed, for each pair of coordinates it was legal to move between, the spaces spanning them. That way, I could allow moves only after confirming that that space wasn’t blocked.\nNext came designing an object to represent game states. It should own h and d, the second of which would take another game state as argument. I decided it should also be responsible for finding adjacent nodes and creating objects to represent them. I decided to call the class State.\nFrom here, my work only got kludgier. State ended up mapping each amphipod type to a set of the positions it occupied as well as tracking the occupants of each side room - a redundancy I couldn’t seem to avoid. From there, d and h were surprisingly simple. d would only ever be called on states that differed by the position of just one amphipod, so all I had to do was find the two coordinate tuples that disagreed in the instances’ coordinate sets, measure the distance between them, and multiply by the cost of moving the relevant amphipod type one space. h was a bit trickier, but hardly brutal — I just computed the distance from each amphipod to the target side room, a simple approach that would never underestimate the true cost.\nThe real bear turned out to be finding the valid neighbors of each instance. It took me an embarrassingly long time to figure out the rules:\n\nAmphipods in side rooms may only move out if they are in the side room of the wrong type, or if an amphipod of the wrong type is positioned behind them.\nAmphipods in side rooms that meet one or both criteria can move to any hall space to which the path is clear, or the innermost open space of their side room if it complies with rule 4.\nAmphipods in hall rooms may only move into the side room of their type, and only if a path to it is clear.\nA side room may only admit amphipods if it either contains no amphipods or only amphipods of its type.\n\nTranslating these directives into conditions was pure hell, and the result turned into pure write-only code. Here’s a representative excerpt:\nif coord[0] in self.sides_idx:\n    x_type = self.side_idx2type(coord[0])\n    if (x_type == k and self.sides[k][\"completed\"]) or (\n            coord[1] < self.ymax - 1\n            and self.sides[x_type][\"room\"][coord[1] + 1] is not None\n            ):\n        continue\nSomehow, I finished it.\nThat left only the A_star function that did the real work. Translating Wikipedia’s pseudocode for the algorithm into Python was simple:\ndef A_star(start, goal, debug=False):\n    start_k = hash(start)\n    open_set = {start_k: start}\n\n    # For node k, node preceding it on cheapest known path to k\n    came_from = {}\n\n    # g_score[k] is cost of cheapest known path to k\n    g_score = defaultdict(lambda: inf)\n    g_score[start_k] = 0\n    # gscore[k] + k.h() - best estimate of total cost (default to infinity if node unknown)\n    f_score = defaultdict(lambda: inf)\n    f_score[start_k] = g_score[start_k] + start.h()\n\n    while open_set:\n        min_cost = inf\n        # h = hash\n        for h, node in open_set.items():\n            score = f_score[h]\n            this_cost = min(min_cost, score)\n            if this_cost < min_cost and h in open_set.keys():\n                current = node\n                # print(current)\n                # print(\"\\n\")\n                if current == goal:\n                    return g_score[hash(current)]  # Cheapest cost to goal\n                min_cost = this_cost\n        current_k = hash(current)\n        current.find_neighbors()\n        if debug:\n            print(hash(current))\n            print(current)\n            print(\"-------------------\\n\")\n            for n in current.neighbors:\n                print(n)\n                print(current.d(n))\n            input(\"Continue: \")\n            print(\"\\n\\n\\n\")\n        open_set.pop(current_k)\n        for neighbor in current.neighbors:\n            # print(neighbor.neighbors)\n\n            # Distance from start to neighbor through current\n            g_score_new = g_score[current_k] + current.d(neighbor)\n            # print(f\"distance: {current.d(neighbor)}\")\n            # print(neighbor)\n            neighbor_k = hash(neighbor)\n            # This path to neighbor cheaper than any known, so record it\n            if g_score_new < g_score[neighbor_k]:\n                came_from[neighbor_k] = current\n                # New estimate of cost from this neighbor\n                # Forgot this line\n                g_score[neighbor_k] = g_score_new\n                f_score[neighbor_k] = g_score_new + neighbor.h()\n                if neighbor not in open_set.values():\n                    open_set[neighbor_k] = neighbo\nMy only addition, naturally, was a debug mode. Then came the really hard part.\nI spent an embarrassing amount of time in the debugger getting everything to work correctly. I fell into in a dispiriting loop of scanning output for evidence of bugs, stepping through the debugger to track them down, and making painstaking changes to fix them. I came close to giving up, and several times regretted starting. Then, one fine July Monday morning, I saw the code spit out a plausible-looking answer. Not expecting success, I checked the Advent of Code website and gasped when I saw it was correct.\nI wasn’t home free; my inefficient kludge algorithm might well be too slow for the second half of the problem. I modified State to handle a larger game board, crossed my fingers, and ran the script again. It took a few minutes longer, but it spit out the correct answer for part 2. I had done it.\nI savored the feeling of blissful triumph, knowing it would not last. I might have just finished the worst implementation of A* of all time, but it was my implementation, and it solved the problem. Somehow, writing your own intricate kludge is far more satisfying then copying someone else’s elegant solution. In any case, I was at last done: I had finished all 25 puzzles for Advent of Code 2021 by myself. Perhaps an achievement in pointlessness, but an achievement nonetheless."
  },
  {
    "objectID": "posts/ghost-machine/index.html",
    "href": "posts/ghost-machine/index.html",
    "title": "Ghost in the Machine: The Remnant of R’s Past That Haunts it Still",
    "section": "",
    "text": "But as many users know, its roots go back further. R was developed from the language S, created in the 1970s by a team led by John Chambers at Bell Labs. Those were the glory days of Bell Labs, when the language C and the Unix ecosystem were developed. Like a modern palace built on the foundations of an ancient one, R bears many traces of its lineage. Syntax is very similar, many features are backward-compatible, and the documentation for some functions even refers to resources about S rather than R. (Try ?sum, for one example).\n(I can’t help but pause here to relay the account the linked presentation gives of R’s origins. It all began with this hallway conversation between Ross Ihaka and Robert Gentleman in the University of Auckland around 1990):\n\nGentleman: “Let’s write some software.”\nIhaka: “Sure, that sounds like fun.”\n\nOne of those traces, harder to observe but certainly still present, is also one of R’s most unusual (and, in some quarters, derided) features: an emphasis on convenience in interactive use. Interpreted languages typically support interactivity in some way, since the ability to run a snippet of code and instantly get results is one of their greatest advantages over compiled languages. But S was designed primarily for interactive data exploration, and R has retained that capability as a design focus. In areas great and small, from core design choices to implementation quirks, R makes it as easy as possible to bang out code in the console and see what happens. That makes it a fast, flexible tool for exploring data and following hunches. It also strews mines in the path of anyone programming in the language without detailed knowledge of the its nuances.\nA few examples will make this painfully clear.\nPartial Matching, Complete Headache\nCan you spot the problem with this call? It runs correctly:\n\nrep(1:3, length = 10)\n\n--  [1] 1 2 3 1 2 3 1 2 3 1\n\n\nbut is missing something. The relevant argument of rep is actually called length.out, not length, but R’s partial argument matching saves us, since length is a shortened form of length.out.\nThis is nice to have when typing code in the console. But relying on partial argument matching in scripts is a very bad idea.\nSuppose you’re working with a package that includes some functions with annoyingly long argument names. All that typing is annoying, so you decide you may as well save some keystrokes:\n\nfoo <- function(xyzabc = 0, abcxyz) {\n  rnorm(100, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n--   [1]  0.21568960  1.45999971  1.10359291\n--   [4] -1.30200555  3.17223491  1.33562253\n--   [7]  0.03823917  0.63919159 -3.66705727\n--  [10]  2.69911257  2.05211283 -2.51452262\n--  [13] -1.28136883  0.11150523  4.04446974\n--  [16]  2.98729988 -1.32272973  1.81112430\n--  [19] -0.94945545 -0.05797119  0.03027148\n--  [22] -0.61693030  1.81194856 -0.47316450\n--  [25] -2.03233060  0.36819023 -0.11433634\n--  [28]  0.69745316 -1.59641666 -0.83445377\n--  [31] -1.02733296  1.14439119  3.68908599\n--  [34] -4.29425306  0.64393747 -4.48878943\n--  [37]  0.84198003  0.44400857  0.43679650\n--  [40] -2.74895529  3.53326045 -3.62858402\n--  [43]  2.28226110 -1.31338956 -4.14889309\n--  [46]  1.05945706 -0.03381360  0.96985393\n--  [49]  0.01405702 -2.90439983 -0.65116498\n--  [52]  1.17507853  2.30892954 -1.68866895\n--  [55]  0.72246549  0.58221661 -3.59496585\n--  [58] -1.49411781 -1.67787986 -1.16178334\n--  [61] -1.80764244  1.95260504 -1.49236004\n--  [64]  1.33631588  0.35961812  3.12517797\n--  [67] -2.49155656 -2.41632541 -3.45484426\n--  [70] -1.52901846 -0.90600316  3.11506403\n--  [73]  0.20781185 -0.74303546 -1.44349124\n--  [76]  0.69732603  3.24685854 -0.94081635\n--  [79] -1.87546853 -2.55349705 -1.95924729\n--  [82]  1.78223595 -2.00921559 -0.49048957\n--  [85] -2.30453566 -1.96201745 -2.12737504\n--  [88]  2.36019294  1.43670555 -0.42101426\n--  [91]  0.46770122  0.91731790  1.14452255\n--  [94] -1.68664269  1.10448660  0.93125117\n--  [97]  2.00032002  0.34457605 -2.16782404\n-- [100] -0.79980040\n\n\nAll seems well. But then a version update adds a new argument:\n\nfoo <- function(abcabc = 100, xyzabc = 0, abcxyz) {\n  rnorm(abcabc, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n-- Error in foo(abc = 2): argument 1 matches multiple formal arguments\n\n\nR throws an error, unable to find an unambiguous match. (Imagine how painful this would be to debug if R defaulted to the first match instead). The way to avoid this scenario is simple: never rely on partial argument matching in permanent code. Nonetheless, many packages do. You can identify offenders yourself by setting the warnPartialMatchArgs option:\n\noptions(warnPartialMatchArgs = TRUE)\nfoo <- function(xyzabc = 0, abcxyz) {\n  rnorm(100, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n-- Warning in foo(abc = 2): partial argument match of\n-- 'abc' to 'abcxyz'\n\n\n--   [1]  0.2226411936 -0.4505538309 -0.7222419695\n--   [4] -3.1537025240  1.0027621056 -2.5086106867\n--   [7] -2.5322042951  1.3284660444  1.3385534121\n--  [10]  1.2556490887  0.9255714499  1.4189508080\n--  [13]  3.8015238993  1.6156805094 -0.8075590330\n--  [16] -2.6727684274 -0.9935527917  1.0832304299\n--  [19]  0.6790134368  1.3365384748  1.3316617317\n--  [22]  1.6992115703 -1.5136440670 -1.1893631672\n--  [25] -0.1311025723  1.3879805236  0.4979383431\n--  [28] -5.1691204874  1.6351727892  2.5427857284\n--  [31]  4.6111965802  0.3418243604 -0.6111471272\n--  [34]  1.0065838563  2.5935085678 -1.7341657819\n--  [37] -2.0334440673 -0.3681836822  0.1757097749\n--  [40]  1.0335777386  0.5416641161  1.5276741109\n--  [43]  3.6437637989  0.2576532823 -4.0943953916\n--  [46]  3.3504640478 -3.0657170406 -1.3338407656\n--  [49]  0.4775468364 -5.0091935269 -1.5404491576\n--  [52] -1.8534152185 -0.1747553105  0.7802676784\n--  [55] -0.3136655681  1.8163700876 -0.7152454503\n--  [58] -2.5216348149  0.4981152987  1.8719278762\n--  [61] -2.3517144872 -2.1286013368  0.2140866494\n--  [64] -1.7763786487  5.9568954652 -1.7957151663\n--  [67]  0.3486519179 -2.1091582863 -3.1502057507\n--  [70] -1.7327672386  0.6981450944 -3.2663646205\n--  [73] -1.3039399566 -1.2860136676  0.5068212726\n--  [76]  3.9314021322  1.3798762905  3.0408999493\n--  [79]  2.3826782399 -3.4527983771 -0.2843151167\n--  [82]  2.2409998910 -1.4167335130  4.0870212577\n--  [85] -1.9014837119  0.5531158392 -1.3824224710\n--  [88]  1.4345328468  0.9162659150  0.4317043787\n--  [91]  0.1103286768  2.6965882292  1.7666302467\n--  [94]  1.3981904244  2.9210212209 -0.1613758949\n--  [97] -0.5981513185  0.0008592863  1.5151510828\n-- [100]  0.3190810573\n\n\nWhen Simplification Complicates\nR is an example of a weakly typed language with dynamic typing. That means data types are known only at runtime, not before, and that the language will try to coerce disparate types to a common type instead of throwing an error. That means the interpreter will happily run code like\n\npaste(mtcars, 1)\n\n--  [1] \"c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26, 30.4, 15.8, 19.7, 15, 21.4) 1\"                    \n--  [2] \"c(6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4) 1\"                                                                                                            \n--  [3] \"c(160, 160, 108, 258, 360, 225, 360, 146.7, 140.8, 167.6, 167.6, 275.8, 275.8, 275.8, 472, 460, 440, 78.7, 75.7, 71.1, 120.1, 318, 304, 350, 400, 79, 120.3, 95.1, 351, 145, 301, 121) 1\"                       \n--  [4] \"c(110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 205, 215, 230, 66, 52, 65, 97, 150, 150, 245, 175, 66, 91, 113, 264, 175, 335, 109) 1\"                                                     \n--  [5] \"c(3.9, 3.9, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92, 3.07, 3.07, 3.07, 2.93, 3, 3.23, 4.08, 4.93, 4.22, 3.7, 2.76, 3.15, 3.73, 3.08, 4.08, 4.43, 3.77, 4.22, 3.62, 3.54, 4.11) 1\"                  \n--  [6] \"c(2.62, 2.875, 2.32, 3.215, 3.44, 3.46, 3.57, 3.19, 3.15, 3.44, 3.44, 4.07, 3.73, 3.78, 5.25, 5.424, 5.345, 2.2, 1.615, 1.835, 2.465, 3.52, 3.435, 3.84, 3.845, 1.935, 2.14, 1.513, 3.17, 2.77, 3.57, 2.78) 1\"  \n--  [7] \"c(16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20, 22.9, 18.3, 18.9, 17.4, 17.6, 18, 17.98, 17.82, 17.42, 19.47, 18.52, 19.9, 20.01, 16.87, 17.3, 15.41, 17.05, 18.9, 16.7, 16.9, 14.5, 15.5, 14.6, 18.6) 1\"\n--  [8] \"c(0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1) 1\"                                                                                                            \n--  [9] \"c(1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1) 1\"                                                                                                            \n-- [10] \"c(4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 5, 4) 1\"                                                                                                            \n-- [11] \"c(4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2, 2, 4, 2, 1, 2, 2, 4, 6, 8, 2) 1\"\n\n\npaste just coerces everything to character, no matter how ludicrous the results. This behavior can trip you up, but it’s not truly insidious.\nUnfortunately, R sometimes changes types under your nose. Suppose we write a function, subset2. It takes as argument a data frame, and two functions that take a data frame as argument. It filters the data column-wise using col_f, then rowwise using row_f.\n\nsubset2 <- function(df, col_f, row_f) {\n  df <- df[, col_f(df)]\n  df[row_f(df), ]\n}\nsubset2(mtcars, \\(x) colSums(x) > 500, \\(x) rowSums(x) > 500)\n\n\n\n  \n\n\n\nThat seems to work. (Deadly words!) But what if my finger had slipped when I typed 500?\n\nsubset2 <- function(df, col_f, row_f) {\n  df <- df[row_f, col_f(df)]\n  df[row_f(df), ]\n}\nsubset2(mtcars, \\(x) colSums(x) > 5000, \\(x) rowSums(x) > 500)\n\n-- Error in xj[i]: invalid subscript type 'closure'\n\n\nWhat happened? Only one column of mtcars, disp, has a column sum greater than 5000. And what happens if you select a single column with array-style indexing?\n\nmtcars[, \"disp\"]\n\n--  [1] 160.0 160.0 108.0 258.0 360.0 225.0 360.0\n--  [8] 146.7 140.8 167.6 167.6 275.8 275.8 275.8\n-- [15] 472.0 460.0 440.0  78.7  75.7  71.1 120.1\n-- [22] 318.0 304.0 350.0 400.0  79.0 120.3  95.1\n-- [29] 351.0 145.0 301.0 121.0\n\n\nR helpfully simplifies to an atomic vector. We can fix our function by disabling this behavior:\n\nsubset3 <- function(df, col_f, row_f) {\n  df <- df[, col_f(df), drop = FALSE]\n  df[row_f(df), ]\n}\nsubset3(mtcars, \\(x) colSums(x) > 5000, \\(x) rowSums(x) > 500)\n\n-- numeric(0)\n\n\nor, even more sensibly, using list subsetting (single brackets, no comma), which never simplifies.\nThis behavior isn’t indefensible. It’s consistent with how subsetting works on arrays (which are usually atomic vectors). In interactive use, it’s convenient, since then you’re usually interested in the data a column contains, not the object containing it. But automatic simplification is easily missed and potentially destructive, and the way to avoid it can be found only if you carefully read the documentation.\nBrevity is the Soul of Bugs\nSuppose you have the following vector:\n\nx <- c(1, 4, 7, NA, -9, NA)\n\nR is strict about missing values, but not about logical constants. T and F can be used as abbreviations for TRUE and FALSE, respectively. The following is a valid way of taking the mean:\n\nmean(x, na.rm = T)\n\n-- [1] 0.75\n\n\nLikewise, with F for FALSE:\n\nmtcars[1:5, \"cyl\", drop = F]\n\n\n\n  \n\n\n\nWhat’s the harm in this? While TRUE and FALSE are reserved words, the abbreviations aren’t. Let’s say your colleague creates a variable T, making sure to use uppercase to avoid masking the t function:\n\nT <- pt(2, df = 10)\n\nThis code now fails in a confusing way:\n\nmean(x, na.rm = T)\n\n-- [1] NA\n\n\nThe reason for this feature, as before, is clear: it’s convenient in interactive use. The problem with it is equally clear: it’s suicidal in programmatic use.\nConclusion\nThe theme here is obvious: features that save a few keystrokes in interactive use can cause maddening bugs if carelessly used in production code. You need familiarity with the language and some degree of vigilance to avoid the pitfalls, and everyone slips now and again.\nThe longer I’ve spent with R, the more convinced I’ve become that R has outgrown these features. R was designed as an environment for interactive data exploration, statistical testing, and graphical displays, but today it can do so much more: serve Web apps, query remote databases, render just about any document (even this one) with Rmarkdown or Quarto, and many other uses. But to fulfill these sophisticated use cases, you have to carefully avoid traps like the ones discussed here. Some organizations have no doubt avoided the problem by switching to Python. So R’s design emphasis on interactivity may limit its growth.\nMoreover, the benefits these features deliver are scant. The three behaviors I describe - partial argument matching, logical abbreviations, and drop = FALSE save a bit of typing (or, in the last case, an extra step of data manipulation). A few key strokes saved here and there adds up quickly, and the savings may have been significant in the days when users were limited to R’s basic readline prompt. But that doesn’t balance the potential harm they can cause in production code today, especially when modern IDEs (and Vim or Emacs, of course) support autocompletion, obviating the need for abbreviated code.\nDon’t get me wrong. R remains a powerful, expressive language built on solid design principles. It’s my first choice for any kind of data manipulation, and I still find it fun and satisfying to use. But some of its behaviors are more at home in its past than its future."
  },
  {
    "objectID": "posts/lesser-known/index.html",
    "href": "posts/lesser-known/index.html",
    "title": "R Tricks I Wish I’d Known as a Beginner",
    "section": "",
    "text": "R is full of quirks, some of them obscure. Getting the most out of the language takes some experience, but is well worth the effort. These techniques will be old hat to seasoned R users, but you never know: you might still learn something."
  },
  {
    "objectID": "posts/lesser-known/index.html#get-the-expressions-passed-as-function-arguments",
    "href": "posts/lesser-known/index.html#get-the-expressions-passed-as-function-arguments",
    "title": "R Tricks I Wish I’d Known as a Beginner",
    "section": "Get the Expressions Passed as Function Arguments",
    "text": "Get the Expressions Passed as Function Arguments\nR passes function arguments by value, not by reference, yet it’s possible to recover the symbol or expression passed to a function using this trick:\n\nf <- function(x) {\n  x <- deparse(substitute(x))\n  print(x)\n}\nf(`I'm a symbol!`)\n\n-- [1] \"I'm a symbol!\"\n\n\nsubstitute, when called in a function, replaces its argument with the expression in the promise corresponding to that argument. (Promises are internal objects that implement function arguments). deparse converts that unevaluated R code into a character vector.\nThis could be used to make a function that automatically labels plot axes:\n\ndescriptive_plot <- function(x, y) {\n  x_name <- deparse(substitute(x))\n  y_name <- deparse(substitute(y))\n  plot(x, y, xlab = x_name, ylab = y_name)\n}\nweight <- mtcars$wt\nmpg <- mtcars$mpg\ndescriptive_plot(mpg, weight)\n\n\n\n\n\n\n\nWhat are your favorite R tricks?"
  },
  {
    "objectID": "posts/next_language/index.html",
    "href": "posts/next_language/index.html",
    "title": "The Data Science Language of the Future",
    "section": "",
    "text": "R, for all its warts, has most of the features I want from a data science language. It’s powerful, surprisingly versatile, and usually fun to use. But, like all languages, it is neither perfect nor likely to be widely used forever. (I doubt it will enjoy - if that is the right word - the endless afterlife of COBOL and its ilk). So I hope the (distant!) future will see statistical languages that replicate and refine R’s strengths while improving its weaknesses. What should those languages look like?\nTo discuss a next-generation language, we need to establish what makes R so great to begin with. On reflection, I identified three key ingredients:\nBut vectorization has benefits beyond mathematical convenience. (For now, let’s use Hadley Wickham’s working definition of a vectorized function: \\(f(x[[i]]) = f(x)[[i]]\\)). It abstracts away the iteration involved in operations, freeing you to think of functions as acting on each element independently. This results in compact, readable code:\nIn base Python or most other languages, this would require a for loop that kept track of letters and indices, resulting in less readable code and a greater likelihood of mistakes. Better still, R features convenience functions like colMeans that operate at a higher level of abstraction: data frames or arrays, which are versatile generalizations of simple atomic vectors. These capabilities let you ignore implementation details of iteration and write nicely abstract code.\nVectorization is hardly unique to R, but I don’t know of another language\nas fundamentally vector-oriented. Our ideal successor language should emulate R in this area.\nToo often, the actual “science” of data science, like dessert after a big feast, is dwarfed by what came before: data tidying, missing value imputation, transformation, and\neverything else required to get messy input into a form that can be analyzed. If a data pipeline doesn’t exist, this can become far more daunting than the analysis itself. No language is better suited for the job than R. A skilled user can achieve even elaborate transformations in ten or twenty lines. With practice, the feeling of power becomes almost addictive. Using another language feels like putting on heavy gloves before tying your shoelaces.\nR’s expressive, powerful data manipulation interface grants it this power. It also makes R hard to learn. You can often find five or six obvious, correct ways to do even a simple task, like obtaining the fourth element of the mtcars columns cyl.\nA successor to R might develop a smaller set of operators, and smooth out some oddities (like drop = FALSE). But it should not go too far in this Emphasizing readability and separating tasks into different functions, as dplyr has done, would make code more readable and easier to debug, but also more verbose. Too radical a departure from R’s approach would fail to replicate what makes it special.\nThe other two areas I identify are widely cited as strengths of R. This one, though, is esoteric. While almost all R users take advantage of the features that power metaprogramming, many without knowing it, few use them extensively. It’s easy (and sometimes advisable) even for experienced users to avoid invoking it directly. Still, it distinguishes R from most other languages, and rests on bold design decisions made long before the language’s inception.\n“Metaprogramming”, as used in the R community, means writing programs that treat R code as data - programming on programs, in other words. It utilizes R’s highly developed capabilities for partial expression substitution, controlled evaluation, and environment manipulation. Books could be written about this topic, and Advanced R covers it in detail.\nAs a basic example, have you ever wondered why most calls to library in R scripts look like library(package), not library(\"package\")? The latter is legal, but seldom used. Most functions will throw an error if passed the name of a nonexistent object:\nBut certain functions capture their inputs directly, without evaluating them, and then evaluate them in a different context. This is called “quoting”, since it captures the syntax of code while ignoring the semantics the way quoting natural language does. The implementation, known as non-standard evaluation, powers much of R’s interface. One prominent example is formulas: a compact mini-language for specifying a statistical relationship to modeling functions. Because the formula is quoted and evaluated in the context of a data frame, the user can provide bare variable names, making for a clean, simple interface:\nThe tidyverse takes this idea much further. Its functions rely on tidy evaluation, an elaborate framework for selecting and modifying variables within “data masks.” In the end, R is really a statistics-oriented descendant of Lisp with more conventional syntax. Many of these ideas - expressions as data, expression substitution, and even optional prefix syntax - come from that immortal language.\nAll this power comes with serious drawbacks - serious enough that it can be reasonably argued that non-standard evaluation is a bad paradigm. Manipulating expressions means code loses referential transparency (evaluating the same if variable names are changed). Controlled evaluation requires programmers to think about environment inheritance, creating the potential for a host of subtle bugs. Functions that quote some of their arguments but not all, or accept quoted and nonquoted forms of the same argument (like library), are harder to use. In the end, all this indirection makes code harder to write and reason about (hence the need for a vignette on simply programming with dplyr). I think the tradeoff is worthwhile; the convenience and flexibility of non-standard evaluation are too valuable to abandon. But unlike the other two characteristics I outlined above, a strong case can be made otherwise.\nIn short, a successor to R should contain R’s most powerful features: vector types and vectorized functions, a terse but expressive subsetting syntax, and support for expression manipulation and controlled evaluation."
  },
  {
    "objectID": "posts/next_language/index.html#finicky-interface",
    "href": "posts/next_language/index.html#finicky-interface",
    "title": "The Data Science Language of the Future",
    "section": "Finicky Interface",
    "text": "Finicky Interface\nR’s user interface, in places, in harder to learn and use than necessary. It uses conventions inconsistently, exposes too much detail to the user, and contains too many “gotchas” that cause confusing errors you can only avoid with experience.\nOne of the unwritten rules of programming is that inconsistency should not exist without reason. If you write a class Foo with methods called bar_bar, baz_baz, and quxQux, your users will wonder why you used camelCase for just one method every time they try to call the logically expected but nonexistent qux_qux. If you put a data frame argument at the head of one function’s argument list but the tail of another’s, they will wonder why every time they forget which is which. Only constant attention in design can avoid inconsistencies like these, but the best designs do so.\nR violates the principle in many places. One trivial but well-known example is the way S3 methods are written generic.class (e.g., mean.default), yet dots are used all the time in the names of functions, including S3 generics. The many exceptions (t.test, all.vars, …) thwart a potentially useful convention. Unlike the other functionals, mapply has the function as the first argument, not the second, and the simplify and use.names arguments are\nactually SIMPLIFY and USE.NAMES (not without reason, but good luck remembering). ave and tapply do similar things, but ave uses ... for grouping factors, while tapply reserves it for arguments to the FUN argument. Once you notice one of these seams in the design, you can’t unsee it.\nR sometimes contains unnecessary complexity. Interfaces often have complicated semantics, and functions sometimes feature multiple operating modes. For instance, there are two slightly different functions for doing principal components analysis, differing in the algorithm used. The function diag has four distinct uses (five, if you count diag<- as part of the same interface). Most troubling to me are the heavily overloaded arguments of certain functions. Consider this passage from the help for get:\nThe ‘pos’ argument can specify the environment in which to look\n     for the object in any of several ways: as a positive integer (the\n     position in the ‘search’ list); as the character string name of an\n     element in the search list; or as an ‘environment’ (including\n     using ‘sys.frame’ to access the currently active function calls).\n     The default of ‘-1’ indicates the current environment of the call\n     to ‘get’. The ‘envir’ argument is an alternative way to specify an\n     environment.\nI count three possible types for pos, all with different meanings, a default value with a special meaning, and another argument that does exactly the same thing for one type. (Plus a suggestion to use call stack introspection, which I’ll leave to braver programmers than me).\nTrying to memorize the intricacies of an interface like this is a fool’s errand: at some point, you’ll get it wrong and cause a nasty bug. That leaves no recourse but referring to the documentation each time you use the function, and nothing makes an interface more annoying to use.\nAnother offender is factors. Factors represent categorical variables by mapping integer codes to levels. Simple idea, but so many potential errors come from this fact. Something as simple as naively concatenating a factor causes disaster:\n\nx <- factor(c(\"a\", \"b\", \"c\"))\ny <- factor(c(\"x\", \"y\", \"z\"))\nc(x, \"d\")\n\n-- [1] \"1\" \"2\" \"3\" \"d\"\n\n\nAttempting to do factor arithmetic only triggers a warning, despite being nonsense (Note also that the factor warning preempts the “mismatched object lengths” warning this would normally trigger):\n\nx + 3:6\n\n-- [1] NA NA NA NA\n\n\nWorst of all, and not widely known: R’s lexical sort order differs by system locale. (See here for an example). When creating a factor, R defaults to ordering the levels lexically. Good luck with that reproducible research!\nIndividually, these criticisms are trivial. I don’t mean to cast them as evidence of incompetence or carelessness by the language designers. I have written much worse interfaces to far simpler programs, so I know from experience how hard it is to implement and maintain a good one. But our successor language can do better by following the tidyverse and making “design for human users” a core principle."
  },
  {
    "objectID": "posts/next_language/index.html#very-weak-typing",
    "href": "posts/next_language/index.html#very-weak-typing",
    "title": "The Data Science Language of the Future",
    "section": "Very Weak Typing",
    "text": "Very Weak Typing\nOur new language should have dynamic typing. Static typing makes code easier to reason about and debug, especially in large applications, but it would be awkward to explore or transform data without quick, easy type conversions that can be done interactively. In its present form, I think R makes these conversions a little too easy. R is a weakly typed language: instead of disallowing operations with objects of disparate types, it casts them to a common type. Sometimes the result is predictable:\n\nc(TRUE, \"abc\")\n\n-- [1] \"TRUE\" \"abc\"\n\nTRUE + 3\n\n-- [1] 4\n\n\nBut sometimes R will allow operations that have no sensible result:\n\npaste0(mtcars, \"abc\")\n\n--  [1] \"c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26, 30.4, 15.8, 19.7, 15, 21.4)abc\"                    \n--  [2] \"c(6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4)abc\"                                                                                                            \n--  [3] \"c(160, 160, 108, 258, 360, 225, 360, 146.7, 140.8, 167.6, 167.6, 275.8, 275.8, 275.8, 472, 460, 440, 78.7, 75.7, 71.1, 120.1, 318, 304, 350, 400, 79, 120.3, 95.1, 351, 145, 301, 121)abc\"                       \n--  [4] \"c(110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 205, 215, 230, 66, 52, 65, 97, 150, 150, 245, 175, 66, 91, 113, 264, 175, 335, 109)abc\"                                                     \n--  [5] \"c(3.9, 3.9, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92, 3.07, 3.07, 3.07, 2.93, 3, 3.23, 4.08, 4.93, 4.22, 3.7, 2.76, 3.15, 3.73, 3.08, 4.08, 4.43, 3.77, 4.22, 3.62, 3.54, 4.11)abc\"                  \n--  [6] \"c(2.62, 2.875, 2.32, 3.215, 3.44, 3.46, 3.57, 3.19, 3.15, 3.44, 3.44, 4.07, 3.73, 3.78, 5.25, 5.424, 5.345, 2.2, 1.615, 1.835, 2.465, 3.52, 3.435, 3.84, 3.845, 1.935, 2.14, 1.513, 3.17, 2.77, 3.57, 2.78)abc\"  \n--  [7] \"c(16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20, 22.9, 18.3, 18.9, 17.4, 17.6, 18, 17.98, 17.82, 17.42, 19.47, 18.52, 19.9, 20.01, 16.87, 17.3, 15.41, 17.05, 18.9, 16.7, 16.9, 14.5, 15.5, 14.6, 18.6)abc\"\n--  [8] \"c(0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1)abc\"                                                                                                            \n--  [9] \"c(1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1)abc\"                                                                                                            \n-- [10] \"c(4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 5, 4)abc\"                                                                                                            \n-- [11] \"c(4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2, 2, 4, 2, 1, 2, 2, 4, 6, 8, 2)abc\"\n\n\nMoreover, R has no equivalent of Python’s type hinting system. If you want to enforce a specific type for function arguments, you have to do it manually:\n\nfoo <- function(x, y, z) {\n  if (!is.character(x)) {\n    stop(\"x must be character\")\n  }\n}\n\nMany of the type-checking helpers like is.character have surprisingly complex behaviors that make them dangerous to rely on.\nR functions also do not always have stable return types. sapply, for example, can return a list, an array, an atomic vector, or even an empty list, depending on the input. Programming guides often recommend lapply or vapply instead, since they enforce stable return types, but many unwary users (including me, at various times) who did not know this have written subtly buggy code.\nR’s very weak typing accounts for much of the unpredictable behavior that makes it challenging to use in large applications. I think strict typing like Python’s would be excessive; operations like paste(1:10, letters[1:10]) are too convenient to part with. But our successor language will dispense with some of the crazier implicit coercions R allows."
  },
  {
    "objectID": "posts/next_language/index.html#string-manipulation",
    "href": "posts/next_language/index.html#string-manipulation",
    "title": "The Data Science Language of the Future",
    "section": "String Manipulation",
    "text": "String Manipulation\nR’s string manipulation facilities leave something to be desired. In other languages, strings are array types or feature array-like subsetting. R, however, handles strings (i.e., the raw character data that make up the elements of character vectors) with an internal type. You can’t extract string elements the way you can in Python:\n\nx = \"A typical string\"\nx[0]\n\n-- 'A'\n\n\nYou have to use substr or substring (barely distinguishable functions again!)\n\nx <- \"A typical string\"\nsubstr(x, 1, 1)\n\n-- [1] \"A\"\n\n\nThe rationale is obvious - the unpalatable alternative would be to implement character vectors as list-like recursive vectors - but it has annoying consequences for the interface, such as strsplit returning a list:\n\nx <- c(\"This is a typical\", \"character vector\")\nstrsplit(x, split = \"\\\\s\")\n\n-- [[1]]\n-- [1] \"This\"    \"is\"      \"a\"       \"typical\"\n-- \n-- [[2]]\n-- [1] \"character\" \"vector\"\n\n\nBut these are quibbles. The real problem is the regular expression interface. This is the only part of base R I actively dislike. There are too many functions with terse, barely distinguishable names. (If you can remember the difference between gregexpr and regexec without looking it up, please teach me your secrets). Functions don’t use PCRE by default, a fact I never remember until it causes an error. They return match data in awkward formats; gregexpr, for instance, returns a list of match start positions and lengths, making it difficult to extract the actual match data.\nPut together, these issues make working with regular expressions much more verbose and painful than necessary. The convoluted snippet below, copied from the documentation, does nothing more than create a matrix with the text from two capture groups. For comparison, Python’s re module contains a groupdict method that stores matches in an appropriate data structure automatically.\n\nnotables <- c(\n  \"  Ben Franklin and Jefferson Davis\",\n  \"\\tMillard Fillmore\"\n)\n# name groups 'first' and 'last'\nname.rex <- \"(?<first>[[:upper:]][[:lower:]]+) (?<last>[[:upper:]][[:lower:]]+)\"\n(parsed <- regexpr(name.rex, notables, perl = TRUE))\n\n-- [1] 3 2\n-- attr(,\"match.length\")\n-- [1] 12 16\n-- attr(,\"index.type\")\n-- [1] \"chars\"\n-- attr(,\"useBytes\")\n-- [1] TRUE\n-- attr(,\"capture.start\")\n--      first last\n-- [1,]     3    7\n-- [2,]     2   10\n-- attr(,\"capture.length\")\n--      first last\n-- [1,]     3    8\n-- [2,]     7    8\n-- attr(,\"capture.names\")\n-- [1] \"first\" \"last\"\n\ngregexpr(name.rex, notables, perl = TRUE)[[2]]\n\n-- [1] 2\n-- attr(,\"match.length\")\n-- [1] 16\n-- attr(,\"index.type\")\n-- [1] \"chars\"\n-- attr(,\"useBytes\")\n-- [1] TRUE\n-- attr(,\"capture.start\")\n--      first last\n-- [1,]     2   10\n-- attr(,\"capture.length\")\n--      first last\n-- [1,]     7    8\n-- attr(,\"capture.names\")\n-- [1] \"first\" \"last\"\n\nparse.one <- function(res, result) {\n  m <- do.call(rbind, lapply(seq_along(res), function(i) {\n    if (result[i] == -1) {\n      return(\"\")\n    }\n    st <- attr(result, \"capture.start\")[i, ]\n    substring(res[i], st, st + attr(result, \"capture.length\")[i, ] - 1)\n  }))\n  colnames(m) <- attr(result, \"capture.names\")\n  m\n}\nparse.one(notables, parsed)\n\n--      first     last      \n-- [1,] \"Ben\"     \"Franklin\"\n-- [2,] \"Millard\" \"Fillmore\"\n\n\n\nimport re \n\nnotables = [\"Ben Franklin and Jefferson Davis\",  \"\\tMillard Fillmore\"]\n[re.match(\".*(?P<first>[A-Z][a-z]+).*(?P<last>[A-Z][a-z]+)\", x).groupdict() for x in notables]\n\n-- [{'first': 'Jefferson', 'last': 'Davis'}, {'first': 'Millard', 'last': 'Fillmore'}]\n\n\n(Is Fillmore’s inclusion a sly joke? He is chiefly notable for being a bottom-tier president).\nThe excellent stringr package provides functions that fix all of these problems. But R users shouldn’t have to choose between taking a major dependency and foregoing easy string processing."
  },
  {
    "objectID": "posts/R6-active/index.html",
    "href": "posts/R6-active/index.html",
    "title": "Programatically Creating Accessor Functions for R6 Objects",
    "section": "",
    "text": "library(R6)\n\nunprotected <- R6Class(\n  classname = \"unprotected\",\n  public = list(foo = 1, bar = 2, baz = 3)\n)\nexample <- unprotected$new()\nexample$foo\n\n-- [1] 1\n\nexample$foo <- 2\nexample$foo\n\n-- [1] 2\n\n\nFields can be protected by sending them to private instead, but that blocks the user from accessing them. The solution is to create an active field. This creates an active binding: a special form of R function that can be used to return a value if called with no arguments and to bind a value if called with one. We can use this capability to create an accessor function that blocks users from changing values:\n\nprotected <- R6Class(\n  classname = \"example\",\n  public = list(\n    bar = 2, baz = 3,\n    initialize = function(foo) private$.foo <- foo\n  ),\n  private = list(.foo = NULL),\n  active = list(foo = function(value) {\n    if (missing(value)) {\n      return(private$.foo)\n    } else {\n      stop(\"Hands off!\")\n    }\n  })\n)\n\nexample <- protected$new(foo = 1)\nexample$foo\n\n-- [1] 1\n\nexample$foo <- 2\n\n-- Error in (function (value) : Hands off!\n\n\n(See chapter 14 of Advanced R for more details).\nThis is all simple enough, but there’s an obvious problem: what if we have a lot of attributes to protect? We could dodge the problem by combining them into a single list attribute, or just copy-paste the same function with different attribute names. But those options aren’t always attractive. I recently confronted this problem while working on an elaborate subclass of torch::dataset, which organizes data for neural networks. I decided to rifle through my bag of functional programming tricks in search of a solution.\nFirst Attempt: Function Factory\nSince each active field requires a function, a function factory was an obvious approach. It’s simple to implement:\n\naccessor_factory <- function(field) {\n  force(field)\n  function(value) {\n    if (missing(value)) {\n      return(private[[\"field\"]])\n    } else {\n      stop(\"Hands off \", field, \"!\")\n    }\n  }\n}\n\n(The real version used a less jocular error message, but I need to have my fun somehow). Because R has lexical scope, field is bound in the manufactured function’s enclosing environment, so when executed it should look there and find it.\nBut it doesn’t work.\n\nprotected <- R6Class(\n  classname = \"example\",\n  public = list(\n    bar = 2, baz = 3,\n    initialize = function(foo) private$.foo <- foo\n  ),\n  private = list(.foo = NULL),\n  active = list(foo = accessor_factory(\".foo\"))\n)\nexample <- protected$new(1)\nexample$foo\n\n-- NULL\n\nexample$foo <- 2\n\n-- Error in lapply(list(...), as.character): object 'field' not found\n\n\nEither R core sneaked support for dynamic scope into the last major version, or the R6Class constructor was doing something funny. Checking the source code found the offending line:\n\ngenerator_funs <- assign_func_envs(generator_funs, generator)\n\nThe constructor modified the environments of function fields (a trick I also resorted to while writing a different subclass, but that’s another story). Relying on scope wouldn’t help, but what would?\nSecond Attempt: as.function\n\nMy next idea was to use R’s obscure but powerful function constructor, as.function. It has a strange implementation: it takes a list, interpreting all elements except the last as name-value pairs for arguments (with an empty value slot designating an argument with no default). The last element should be an expression defining the function body. This is what I wrote:\n\naccessor_factory <- function(field) {\n  force(field)\n  code <- substitute(\n    {\n      if (missing(value)) {\n        return(private[[field]])\n      } else {\n        stop(sQuote(field), \" is read-only\")\n      }\n    },\n    list(field = field)\n  )\n  as.function(eval(substitute(\n    alist(value = , code),\n    list(code = code)\n  )),\n  envir = globalenv()\n  )\n}\n\nThis code demands some explanation. The idea is to return a function with the value of field already substituted, not set at runtime. The first step uses substitute to replace the symbol field with the value passed to the function (i.e., the name of the target attribute). The result forms the body of the manufactured function. I have to call substitute again to substitute this expression into the call to alist passed to as.function, because alist quotes its arguments. That expression actually creates the function we need. (See why most people consider me weird for liking metaprogramming?).\n\nprotected <- R6Class(\n  classname = \"example\",\n  public = list(\n    bar = 2, baz = 3,\n    initialize = function(foo) private$.foo <- foo\n  ),\n  private = list(.foo = NULL),\n  active = list(foo = accessor_factory(\".foo\"))\n)\nexample <- protected$new(1)\nexample$foo\n\n-- [1] 1\n\nexample$foo <- 2\n\n-- Error in (function (value) : '.foo' is read-only\n\n\nThis works. But can we do better?\nThird Attempt: Body Substitution\nR features assignment functions to modify all three parts of a closure: formal arguments, body, and environment. We’re interested in creating a set of functions with slightly different bodies, so pairing body<- with substitute is a natural approach. It’s a lot more readable than my last attempt, too. The classic double-substitute trick for substituting the result of an expression comes from Advanced R.\n\nsubstitute_body <- function(fn, mapping) {\n  body(fn) <- eval(substitute(substitute(temp, mapping), list(temp = body(fn))))\n  fn\n}\n\ntemplate <- function(value) {\n  if (missing(value)) {\n    return(private[[field]])\n  } else {\n    stop(sQuote(field), \" is read-only\")\n  }\n}\nsubstitute_body(template, mapping = list(field = \"test\"))\n\n-- function (value) \n-- {\n--     if (missing(value)) {\n--         return(private[[\"test\"]])\n--     }\n--     else {\n--         stop(sQuote(\"test\"), \" is read-only\")\n--     }\n-- }\n\n\nVictory! Well, almost. To make this truly useful, we need a wrapper function to create a list of accessors from field names. Thankfully, that’s much easier than figuring out the substitution.\n\nset_active_fields <- function(fields) {\n  out <- lapply(fields, function(x) {\n    substitute_body(\n      fn = template,\n      mapping = list(field = x)\n    )\n  })\n  names(out) <- gsub(\"^\\\\.\", \"\", fields)\n  out\n}\n\nA bog-standard use of lapply does the job, with the annoying complication of removing leading dots from the names of private fields.\nWe can even go one step further and write a wrapper to R6Class to automatically create accessors from a list of private attributes.\n\nwith_accessors <- function(classname = NULL,\n                           public,\n                           private,\n                           inherit = NULL, lock_objects = TRUE,\n                           class = TRUE,\n                           portable = TRUE, lock_class = FALSE,\n                           cloneable = TRUE,\n                           parent_env = (function() parent.frame())()) {\n  force(parent_env)\n  active <- set_active_fields(names(private))\n  R6Class(\n    classname = classname, public = public,\n    private = NULL, active = active,\n    inherit = inherit, lock_objects = lock_objects,\n    class = class,\n    portable = portable,\n    lock_class = lock_class,\n    cloneable = cloneable,\n    parent_env = parent_env\n  )\n}\n\n\npublic <- list(initialize = function(foo) {\n  private$.foo <<- foo\n})\nprivate <- list(.foo = NULL, .bar = 2, .baz = 3)\nprotected <- with_accessors(\"example\", public = public, private = private)\n\nexample <- protected$new(foo = 1)\nexample$foo\n\n-- [1] 1\n\nexample$bar\n\n-- [1] 2\n\nexample$baz\n\n-- [1] 3\n\nexample$foo <- 2\n\n-- Error in (function (value) : '.foo' is read-only\n\nexample$baz <- 5\n\n-- Error in (function (value) : '.baz' is read-only\n\n\nNote that because of the indirection, I have to use <<- in initialize. I also have to make parent_env the execution environment of the wrapper, which is the caller environment of R6Class here. There may also be other nasty surprises buried in this use of reference semantics. Still, this was a fun diversion, and proof of how much power R grants the user over environments and evaluation."
  },
  {
    "objectID": "posts/that_kind_of_a_day/index.html",
    "href": "posts/that_kind_of_a_day/index.html",
    "title": "That Kind of a Day",
    "section": "",
    "text": "In fairness, is.numeric ought to be called is_numeric, because dots are supposed to be reserved for S3 methods. R breaks this rule all the time, leading to names like as.data.frame.data.frame. R updates are named after Peanuts strips; getting dinged by the linter for using a base function is something that would happen to Charlie Brown if he ever took up programming. Apparently, whatever method lintr uses to exempt base function names from linting doesn’t work when those function names are arguments to another function.\nAs you might guess from the traceback thirty calls deep on the right of the screen, this wasn’t a great day. But I can’t help but smile when I see a linter commit heresy."
  },
  {
    "objectID": "posts/triumph-travesty/index.html",
    "href": "posts/triumph-travesty/index.html",
    "title": "Triumph and Travesty: Earning All 50 Stars in Advent of Code 2021",
    "section": "",
    "text": "If you haven’t heard of Advent of Code, it’s well worth your time to check out. Created and maintained by software engineer Eric Wastl, Advent of Code (AoC for short) is an annual event involving an advent calendar of Christmas-themed programming challenges. Anyone can participate for free, anonymously if they like. A new puzzle is released on each of the first 25 days of December. They start simple and gradually increase in difficulty. Elite players compete for spots on the official leaderboard of the fastest solutions, but most (myself included) just aim to solve the puzzles. Each puzzle(with one exception) awards two “gold stars” when completed, providing a way to track your progress.\nThe puzzles themselves take the form of well-posed problems, connected through a whimsical yuletide narrative. This year’s edition sent players to the ocean depths in a submarine to retrieve the lost keys to Santa’s sleigh. Along the way, they encountered treacherous currents, labyrinthine caves, and a whole lot of obstreperous sea creatures - all of which could only be overcome with some creative programming (Half the fun is recognizing the classical computer science problems underneath the intentonally silly presentation). Each puzzle consists of two parts. The first part states the problem, with any necessary rules, and offers a plaintext input (randomly generated for each player) to work from. If the player submits the correct answer, they receive a gold star…and updated instructions with a new version of the problem to solve. It usually adds a new constraint or asks the player to interpret the input in a different way; depending on the problem and the player’s approach for part 1, overcoming it could take anything from changing a single line to starting from scratch. Submitting the correct answer for the second part earns another gold star and completes that day’s puzzle. (The lone exception to the standard format is the Christmas Day puzzle, which differs in a way I won’t spoil). Players can use whatever language and strategy they like; some solve puzzles in absurd (or do I mean awesome) languages like Rockstar, or impose tough constraints, because they can.\nThe puzzles test a wide variety of programming techniques, from recursion to graph traversal to regular expressions. The problem statements are all “fair” - there are no hidden rules or lawyerly gotchas - but even a subtle misunderstanding can cost you hours of frustrating debugging (just like real life!). With no constraints and no expectation to write production-quality code, you’re free to tackle each problem as you see fit, limited only by your knowledge and creativity.\nI stumbled across AoC in late 2020, a pivotal time in my life. Perhaps a month before, realizing I liked programming a lot more than policy analysis, I had decided to convert my masters degree from public policy to data science. With enough experience in R to feel (over)confident in my programming skills, I dove in without hesitation and spent much of that holiday break absorbed in the puzzles. Tackling such beautifully abstract problems, with no pressure and no shame in failing, was bliss; I enjoyed even the frustration. Realizing R was ill-suited for many of the puzzles, I switched to Python, learning it as I went. I only ever solved some of the puzzles, and those in amateurish fashion (look here if you’re morbidly curious), but I became a much better programmer for it. Having had so much fun, I resolved to come back next year truly prepared.\nWhen December 2021 came, I threw myself into the puzzles. (I probably should have spent more time studying for exams instead, but this questionable time allocation thankfully didn’t hurt my GPA). The first few days came easily, aside from day 3, for which I kludged together an overcomplicated solution involving bitshifting. I switched between R and Python, preferring R for problems involving matrices and similar structures and Python where iteration was emphasized. Once again, I learned plenty along the way: queues for day 6, optimization for day 7, complex numbers as coordinates for day 11. For longer than I expected, I managed not to fall a day behind.\nBut that couldn’t last. I got badly stuck on part 2 of day 14 (which was not a hard problem, in hindsight). The end came on day 15, a tough problem involving graph traversal. I stalled out after hours of work, until a post on the subreddit pointed me toward Dijkstra’s algorithm. After writing probably the worst implementation of all time and letting my computer chug along for about an hour, I claimed both gold stars. But I had almost burnt myself out. The remaining puzzles (aside from a few easier “breather” problems) seemed impossible, and I ceased trying to keep up. Determined to keep going, I gutted my way through day 16, completing it only after spending hours looking in the wrong places for a simple bug. I knew then I had to stop.\nI had done better than I had expected; 50 stars seemed within reach. After taking a few days off, I knocked out a few of the easier puzzles, leaving thirty-odd stars secured. But then the spring semester started, depriving me of free time. Somehow, I still managed to complete the very challenging day 24, guided by a kind user on the Python discord. After that, as the holidays became a distant memory, Advent of Code fell to the bottom of my priorities.\nThat is, until I graduated. Without a job, and itching to work on something that didn’t involve complex data manipulation, I picked up where I had left off. The first puzzles fell with surprising ease: day 18, after some crude but effective string processing; day 19, after browsing the subreddit for tips; and even part 2 of day 21, completed after I spent half an hour fiddling with code I hadn’t touched for five months (when does that ever happen?). Day 22 stumped me for a while, so I asked for advice on the subreddit and followed a set-theory approach that ended up yielding a very elegant solution. That left just day 23: finding the optimal strategy for a simple puzzle that would be very, very hard to solve programatically. I dimly remembered some post on the subreddit recommending the A* algorithm. Knowing it was always smart to work out the problem with pen and paper before writing any code, I sketched out a game board, cut up some sticky notes to use as tokens, and set to work. I solved part 1 easily enough this way, so for the hell of it I tried again on part 2, which posed the same problem on an expanded game board. A few failed attempts later, I nearly gave up; I had learned the hard way how easily “just one more try” turns into a few hours of futile coding. But that time, I didn’t. When I entered my answer, I saw for the last time the familiar message:\nYou have solved part 2 of this puzzle! It provides one gold star.\nIt came as an anticlimax; I would not have to code that A* nightmare after all. Perversely, I felt cheated. Maybe I had cheated. The official description of Advent of Code entreats you to solve puzzles in “any programming language you like,” after all. Was I violating the spirit of the event by avoiding a programmatic solution entirely? Perhaps. The thought nags at me, so I suspect I’ll come back to this problem eventually, when I’m more confident in graph traversal algorithms. But still, I had all 50 stars, a feat that had seemed impossible just a year before.\nViewed one way, this is a trivial achievement: writing throwaway code to solve toy problems invented to kill time over in the weeks before Christmas. Viewed another way, it’s legitimately impressive. I solved all 25 of these puzzles in the time I could spare, just to sharpen my skills and indulge my love of the art of programming. I think it’s enough to say that grad students my age have found worse diversions. Either way, I emphasize that I had plenty of help: people on the subreddit and other forums to guide me, tutorials to consult, and above all the knowledge that many other people persevered through the same frustrations and got to 50 stars.\nI’ll probably be back next year, of course. I’ll have a lot less time to devote, since I expect to have a job by then. I don’t know if I’ll grind out all 50 stars again, now that I’ve done it already. But I do know that any time I spend on Advent of Code won’t be wasted, and I’ll be a better programmer for it.\nI just hope there aren’t as many graphs this time.\nMy repository for Advent of Code 2021: https://github.com/ryan-heslin/AoC2021"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Hello, world!",
    "section": "",
    "text": "Those goals are:\n\nEngage with the wider R community.\nShare some of the R esoterica I’ve learned\nLearn a web framework, as a first step toward more sophisticated web design\nPractice writing, which I find satisfies me the same way coding does\n\nI plan to mostly write about R and other data science topics, though I might broaden scope later on. If you’ve found your way here, I hope at least you’re entertained for a little while."
  }
]