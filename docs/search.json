[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Why the Name?\nNaming things really is as hard as cache invalidation. I spent a long time casting about for something to call the site before hitting on Verso. It means the reverse side of a physical document, such as the left-hand page of a book. That definition attracted me because I aim to make this blog like a good anthology: you can visit any page and find something short but worth reading. As a publishing term derived from Italian, it has a pleasingly archaic resonance similar to “quarto”, the namesake of the file format this site’s documents are written in. Most important, it’s got an R.\nVerso was built with Quarto and deployed using Github Pages with a GitHub workflow from https://github.com/pommevilla/quarto-render. A previous version of this site was built with rendered Rmarkdown and the blogdown R package."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "source2",
    "section": "",
    "text": "R\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2022-05-05-lesser-known/index.html",
    "href": "post/2022-05-05-lesser-known/index.html",
    "title": "R tricks I wish I’d known as a beginner",
    "section": "",
    "text": "R is full of quirks, some of them obscure. Getting the most out of the language takes some experience, but is well worth the effort. These techniques will be old hat to seasoned R users, but you never know: you might still learn something."
  },
  {
    "objectID": "post/2022-05-05-lesser-known/index.html#bind-multiple-names-to-the-same-value-at-once",
    "href": "post/2022-05-05-lesser-known/index.html#bind-multiple-names-to-the-same-value-at-once",
    "title": "R tricks I wish I’d known as a beginner",
    "section": "Bind Multiple Names to the Same Value At Once",
    "text": "Bind Multiple Names to the Same Value At Once\nThe assignment operator <- (like its right- and super-assigning cousins) is actually a function that returns its right-hand side. That means code like:\n\nx <- 5\ny <- 5\nz <- 5\nx\n\n# [1] 5\n\ny\n\n# [1] 5\n\nz\n\n# [1] 5\n\n\ncan be condensed into one line.\n\nrm(list = ls())\nx <- y <- z <- 5\nx\n\n# [1] 5\n\ny\n\n# [1] 5\n\nz\n\n# [1] 5\n\n\nThis does nothing but make your code more readable, but it’s still good to know."
  },
  {
    "objectID": "post/2022-05-05-lesser-known/index.html#examine-the-source-code-of-functions",
    "href": "post/2022-05-05-lesser-known/index.html#examine-the-source-code-of-functions",
    "title": "R tricks I wish I’d known as a beginner",
    "section": "Examine the Source Code of Functions",
    "text": "Examine the Source Code of Functions\nWhen the documentation fails to resolve a question, the next step is learning what the function actually does. For closures (which are most R functions), inspecting the source code is as simple as entering the function’s name in the console:\n\nsetNames\n\n# function (object = nm, nm) \n# {\n#     names(object) <- nm\n#     object\n# }\n# <bytecode: 0x5617659e7510>\n# <environment: namespace:stats>\n\ntapply\n\n# function (X, INDEX, FUN = NULL, ..., default = NA, simplify = TRUE) \n# {\n#     FUN <- if (!is.null(FUN)) \n#         match.fun(FUN)\n#     if (!is.list(INDEX)) \n#         INDEX <- list(INDEX)\n#     INDEX <- lapply(INDEX, as.factor)\n#     nI <- length(INDEX)\n#     if (!nI) \n#         stop(\"'INDEX' is of length zero\")\n#     if (!all(lengths(INDEX) == length(X))) \n#         stop(\"arguments must have same length\")\n#     namelist <- lapply(INDEX, levels)\n#     extent <- lengths(namelist, use.names = FALSE)\n#     cumextent <- cumprod(extent)\n#     if (cumextent[nI] > .Machine$integer.max) \n#         stop(\"total number of levels >= 2^31\")\n#     storage.mode(cumextent) <- \"integer\"\n#     ngroup <- cumextent[nI]\n#     group <- as.integer(INDEX[[1L]])\n#     if (nI > 1L) \n#         for (i in 2L:nI) group <- group + cumextent[i - 1L] * \n#             (as.integer(INDEX[[i]]) - 1L)\n#     if (is.null(FUN)) \n#         return(group)\n#     levels(group) <- as.character(seq_len(ngroup))\n#     class(group) <- \"factor\"\n#     ans <- split(X, group)\n#     names(ans) <- NULL\n#     index <- as.logical(lengths(ans))\n#     ans <- lapply(X = ans[index], FUN = FUN, ...)\n#     ansmat <- array(if (simplify && all(lengths(ans) == 1L)) {\n#         ans <- unlist(ans, recursive = FALSE, use.names = FALSE)\n#         if (!is.null(ans) && is.na(default) && is.atomic(ans)) \n#             vector(typeof(ans))\n#         else default\n#     }\n#     else vector(\"list\", prod(extent)), dim = extent, dimnames = namelist)\n#     if (length(ans)) {\n#         ansmat[index] <- ans\n#     }\n#     ansmat\n# }\n# <bytecode: 0x561764c39720>\n# <environment: namespace:base>\n\n\nIn my experience, beginners often don’t think to do this, perhaps because they don’t grasp that functions are first-class objects that can be interacted with.\nWe have to do a little more work if the function in question is actually an S3 generic, or is not exported from the package it belongs to. In the first case, we can use the fact that S3 methods are generally named genericname.classname:\n\nprint.data.frame\n\n# function (x, ..., digits = NULL, quote = FALSE, right = TRUE, \n#     row.names = TRUE, max = NULL) \n# {\n#     n <- length(row.names(x))\n#     if (length(x) == 0L) {\n#         cat(sprintf(ngettext(n, \"data frame with 0 columns and %d row\", \n#             \"data frame with 0 columns and %d rows\"), n), \"\\n\", \n#             sep = \"\")\n#     }\n#     else if (n == 0L) {\n#         print.default(names(x), quote = FALSE)\n#         cat(gettext(\"<0 rows> (or 0-length row.names)\\n\"))\n#     }\n#     else {\n#         if (is.null(max)) \n#             max <- getOption(\"max.print\", 99999L)\n#         if (!is.finite(max)) \n#             stop(\"invalid 'max' / getOption(\\\"max.print\\\"): \", \n#                 max)\n#         omit <- (n0 <- max%/%length(x)) < n\n#         m <- as.matrix(format.data.frame(if (omit) \n#             x[seq_len(n0), , drop = FALSE]\n#         else x, digits = digits, na.encode = FALSE))\n#         if (!isTRUE(row.names)) \n#             dimnames(m)[[1L]] <- if (isFALSE(row.names)) \n#                 rep.int(\"\", if (omit) \n#                   n0\n#                 else n)\n#             else row.names\n#         print(m, ..., quote = quote, right = right, max = max)\n#         if (omit) \n#             cat(\" [ reached 'max' / getOption(\\\"max.print\\\") -- omitted\", \n#                 n - n0, \"rows ]\\n\")\n#     }\n#     invisible(x)\n# }\n# <bytecode: 0x56176289afc0>\n# <environment: namespace:base>\n\nmean.default\n\n# function (x, trim = 0, na.rm = FALSE, ...) \n# {\n#     if (!is.numeric(x) && !is.complex(x) && !is.logical(x)) {\n#         warning(\"argument is not numeric or logical: returning NA\")\n#         return(NA_real_)\n#     }\n#     if (isTRUE(na.rm)) \n#         x <- x[!is.na(x)]\n#     if (!is.numeric(trim) || length(trim) != 1L) \n#         stop(\"'trim' must be numeric of length one\")\n#     n <- length(x)\n#     if (trim > 0 && n) {\n#         if (is.complex(x)) \n#             stop(\"trimmed means are not defined for complex data\")\n#         if (anyNA(x)) \n#             return(NA_real_)\n#         if (trim >= 0.5) \n#             return(stats::median(x, na.rm = FALSE))\n#         lo <- floor(n * trim) + 1\n#         hi <- n + 1 - lo\n#         x <- sort.int(x, partial = unique(c(lo, hi)))[lo:hi]\n#     }\n#     .Internal(mean(x))\n# }\n# <bytecode: 0x561761a19308>\n# <environment: namespace:base>\n\n\nIn the second case, we can use the ::: function, ::’s nosier cousin, to access a non-exported object:\n\ntidyselect:::where\n\n# function (fn) \n# {\n#     predicate <- as_function(fn)\n#     function(x, ...) {\n#         out <- predicate(x, ...)\n#         if (!is_bool(out)) {\n#             abort(\"`where()` must be used with functions that return `TRUE` or `FALSE`.\")\n#         }\n#         out\n#     }\n# }\n# <bytecode: 0x56176559d5d8>\n# <environment: namespace:tidyselect>\n\n\nIt’s seldom a good idea to use non-exported functions, because they probably weren’t exported for a reason!\nOf course, for primitives and certain special functions, the source code isn’t in R to begin with:\n\nsum\n\n# function (..., na.rm = FALSE)  .Primitive(\"sum\")\n\n\nTo understand how those work, you’ll have to find the appropriate file in the R source code and read the C code."
  },
  {
    "objectID": "post/2022-05-05-lesser-known/index.html#automatically-trigger-postmortem-debugging",
    "href": "post/2022-05-05-lesser-known/index.html#automatically-trigger-postmortem-debugging",
    "title": "R tricks I wish I’d known as a beginner",
    "section": "Automatically Trigger Postmortem Debugging",
    "text": "Automatically Trigger Postmortem Debugging\nWhen a function misbehaves, the standard advice is to call debug on it and run the problematic call. That will enter the browser in the first line of the function, allowing you to (hoperfully) step through and track down the error.\nBut this can be automated. The error option allows you to set an error handler - a function that is called whenever R experiences an error. If you run\n\noptions(error = utils::recover)\n\nthen R will trigger the debugger whenever it encounters an error (so long as it’s an interactive session). That will allow you to inspect the state of the function that caused the error right before it occurred, and any other frames on the call stack at that moment. R’s error messages are often inscrutable, so this technique is quite powerful.\nHere’s what it looks like in practice. Try running this line yourself and see if you can pinpoint the subtle problem with it (though you can also guess it from ave’s signature).\n\nave(mtcars$mpg, mtcars$vs, mean)\n\n# Error in unique.default(x, nmax = nmax): unique() applies only to vectors\n\n\nThe traceback hints that the problem lies with the ..., and inspecting the stack frames should be enough to track it down.\n\nError in unique.default(x, nmax = nmax) :\n  unique() applies only to vectors\n\nEnter a frame number, or 0 to exit\n\n1: ave(mtcars$mpg, mtcars$vs, mean)\n2: interaction(...)\n3: as.factor(args[[i]])\n4: factor(x)\n5: unique(x, nmax = nmax)\n6: unique.default(x, nmax = nmax)\n\nSelection:\n\nOf course, this can become annoying when it happens for a trivial error like\n\nlm(mpg ~ cyll + wt, data = mtcars)\n\n# Error in eval(predvars, data, env): object 'cyll' not found\n\n\nbut in that case, you can simply set ."
  },
  {
    "objectID": "post/2022-05-05-lesser-known/index.html#get-the-expressions-passed-as-function-arguments",
    "href": "post/2022-05-05-lesser-known/index.html#get-the-expressions-passed-as-function-arguments",
    "title": "R tricks I wish I’d known as a beginner",
    "section": "Get the Expressions Passed as Function Arguments",
    "text": "Get the Expressions Passed as Function Arguments\nR passes function arguments by value, not by reference, yet it’s possible to recover the symbol or expression passed to a function using this trick:\n\nf <- function(x) {\n  x <- deparse(substitute(x))\n  print(x)\n}\nf(`I'm a symbol!`)\n\n# [1] \"I'm a symbol!\"\n\n\nsubstitute, when called in a function, replaces its argument with the expression in the promise corresponding to that argument. (Promises are internal objects that implement function arguments). deparse converts that unevaluated R code into a character vector.\nThis could be used to make a function that automatically labels plot axes:\n\ndescriptive_plot <- function(x, y) {\n  x_name <- deparse(substitute(x))\n  y_name <- deparse(substitute(y))\n  plot(x, y, xlab = x_name, ylab = y_name)\n}\nweight <- mtcars$wt\nmpg <- mtcars$mpg\ndescriptive_plot(mpg, weight)\n\n\n\n\n\n\n\n\nWhat are your favorite R tricks?"
  },
  {
    "objectID": "post/2022-05-05-welcome/index.html",
    "href": "post/2022-05-05-welcome/index.html",
    "title": "Hello, world!",
    "section": "",
    "text": "Those goals are:\n\nEngage with the wider R community.\nFind an outlet for the R esoterica I’ve learned\nLearn a web framework, as a first step toward more sophisticated web design\nPractice writing, which I find satisfies me the same way coding does\n\nI plan to mostly write about R and other data science topics, though I might broaden scope later on. If you’ve found your way here, I hope at least you’re entertained for a little while."
  },
  {
    "objectID": "post/2022-06-02-triumph-travesty/index.html",
    "href": "post/2022-06-02-triumph-travesty/index.html",
    "title": "Triumph and Travesty: Earning All 50 Stars in Advent of Code 2021",
    "section": "",
    "text": "If you haven’t heard of Advent of Code, it’s well worth your time to check out. Created and maintained by software engineer Eric Wastl, Advent of Code (AoC for short) is an annual event involving an advent calendar of Christmas-themed programming challenges. Anyone can participate for free, anonymously if they like. A new puzzle is released on each of the first 25 days of December. They start simple and gradually increase in difficulty. Elite players compete for spots on the official leaderboard of the fastest solutions, but most (myself included) just aim to solve the puzzles. Each puzzle(with one exception) awards two “gold stars” when completed, providing a way to track your progress.\nThe puzzles themselves take the form of well-posed problems, connected through a whimsical yuletide narrative. This year’s edition sent players to the ocean depths in a submarine to retrieve the lost keys to Santa’s sleigh. Along the way, they encountered treacherous currents, labyrinthine caves, and a whole lot of obstreperous sea creatures - all of which could only be overcome with some creative programming (Half the fun is recognizing the classical computer science problems underneath the intentonally silly presentation). Each puzzle consists of two parts. The first part states the problem, with any necessary rules, and offers a plaintext input (randomly generated for each player) to work from. If the player submits the correct answer, they receive a gold star…and updated instructions with a new version of the problem to solve. It usually adds a new constraint or asks the player to interpret the input in a different way; depending on the problem and the player’s approach for part 1, overcoming it could take anything from changing a single line to starting from scratch. Submitting the correct answer for the second part earns another gold star and completes that day’s puzzle. (The lone exception to the standard format is the Christmas Day puzzle, which differs in a way I won’t spoil). Players can use whatever language and strategy they like; some solve puzzles in absurd (or do I mean awesome) languages like Rockstar, or impose tough constraints, because they can.\nThe puzzles test a wide variety of programming techniques, from recursion to graph traversal to regular expressions. The problem statements are all “fair” - there are no hidden rules or lawyerly gotchas - but even a subtle misunderstanding can result in hours of frustrating debugging (just like real life!). With no constraints and no expectation to write production-quality code, you’re free to tackle each problem as you see fit.\nI stumbled across AoC in late 2020, a pivotal time in my life. Perhaps a month before, realizing I liked programming a lot more than policy analysis, I had decided to convert my masters degree from public policy to data science. With enough experience in R to feel (over)confident in my programming skills, I dove in without hesitation and spent much of that holiday break absorbed in the puzzles. Tackling such beautifully abstract problems, with no pressure and no shame in failing, was bliss; I enjoyed even the frustration. Realizing R was ill-suited for many of the puzzles, I switched to Python, learning it as I went. I only ever solved some of the puzzles, and those in amateurish fashion (look here if you’re morbidly curious), but I became a much better programmer for it. Having had so much fun, I resolved to come back next year truly prepared.\nWhen December 2021 came, I threw myself into the puzzles. (I probably should have spent more time studying for exams instead, but this questionable time allocation thankfully didn’t hurt my GPA). The first few days came easily, aside from day 3, for which I kludged together an overcomplicated solution involving bitshifting. I switched between R and Python, preferring R for problems involving matrices and similar structures and Python where iteration was emphasized. Once again, I learned plenty along the way: queues for day 6, optimization for day 7, complex numbers as coordinates for day 11. For longer than I expected, I managed not to fall a day behind.\nBut that couldn’t last. I got badly stuck on part 2 of day 14 (which was not a hard problem, in hindsight). The end came on day 15, a tough problem involving graph traversal. I stalled out after hours of work, until a post on the subreddit pointed me toward Dijkstra’s algorithm. After writing probably the worst implementation of all time and letting my computer chug along for about an hour, I claimed both gold stars. But I had almost burnt myself out. The remaining puzzles (aside from a few easier “breather” problems) seemed impossible, and I ceased trying to keep up. Determined to keep going, I gutted my way through day 16, completing it only after spending hours looking in the wrong places for a simple bug. I knew then I had to stop.\nI had done better than I had expected; 50 stars seemed within reach. After taking a few days off, I knocked out a few of the easier puzzles, leaving thirty-odd stars secured. But then the spring semester started, depriving me of free time. Somehow, I still managed to complete the very challenging day 24, guided by a kind user on the Python discord. After that, as the holidays became a distant memory, Advent of Code fell to the bottom of my priorities.\nThat is, until I graduated. Without a job, and itching to work on something that didn’t involve complex data manipulation, I picked up where I had left off. The first puzzles fell with surprising ease: day 18, after some crude but effective string processing; day 19, after browsing the subreddit for tips; and even part 2 of day 21, completed after I spent half an hour fiddling with code I hadn’t touched for five months (when does that ever happen?). Day 22 stumped me for a while, so I asked for advice on the subreddit and followed a set-theory approach that ended up yielding a very elegant solution. That left just day 23: finding the optimal strategy for a simple puzzle that would be very, very hard to solve programatically. I dimly remembered some post on the subreddit recommending the A* algorithm. Knowing it was always smart to work out the problem with pen and paper before writing any code, I sketched out a game board, cut up some sticky notes to use as tokens, and set to work. I solved part 1 easily enough this way, so for the hell of it I tried again on part 2, which posed the same problem on an expanded game board. A few failed attempts later, I nearly gave up; I had learned the hard way how easily “just one more try” turns into a few hours of futile coding. But that time, I didn’t. When I entered my answer, I saw for the last time the familiar message:\nYou have solved part 2 of this puzzle! It provides one gold star.\nIt came as an anticlimax; I would not have to code that A* nightmare after all. Perversely, I felt cheated. Maybe I had cheated. The official description of Advent of Code entreats you to solve puzzles in “any programming language you like,” after all. Was I violating the spirit of the event by avoiding a programmatic solution entirely? Perhaps. The thought nags at me, so I suspect I’ll come back to this problem eventually, when I’m more confident in graph traversal algorithms. But still, I had all 50 stars, a feat that had seemed impossible just a year before.\nViewed one way, this is a trivial achievement: writing throwaway code to solve toy problems invented to kill time over in the weeks before Christmas. Viewed another way, it’s legitimately impressive. I solved all 25 of these puzzles in the time I could spare, just to sharpen my skills and indulge my love of the art of programming. I think it’s enough to say that grad students my age have found worse diversions. Either way, I emphasize that I had plenty of help: people on the subreddit and other forums to guide me, tutorials to consult, and above all the knowledge that many other people persevered through the same frustrations and got to 50 stars.\nI’ll probably be back next year, of course. I’ll have a lot less time to devote, since I expect to have a job by then. I don’t know if I’ll grind out all 50 stars again, now that I’ve done it already. But I do know that any time I spend on Advent of Code won’t be wasted, and I’ll be a better programmer for it.\nI just hope there aren’t as many graphs this time.\nMy repository for Advent of Code 2021: (https://github.com/ryan-heslin/AoC2021)[https://github.com/ryan-heslin/AoC2021]"
  },
  {
    "objectID": "post/2022-06-05-hello-world/index.html",
    "href": "post/2022-06-05-hello-world/index.html",
    "title": "Hello, world!",
    "section": "",
    "text": "Those goals are:\n\nEngage with the wider R community.\nFind an outlet for the R esoterica I’ve learned\nLearn a web framework, as a first step toward more sophisticated web design\nPractice writing, which I find satisfies me the same way coding does\n\nI plan to mostly write about R and other data science topics, though I might broaden scope later on. If you’ve found your way here, I hope at least you’re entertained for a little while."
  },
  {
    "objectID": "post/2022-06-12-ghost-machine/index.html",
    "href": "post/2022-06-12-ghost-machine/index.html",
    "title": "Ghost in the Machine: The Remnant of R’s Past That Haunts it Still",
    "section": "",
    "text": "(I can’t help but pause here to relay the account the linked presentation gives of R’s origins. It all began with this hallway conversation between Ross Ihaka and Robert Gentleman in the University of Auckland around 1990):\n\nGentleman: “Let’s write some software.” Ihaka: “Sure, that sounds like fun.”\n\nBut as many users know, its roots go back further. R was developed from the language S, created in the 1970s by a team led by John Chambers at Bell Labs. Those were the glory days of Bell Labs, when the language C and the Unix ecosystem were developed. Like a modern palace built on the foundations of an ancient one, R bears many traces of its lineage. Syntax is very similar, many features are backward-compatible, and the documentation for some functions even refers to resources about S rather than R. (Try ?sum, for one example).\nOne of those traces, harder to observe but certainly still present, is also one of R’s most unusual (and, in some quarters, derided) features: an emphasis on convenience in interactive use. Interpreted languages typically support interactivity in some way, since the ability to run a snippet of code and instantly get results is one of their greatest advantages over compiled languages. But S was designed primarily for interactive data exploration, and R has retained that capability as a design focus. In areas great and small, from core design choices to implementation quirks, R makes it as easy as possible to bang out code in the console and see what happens. That makes it a fast, flexible tool for exploring data and following hunches. It also strews mines in the path of anyone programming in the language without detailed knowledge of the language’s nuances.\nA few examples will make this painfully clear.\n\nPartial Matching, Complete Headache\nCan you spot the problem with this call? It runs correctly:\n\nrep(1:3, length = 10)\n\n#  [1] 1 2 3 1 2 3 1 2 3 1\n\n\nbut is missing something. The relevant argument of rep is actually called length.out, not length, but R’s partial argument matching saves us, since length is a shortened form of length.out.\nThis is nice to have when typing code in the console. But relying on partial argument matching in scripts is a very bad idea.\nSuppose you’re working with a package that includes some functions with annoyingly long argument names. All that typing is annoying, so you decide you may as well save some keystrokes:\n\nfoo <- function(xyzabc = 0, abcxyz) {\n  rnorm(100, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n#   [1]  1.18772811 -2.57536507 -1.58505266\n#   [4] -0.72715359 -0.59444593  2.16739497\n#   [7]  1.68861123 -1.91503332  2.63191150\n#  [10] -1.15614172 -2.76106603 -0.21240216\n#  [13] -0.20151260 -1.79070949  1.36979744\n#  [16] -1.88352171 -3.94302220 -0.38676598\n#  [19] -2.42975912  1.32333137  2.14419207\n#  [22]  0.32518271 -0.09251910 -0.53351786\n#  [25] -0.71804320  0.34086552 -1.58105314\n#  [28] -0.68276235  0.48319128 -4.41103178\n#  [31] -0.79560471  0.13525710  1.34255398\n#  [34]  2.94085936 -1.51489247 -0.55068428\n#  [37] -0.84168995 -1.10655372  0.95316807\n#  [40]  2.14069731 -3.32654196 -0.85730764\n#  [43] -2.66145362 -0.26731908  0.61907297\n#  [46]  1.16544014  0.21551372  1.82916461\n#  [49] -1.38842049 -0.63971696  0.13592802\n#  [52] -0.37017736  0.07512174 -3.91815576\n#  [55] -0.60827878 -3.47840078 -1.99640067\n#  [58]  3.48874290  0.02611881 -0.96680590\n#  [61] -1.39897333  0.75415445  1.53229357\n#  [64] -3.25325750  0.30633678  4.01245004\n#  [67] -1.92059128 -1.05045277  2.16968513\n#  [70] -0.57613031  0.78409814  0.48255083\n#  [73]  0.94384415  0.51211819 -0.46917973\n#  [76]  1.54460492 -0.26142621 -2.68313109\n#  [79]  0.49588817  0.26255455  0.29949180\n#  [82]  1.06700237  1.29540351 -1.14076945\n#  [85]  0.66853331 -3.73537725  4.02039038\n#  [88]  1.65109063  1.31474188  2.63322064\n#  [91] -2.02951598 -4.82495462 -1.87790836\n#  [94]  1.07904059  1.06968136  2.46165972\n#  [97]  2.54750767 -3.87146461  2.35450978\n# [100]  0.15077431\n\n\nAll seems well. But then a version update adds a new argument:\n\nfoo <- function(abcabc = 100, xyzabc = 0, abcxyz) {\n  rnorm(abcabc, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n# Error in foo(abc = 2): argument 1 matches multiple formal arguments\n\n\nR throws an error, unable to find an unambiguous match. (Imagine how painful this would be to debug if R defaulted to the first match instead). The way to avoid this scenario is simple: never rely on partial argument matching in permanent code. Nonetheless, many packages do. You can identify offenders yourself by setting the warnPartialMatchArgs option:\n\noptions(warnPartialMatchArgs = TRUE)\nfoo <- function(xyzabc = 0, abcxyz) {\n  rnorm(100, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n# Warning in foo(abc = 2): partial argument match of\n# 'abc' to 'abcxyz'\n\n\n#   [1] -0.453317893  1.306236981 -2.632487842\n#   [4] -1.714061118  0.065765170 -2.641946062\n#   [7]  1.686807975  1.250018839  0.369893845\n#  [10] -0.641775225 -1.709441095  0.908837085\n#  [13] -0.790816589 -0.496031879  1.305793506\n#  [16]  0.490506697  1.567351139 -2.890075109\n#  [19] -2.941597656 -2.746591170 -0.772168245\n#  [22]  2.326118893  0.009355166 -2.152489556\n#  [25]  1.362320197 -2.268523982 -3.851974908\n#  [28] -2.530832183  2.511142799 -2.814752265\n#  [31] -1.190097320  0.510552511 -0.484870728\n#  [34]  0.458402664 -3.651149958  1.059708391\n#  [37]  1.643933469  1.207642363 -2.362661521\n#  [40] -0.932858256 -1.496182080  0.992694649\n#  [43] -3.369598200  2.806288937 -5.066535674\n#  [46]  0.974403025  2.239096871  0.380443082\n#  [49]  1.908152335 -1.170534068  0.728850513\n#  [52]  1.579843107 -1.244214705  1.502471711\n#  [55] -4.589283402  1.714666317 -0.575241369\n#  [58] -0.776774949 -0.095899186  0.633146997\n#  [61] -1.316731281  0.167514351  0.856183619\n#  [64] -0.146683709 -1.368397776  0.437089107\n#  [67] -1.666103810 -1.200372697  2.605907760\n#  [70]  1.097974835 -0.639711762 -2.521428495\n#  [73] -0.562183960  0.244011108  1.847805924\n#  [76] -2.391612742 -0.766733307  1.209862340\n#  [79] -3.616335538  0.588264958  0.135325977\n#  [82] -1.786709117 -0.954581509 -0.804861060\n#  [85]  1.118992551  0.238597418  2.646951453\n#  [88]  0.731808241  2.145717152  1.504771108\n#  [91]  0.884070436  2.037995166  0.059890039\n#  [94] -1.138846968 -0.071061703 -1.446988361\n#  [97] -0.769846064 -0.810762969 -0.243019902\n# [100]  0.807212976\n\n\n\n\nWhen Simplification Complicates\nR is an example of a weakly typed language with dynamic typing. That means data types are known only at runtime, not before, and that the language will try to coerce disparate types to a common type instead of throwing an error. That means the interpreter will happily run code like\n\npaste(mtcars, 1)\n\n#  [1] \"c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26, 30.4, 15.8, 19.7, 15, 21.4) 1\"                    \n#  [2] \"c(6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4) 1\"                                                                                                            \n#  [3] \"c(160, 160, 108, 258, 360, 225, 360, 146.7, 140.8, 167.6, 167.6, 275.8, 275.8, 275.8, 472, 460, 440, 78.7, 75.7, 71.1, 120.1, 318, 304, 350, 400, 79, 120.3, 95.1, 351, 145, 301, 121) 1\"                       \n#  [4] \"c(110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 205, 215, 230, 66, 52, 65, 97, 150, 150, 245, 175, 66, 91, 113, 264, 175, 335, 109) 1\"                                                     \n#  [5] \"c(3.9, 3.9, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92, 3.07, 3.07, 3.07, 2.93, 3, 3.23, 4.08, 4.93, 4.22, 3.7, 2.76, 3.15, 3.73, 3.08, 4.08, 4.43, 3.77, 4.22, 3.62, 3.54, 4.11) 1\"                  \n#  [6] \"c(2.62, 2.875, 2.32, 3.215, 3.44, 3.46, 3.57, 3.19, 3.15, 3.44, 3.44, 4.07, 3.73, 3.78, 5.25, 5.424, 5.345, 2.2, 1.615, 1.835, 2.465, 3.52, 3.435, 3.84, 3.845, 1.935, 2.14, 1.513, 3.17, 2.77, 3.57, 2.78) 1\"  \n#  [7] \"c(16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20, 22.9, 18.3, 18.9, 17.4, 17.6, 18, 17.98, 17.82, 17.42, 19.47, 18.52, 19.9, 20.01, 16.87, 17.3, 15.41, 17.05, 18.9, 16.7, 16.9, 14.5, 15.5, 14.6, 18.6) 1\"\n#  [8] \"c(0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1) 1\"                                                                                                            \n#  [9] \"c(1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1) 1\"                                                                                                            \n# [10] \"c(4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 5, 4) 1\"                                                                                                            \n# [11] \"c(4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2, 2, 4, 2, 1, 2, 2, 4, 6, 8, 2) 1\"\n\n\npaste just coerces everything to character, no matter how ludicrous the results. This behavior can trip you up, but it’s not truly insidious.\nUnfortunately, R sometimes changes types under your nose. Suppose we write a function, subset2. It takes as argument a data frame, and two functions that take a data frame as argument. It filters the data column-wise using col_f, then rowwise using row_f.\n\nsubset2 <- function(df, col_f, row_f) {\n  df <- df[, col_f(df)]\n  df[row_f(df), ]\n}\nsubset2(mtcars, \\(x) colSums(x) > 500, \\(x) rowSums(x) > 500)\n\n#                      mpg disp  hp  qsec\n# Hornet Sportabout   18.7  360 175 17.02\n# Duster 360          14.3  360 245 15.84\n# Cadillac Fleetwood  10.4  472 205 17.98\n# Lincoln Continental 10.4  460 215 17.82\n# Chrysler Imperial   14.7  440 230 17.42\n# Dodge Challenger    15.5  318 150 16.87\n# Camaro Z28          13.3  350 245 15.41\n# Pontiac Firebird    19.2  400 175 17.05\n# Ford Pantera L      15.8  351 264 14.50\n# Maserati Bora       15.0  301 335 14.60\n\n\nThat seems to work. (Deadly words!) But what if my finger had slipped when I typed 500?\n\nsubset2 <- function(df, col_f, row_f) {\n  df <- df[row_f, col_f(df)]\n  df[row_f(df), ]\n}\nsubset2(mtcars, \\(x) colSums(x) > 5000, \\(x) rowSums(x) > 500)\n\n# Error in xj[i]: invalid subscript type 'closure'\n\n\nWhat happened? Only one column of mtcars, disp, has a column sum greater than 5000. And what happens if you select a single column with array-style indexing?\n\nmtcars[, \"disp\"]\n\n#  [1] 160.0 160.0 108.0 258.0 360.0 225.0 360.0\n#  [8] 146.7 140.8 167.6 167.6 275.8 275.8 275.8\n# [15] 472.0 460.0 440.0  78.7  75.7  71.1 120.1\n# [22] 318.0 304.0 350.0 400.0  79.0 120.3  95.1\n# [29] 351.0 145.0 301.0 121.0\n\n\nR helpfully simplifies to an atomic vector. We can fix our function by disabling this behavior:\n\nsubset3 <- function(df, col_f, row_f) {\n  df <- df[, col_f(df), drop = FALSE]\n  df[row_f(df), ]\n}\nsubset3(mtcars, \\(x) colSums(x) > 5000, \\(x) rowSums(x) > 500)\n\n# numeric(0)\n\n\nor, even more sensibly, using list subsetting (single brackets, no comma), which never simplifies.\nThis behavior isn’t indefensible. It’s consistent with how subsetting works on arrays (which are usually atomic vectors). In interactive use, it’s convenient, since then you’re usually interested in the data a column contains, not the object containing it. But automatic simplification is easily missed and potentially destructive, and the way to avoid it can be found only if you carefully read the documentation.\n\n\nBrevity is the Soul of Bugs\nSuppose you have the following vector:\n\nx <- c(1, 4, 7, NA, -9, NA)\n\nR is strict about missing values, but not about logical constants. T and F can be used as abbreviations for TRUE and FALSE, respectively. The following is a valid way of taking the mean:\n\nmean(x, na.rm = T)\n\n# [1] 0.75\n\n\nLikewise, with F for FALSE:\n\nmtcars[1:5, \"cyl\", drop = F]\n\n#                   cyl\n# Mazda RX4           6\n# Mazda RX4 Wag       6\n# Datsun 710          4\n# Hornet 4 Drive      6\n# Hornet Sportabout   8\n\n\nWhat’s the harm in this? While TRUE and FALSE are reserved words, the abbreviations aren’t. Let’s say your colleague creates a variable T, making sure to use uppercase to avoid masking the t function:\n\nT <- pt(2, df = 10)\n\nThis code now fails in a confusing way:\n\nmean(x, na.rm = T)\n\n# [1] NA\n\n\nThe reason for this feature, as before, is clear: it’s convenient in interactive use. The problem with it is equally clear: it’s suicidal in programmatic use.\n\n\nConclusion\nThe theme here is obvious: features that save a few keystrokes in interactive use can cause maddening bugs if carelessly used in production code. You need familiarity with the language and some degree of vigilance to avoid the pitfalls, and everyone slips now and again.\nThe longer I’ve spent with R, the more convinced I’ve become that R has outgrown these features. R was designed as an environment for interactive data exploration, statistical testing, and graphical displays, but today it can do so much more: serve Web apps, query remote databases, render just about any document (even this one) with Rmarkdown, and many other uses. But to fulfill these sophisticated use cases, you have to carefully avoid traps like the ones discussed here. That fact has probably spurred some organizations to use Python instead, since it lacks these quirks. So R’s design emphasis on interactivity may limit its growth.\nMoreover, the benefits these features deliver are scant. The three behaviors I describe - partial argumetn matching, logical abbreviations, and drop = FALSE save a bit of typing (or, in the last case, an extra step of data manipulation). That doesn’t balance the potential harm they can cause in production code, especially when modern IDEs (and Vim or Emacs, of course) support autocompletion, obviating the need for abbreviated code.\nDon’t get me wrong. R remains a powerful, expressive language built on solid design principles. It’s my first choice for any kind of data manipulation, and I still find it fun and satisfying to use. But some of its behaviors are more at home in its past than its future."
  },
  {
    "objectID": "posts/ghost-machine/index.html",
    "href": "posts/ghost-machine/index.html",
    "title": "Ghost in the Machine: The Remnant of R’s Past That Haunts it Still",
    "section": "",
    "text": "But as many users know, its roots go back further. R was developed from the language S, created in the 1970s by a team led by John Chambers at Bell Labs. Those were the glory days of Bell Labs, when the language C and the Unix ecosystem were developed. Like a modern palace built on the foundations of an ancient one, R bears many traces of its lineage. Syntax is very similar, many features are backward-compatible, and the documentation for some functions even refers to resources about S rather than R. (Try ?sum, for one example).\n(I can’t help but pause here to relay the account the linked presentation gives of R’s origins. It all began with this hallway conversation between Ross Ihaka and Robert Gentleman in the University of Auckland around 1990):\n\nGentleman: “Let’s write some software.”\nIhaka: “Sure, that sounds like fun.”\n\nOne of those traces, harder to observe but certainly still present, is also one of R’s most unusual (and, in some quarters, derided) features: an emphasis on convenience in interactive use. Interpreted languages typically support interactivity in some way, since the ability to run a snippet of code and instantly get results is one of their greatest advantages over compiled languages. But S was designed primarily for interactive data exploration, and R has retained that capability as a design focus. In areas great and small, from core design choices to implementation quirks, R makes it as easy as possible to bang out code in the console and see what happens. That makes it a fast, flexible tool for exploring data and following hunches. It also strews mines in the path of anyone programming in the language without detailed knowledge of the language’s nuances.\nA few examples will make this painfully clear.\nPartial Matching, Complete Headache\nCan you spot the problem with this call? It runs correctly:\n\nrep(1:3, length = 10)\n\n--  [1] 1 2 3 1 2 3 1 2 3 1\n\n\nbut is missing something. The relevant argument of rep is actually called length.out, not length, but R’s partial argument matching saves us, since length is a shortened form of length.out.\nThis is nice to have when typing code in the console. But relying on partial argument matching in scripts is a very bad idea.\nSuppose you’re working with a package that includes some functions with annoyingly long argument names. All that typing is annoying, so you decide you may as well save some keystrokes:\n\nfoo <- function(xyzabc = 0, abcxyz) {\n  rnorm(100, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n--   [1]  0.894357887 -1.844374861 -0.057621129\n--   [4]  2.489996875 -1.390795420  0.884537644\n--   [7] -0.736702007  2.614450390  3.553828645\n--  [10]  1.971025220 -0.224352924  2.209807108\n--  [13] -2.688748490  0.390007174 -0.459719446\n--  [16]  5.186112600  1.013876138 -0.993730299\n--  [19]  0.945104237  5.154313158 -0.941837192\n--  [22]  0.581707600 -1.635026378  0.190878346\n--  [25] -1.083736267  0.023979477  4.286030420\n--  [28] -1.585059762  1.957567064 -1.928036391\n--  [31] -3.545022250 -1.446996540 -1.541110758\n--  [34]  1.993593082  3.715675773  2.000841748\n--  [37] -1.873934830 -1.890349558 -0.097384415\n--  [40] -3.481766031  1.132364388 -1.583196879\n--  [43] -0.912385824  1.505920137 -2.840027103\n--  [46] -1.180822399  0.897039349 -0.009281036\n--  [49] -2.886727947  2.447976296 -3.033588784\n--  [52] -3.019260325  5.314555238  0.887740827\n--  [55]  3.846252588 -0.130384525 -0.746065603\n--  [58] -0.821264902 -2.552182320  0.148327060\n--  [61] -2.492372378 -0.611211331 -2.723340447\n--  [64]  1.211997811 -0.360574047  1.797332645\n--  [67] -2.053911560  2.322585094  0.442166573\n--  [70] -0.201033272  4.197927479 -1.000098288\n--  [73]  3.668802740 -1.220602596 -2.441038643\n--  [76]  1.190559651 -0.285751687  0.767513992\n--  [79] -3.340677153 -0.393842469 -0.187860198\n--  [82]  2.160603057 -0.431207092 -2.096432489\n--  [85]  0.406328261  1.622388487  1.129502127\n--  [88]  1.753481684 -1.009753512  1.744162116\n--  [91] -0.151472777  1.206764972  0.113557151\n--  [94]  6.040184620  0.369281852 -1.088381654\n--  [97]  2.319542171 -3.494846607 -0.113984682\n-- [100]  2.180527674\n\n\nAll seems well. But then a version update adds a new argument:\n\nfoo <- function(abcabc = 100, xyzabc = 0, abcxyz) {\n  rnorm(abcabc, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n-- Error in foo(abc = 2): argument 1 matches multiple formal arguments\n\n\nR throws an error, unable to find an unambiguous match. (Imagine how painful this would be to debug if R defaulted to the first match instead). The way to avoid this scenario is simple: never rely on partial argument matching in permanent code. Nonetheless, many packages do. You can identify offenders yourself by setting the warnPartialMatchArgs option:\n\noptions(warnPartialMatchArgs = TRUE)\nfoo <- function(xyzabc = 0, abcxyz) {\n  rnorm(100, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n-- Warning in foo(abc = 2): partial argument match of\n-- 'abc' to 'abcxyz'\n\n\n--   [1]  0.89927010  1.95084102  2.00920129\n--   [4]  1.61525383  0.25029819 -4.22472437\n--   [7] -0.36577019  0.13838598 -0.45283263\n--  [10] -0.04457963  0.51786909 -2.08226449\n--  [13] -5.81286673 -1.85960050 -0.91836202\n--  [16]  1.28842869  0.93739761 -0.40296221\n--  [19] -3.18824496 -1.18929808  2.86126465\n--  [22]  0.40418551 -3.26925891  5.86206214\n--  [25]  1.17047473  2.04994829  0.82386204\n--  [28]  2.54901518  1.19740996  0.14051902\n--  [31] -0.28186686 -0.81828303 -3.26746608\n--  [34] -2.16554170  1.11390540  2.40128047\n--  [37] -1.58957651 -0.11426091  2.34353576\n--  [40]  2.32447372 -2.92399568 -0.31075393\n--  [43] -2.57885466  0.64934265  0.99541747\n--  [46] -1.02499388  2.32220784 -2.29705189\n--  [49]  0.11963764  1.44345399  0.15556081\n--  [52]  1.12520365 -1.03919365  1.33284974\n--  [55] -0.90857987  0.07369093 -1.57326072\n--  [58]  2.65784194 -2.55138398  5.14828131\n--  [61] -0.49967791  2.33709746  2.98066646\n--  [64]  0.18025854 -4.35551205  1.15956247\n--  [67]  2.08037060 -1.77100278  0.27069095\n--  [70]  2.71998793  0.95583104 -0.34719505\n--  [73]  0.97637745  0.28615076 -4.01965934\n--  [76]  0.10166825 -0.56836715 -2.10260226\n--  [79] -2.36087304  2.94449081  3.04928522\n--  [82] -0.91322354  0.63199603 -4.82045864\n--  [85]  0.71261788  0.87445172 -1.78421828\n--  [88] -0.50252237 -1.65478570 -0.10393510\n--  [91] -4.64486955  0.06464701  0.66081759\n--  [94] -0.46445960 -0.32894655 -0.43112524\n--  [97]  1.12930747  0.49027199  0.94456248\n-- [100] -2.44620006\n\n\nWhen Simplification Complicates\nR is an example of a weakly typed language with dynamic typing. That means data types are known only at runtime, not before, and that the language will try to coerce disparate types to a common type instead of throwing an error. That means the interpreter will happily run code like\n\npaste(mtcars, 1)\n\n--  [1] \"c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26, 30.4, 15.8, 19.7, 15, 21.4) 1\"                    \n--  [2] \"c(6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4) 1\"                                                                                                            \n--  [3] \"c(160, 160, 108, 258, 360, 225, 360, 146.7, 140.8, 167.6, 167.6, 275.8, 275.8, 275.8, 472, 460, 440, 78.7, 75.7, 71.1, 120.1, 318, 304, 350, 400, 79, 120.3, 95.1, 351, 145, 301, 121) 1\"                       \n--  [4] \"c(110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 205, 215, 230, 66, 52, 65, 97, 150, 150, 245, 175, 66, 91, 113, 264, 175, 335, 109) 1\"                                                     \n--  [5] \"c(3.9, 3.9, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92, 3.07, 3.07, 3.07, 2.93, 3, 3.23, 4.08, 4.93, 4.22, 3.7, 2.76, 3.15, 3.73, 3.08, 4.08, 4.43, 3.77, 4.22, 3.62, 3.54, 4.11) 1\"                  \n--  [6] \"c(2.62, 2.875, 2.32, 3.215, 3.44, 3.46, 3.57, 3.19, 3.15, 3.44, 3.44, 4.07, 3.73, 3.78, 5.25, 5.424, 5.345, 2.2, 1.615, 1.835, 2.465, 3.52, 3.435, 3.84, 3.845, 1.935, 2.14, 1.513, 3.17, 2.77, 3.57, 2.78) 1\"  \n--  [7] \"c(16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20, 22.9, 18.3, 18.9, 17.4, 17.6, 18, 17.98, 17.82, 17.42, 19.47, 18.52, 19.9, 20.01, 16.87, 17.3, 15.41, 17.05, 18.9, 16.7, 16.9, 14.5, 15.5, 14.6, 18.6) 1\"\n--  [8] \"c(0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1) 1\"                                                                                                            \n--  [9] \"c(1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1) 1\"                                                                                                            \n-- [10] \"c(4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 5, 4) 1\"                                                                                                            \n-- [11] \"c(4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2, 2, 4, 2, 1, 2, 2, 4, 6, 8, 2) 1\"\n\n\npaste just coerces everything to character, no matter how ludicrous the results. This behavior can trip you up, but it’s not truly insidious.\nUnfortunately, R sometimes changes types under your nose. Suppose we write a function, subset2. It takes as argument a data frame, and two functions that take a data frame as argument. It filters the data column-wise using col_f, then rowwise using row_f.\n\nsubset2 <- function(df, col_f, row_f) {\n  df <- df[, col_f(df)]\n  df[row_f(df), ]\n}\nsubset2(mtcars, \\(x) colSums(x) > 500, \\(x) rowSums(x) > 500)\n\n\n\n  \n\n\n\nThat seems to work. (Deadly words!) But what if my finger had slipped when I typed 500?\n\nsubset2 <- function(df, col_f, row_f) {\n  df <- df[row_f, col_f(df)]\n  df[row_f(df), ]\n}\nsubset2(mtcars, \\(x) colSums(x) > 5000, \\(x) rowSums(x) > 500)\n\n-- Error in xj[i]: invalid subscript type 'closure'\n\n\nWhat happened? Only one column of mtcars, disp, has a column sum greater than 5000. And what happens if you select a single column with array-style indexing?\n\nmtcars[, \"disp\"]\n\n--  [1] 160.0 160.0 108.0 258.0 360.0 225.0 360.0\n--  [8] 146.7 140.8 167.6 167.6 275.8 275.8 275.8\n-- [15] 472.0 460.0 440.0  78.7  75.7  71.1 120.1\n-- [22] 318.0 304.0 350.0 400.0  79.0 120.3  95.1\n-- [29] 351.0 145.0 301.0 121.0\n\n\nR helpfully simplifies to an atomic vector. We can fix our function by disabling this behavior:\n\nsubset3 <- function(df, col_f, row_f) {\n  df <- df[, col_f(df), drop = FALSE]\n  df[row_f(df), ]\n}\nsubset3(mtcars, \\(x) colSums(x) > 5000, \\(x) rowSums(x) > 500)\n\n-- numeric(0)\n\n\nor, even more sensibly, using list subsetting (single brackets, no comma), which never simplifies.\nThis behavior isn’t indefensible. It’s consistent with how subsetting works on arrays (which are usually atomic vectors). In interactive use, it’s convenient, since then you’re usually interested in the data a column contains, not the object containing it. But automatic simplification is easily missed and potentially destructive, and the way to avoid it can be found only if you carefully read the documentation.\nBrevity is the Soul of Bugs\nSuppose you have the following vector:\n\nx <- c(1, 4, 7, NA, -9, NA)\n\nR is strict about missing values, but not about logical constants. T and F can be used as abbreviations for TRUE and FALSE, respectively. The following is a valid way of taking the mean:\n\nmean(x, na.rm = T)\n\n-- [1] 0.75\n\n\nLikewise, with F for FALSE:\n\nmtcars[1:5, \"cyl\", drop = F]\n\n\n\n  \n\n\n\nWhat’s the harm in this? While TRUE and FALSE are reserved words, the abbreviations aren’t. Let’s say your colleague creates a variable T, making sure to use uppercase to avoid masking the t function:\n\nT <- pt(2, df = 10)\n\nThis code now fails in a confusing way:\n\nmean(x, na.rm = T)\n\n-- [1] NA\n\n\nThe reason for this feature, as before, is clear: it’s convenient in interactive use. The problem with it is equally clear: it’s suicidal in programmatic use.\nConclusion\nThe theme here is obvious: features that save a few keystrokes in interactive use can cause maddening bugs if carelessly used in production code. You need familiarity with the language and some degree of vigilance to avoid the pitfalls, and everyone slips now and again.\nThe longer I’ve spent with R, the more convinced I’ve become that R has outgrown these features. R was designed as an environment for interactive data exploration, statistical testing, and graphical displays, but today it can do so much more: serve Web apps, query remote databases, render just about any document (even this one) with Rmarkdown or Quarto, and many other uses. But to fulfill these sophisticated use cases, you have to carefully avoid traps like the ones discussed here. Some organizations have no doubt avoided the problem by switching to Python. So R’s design emphasis on interactivity may limit its growth.\nMoreover, the benefits these features deliver are scant. The three behaviors I describe - partial argument matching, logical abbreviations, and drop = FALSE save a bit of typing (or, in the last case, an extra step of data manipulation). That doesn’t balance the potential harm they can cause in production code, especially when modern IDEs (and Vim or Emacs, of course) support autocompletion, obviating the need for abbreviated code.\nDon’t get me wrong. R remains a powerful, expressive language built on solid design principles. It’s my first choice for any kind of data manipulation, and I still find it fun and satisfying to use. But some of its behaviors are more at home in its past than its future."
  },
  {
    "objectID": "posts/lesser-known/index.html",
    "href": "posts/lesser-known/index.html",
    "title": "R Tricks I Wish I’d Known as a Beginner",
    "section": "",
    "text": "R is full of quirks, some of them obscure. Getting the most out of the language takes some experience, but is well worth the effort. These techniques will be old hat to seasoned R users, but you never know: you might still learn something."
  },
  {
    "objectID": "posts/lesser-known/index.html#get-the-expressions-passed-as-function-arguments",
    "href": "posts/lesser-known/index.html#get-the-expressions-passed-as-function-arguments",
    "title": "R Tricks I Wish I’d Known as a Beginner",
    "section": "Get the Expressions Passed as Function Arguments",
    "text": "Get the Expressions Passed as Function Arguments\nR passes function arguments by value, not by reference, yet it’s possible to recover the symbol or expression passed to a function using this trick:\n\nf <- function(x) {\n  x <- deparse(substitute(x))\n  print(x)\n}\nf(`I'm a symbol!`)\n\n-- [1] \"I'm a symbol!\"\n\n\nsubstitute, when called in a function, replaces its argument with the expression in the promise corresponding to that argument. (Promises are internal objects that implement function arguments). deparse converts that unevaluated R code into a character vector.\nThis could be used to make a function that automatically labels plot axes:\n\ndescriptive_plot <- function(x, y) {\n  x_name <- deparse(substitute(x))\n  y_name <- deparse(substitute(y))\n  plot(x, y, xlab = x_name, ylab = y_name)\n}\nweight <- mtcars$wt\nmpg <- mtcars$mpg\ndescriptive_plot(mpg, weight)\n\n\n\n\n\n\n\nWhat are your favorite R tricks?"
  },
  {
    "objectID": "posts/R6-active/index.html",
    "href": "posts/R6-active/index.html",
    "title": "Programatically Creating Accessor Functions for R6 Objects",
    "section": "",
    "text": "library(R6)\n\nunprotected <- R6Class(\n  classname = \"unprotected\",\n  public = list(foo = 1, bar = 2, baz = 3)\n)\nexample <- unprotected$new()\nexample$foo\n\n-- [1] 1\n\nexample$foo <- 2\nexample$foo\n\n-- [1] 2\n\n\nFields can be protected by sending them to private instead, but that blocks the user from accessing them. The solution is to create an active field. This creates an active binding: a special form of R function that can be used to return a value if called with no arguments and to bind a value if called with one. We can use this capability to create an accessor function that blocks users from changing values:\n\nprotected <- R6Class(\n  classname = \"example\",\n  public = list(\n    bar = 2, baz = 3,\n    initialize = function(foo) private$.foo <- foo\n  ),\n  private = list(.foo = NULL),\n  active = list(foo = function(value) {\n    if (missing(value)) {\n      return(private$.foo)\n    } else {\n      stop(\"Hands off!\")\n    }\n  })\n)\n\nexample <- protected$new(foo = 1)\nexample$foo\n\n-- [1] 1\n\nexample$foo <- 2\n\n-- Error in (function (value) : Hands off!\n\n\n(See chapter 14 of Advanced R for more details).\nThis is all simple enough, but there’s an obvious problem: what if we have a lot of attributes to protect? We could dodge the problem by combining them into a single list attribute, or just copy-paste the same function with different attribute names. But those options aren’t always attractive. I recently confronted this problem while working on an elaborate subclass of torch::dataset, which organizes data for neural networks. I decided to rifle through my bag of functional programming tricks in search of a solution.\nFirst Attempt: Function Factory\nSince each active field requires a function, a function factory was an obvious approach. It’s simple to implement:\n\naccessor_factory <- function(field) {\n  force(field)\n  function(value) {\n    if (missing(value)) {\n      return(private[[\"field\"]])\n    } else {\n      stop(\"Hands off \", field, \"!\")\n    }\n  }\n}\n\n(The real version used a less jocular error message, but I need to have my fun somehow). Because R has lexical scope, field is bound in the manufactured function’s enclosing environment, so when executed it should look there and find it.\nBut it doesn’t work.\n\nprotected <- R6Class(\n  classname = \"example\",\n  public = list(\n    bar = 2, baz = 3,\n    initialize = function(foo) private$.foo <- foo\n  ),\n  private = list(.foo = NULL),\n  active = list(foo = accessor_factory(\".foo\"))\n)\nexample <- protected$new(1)\nexample$foo\n\n-- NULL\n\nexample$foo <- 2\n\n-- Error in lapply(list(...), as.character): object 'field' not found\n\n\nEither R core sneaked support for dynamic scope into the last major version, or the R6Class constructor was doing something funny. Checking the source code found the offending line:\n\ngenerator_funs <- assign_func_envs(generator_funs, generator)\n\nThe constructor modified the environments of function fields (a trick I also resorted to while writing a different subclass, but that’s another story). Relying on scope wouldn’t help, but what would?\nSecond Attempt: as.function\n\nMy next idea was to use R’s obscure but powerful function constructor, as.function. It has a strange implementation: it takes a list, interpreting all elements except the last as name-value pairs for arguments (with an empty value slot designating an argument with no default). The last element should be an expression defining the function body. This is what I wrote:\n\naccessor_factory <- function(field) {\n  force(field)\n  code <- substitute(\n    {\n      if (missing(value)) {\n        return(private[[field]])\n      } else {\n        stop(sQuote(field), \" is read-only\")\n      }\n    },\n    list(field = field)\n  )\n  as.function(eval(substitute(\n    alist(value = , code),\n    list(code = code)\n  )),\n  envir = globalenv()\n  )\n}\n\nThis code demands some explanation. The idea is to return a function with the value of field already substituted, not set at runtime. The first step uses substitute to replace the symbol field with the value passed to the function (i.e., the name of the target attribute). The result forms the body of the manufactured function. I have to call substitute again to substitute this expression into the call to alist passed to as.function, because alist quotes its arguments. That expression actually creates the function we need. (See why most people consider me weird for liking metaprogramming?).\n\nprotected <- R6Class(\n  classname = \"example\",\n  public = list(\n    bar = 2, baz = 3,\n    initialize = function(foo) private$.foo <- foo\n  ),\n  private = list(.foo = NULL),\n  active = list(foo = accessor_factory(\".foo\"))\n)\nexample <- protected$new(1)\nexample$foo\n\n-- [1] 1\n\nexample$foo <- 2\n\n-- Error in (function (value) : '.foo' is read-only\n\n\nThis works. But can we do better?\nThird Attempt: Body Substitution\nR features assignment functions to modify all three parts of a closure: formal arguments, body, and environment. We’re interested in creating a set of functions with slightly different bodies, so pairing body<- with substitute is a natural approach. It’s a lot more readable than my last attempt, too. The classic double-substitute trick for substituting the result of an expression comes from Advanced R.\n\nsubstitute_body <- function(fn, mapping) {\n  body(fn) <- eval(substitute(substitute(temp, mapping), list(temp = body(fn))))\n  fn\n}\n\ntemplate <- function(value) {\n  if (missing(value)) {\n    return(private[[field]])\n  } else {\n    stop(sQuote(field), \" is read-only\")\n  }\n}\nsubstitute_body(template, mapping = list(field = \"test\"))\n\n-- function (value) \n-- {\n--     if (missing(value)) {\n--         return(private[[\"test\"]])\n--     }\n--     else {\n--         stop(sQuote(\"test\"), \" is read-only\")\n--     }\n-- }\n\n\nVictory! Well, almost. To make this truly useful, we need a wrapper function to create a list of accessors from field names. Thankfully, that’s much easier than figuring out the substitution.\n\nset_active_fields <- function(fields) {\n  out <- lapply(fields, function(x) {\n    substitute_body(\n      fn = template,\n      mapping = list(field = x)\n    )\n  })\n  names(out) <- gsub(\"^\\\\.\", \"\", fields)\n  out\n}\n\nA bog-standard use of lapply does the job, with the annoying complication of removing leading dots from the names of private fields.\nWe can even go one step further and write a wrapper to R6Class to automatically create accessors from a list of private attributes.\n\nwith_accessors <- function(classname = NULL,\n                           public,\n                           private,\n                           inherit = NULL, lock_objects = TRUE,\n                           class = TRUE,\n                           portable = TRUE, lock_class = FALSE,\n                           cloneable = TRUE,\n                           parent_env = (function() parent.frame())()) {\n  force(parent_env)\n  active <- set_active_fields(names(private))\n  R6Class(\n    classname = classname, public = public,\n    private = NULL, active = active,\n    inherit = inherit, lock_objects = lock_objects,\n    class = class,\n    portable = portable,\n    lock_class = lock_class,\n    cloneable = cloneable,\n    parent_env = parent_env\n  )\n}\n\n\npublic <- list(initialize = function(foo) {\n  private$.foo <<- foo\n})\nprivate <- list(.foo = NULL, .bar = 2, .baz = 3)\nprotected <- with_accessors(\"example\", public = public, private = private)\n\nexample <- protected$new(foo = 1)\nexample$foo\n\n-- [1] 1\n\nexample$bar\n\n-- [1] 2\n\nexample$baz\n\n-- [1] 3\n\nexample$foo <- 2\n\n-- Error in (function (value) : '.foo' is read-only\n\nexample$baz <- 5\n\n-- Error in (function (value) : '.baz' is read-only\n\n\nNote that because of the indirection, I have to use <<- in initialize. I also have to make parent_env the execution environment of the wrapper, which is the caller environment of R6Class here. There may also be other nasty surprises buried in this use of reference semantics. Still, this was a fun diversion, and proof of how much power R grants the user over environments and evaluation."
  },
  {
    "objectID": "posts/triumph-travesty/index.html",
    "href": "posts/triumph-travesty/index.html",
    "title": "Triumph and Travesty: Earning All 50 Stars in Advent of Code 2021",
    "section": "",
    "text": "If you haven’t heard of Advent of Code, it’s well worth your time to check out. Created and maintained by software engineer Eric Wastl, Advent of Code (AoC for short) is an annual event involving an advent calendar of Christmas-themed programming challenges. Anyone can participate for free, anonymously if they like. A new puzzle is released on each of the first 25 days of December. They start simple and gradually increase in difficulty. Elite players compete for spots on the official leaderboard of the fastest solutions, but most (myself included) just aim to solve the puzzles. Each puzzle(with one exception) awards two “gold stars” when completed, providing a way to track your progress.\nThe puzzles themselves take the form of well-posed problems, connected through a whimsical yuletide narrative. This year’s edition sent players to the ocean depths in a submarine to retrieve the lost keys to Santa’s sleigh. Along the way, they encountered treacherous currents, labyrinthine caves, and a whole lot of obstreperous sea creatures - all of which could only be overcome with some creative programming (Half the fun is recognizing the classical computer science problems underneath the intentonally silly presentation). Each puzzle consists of two parts. The first part states the problem, with any necessary rules, and offers a plaintext input (randomly generated for each player) to work from. If the player submits the correct answer, they receive a gold star…and updated instructions with a new version of the problem to solve. It usually adds a new constraint or asks the player to interpret the input in a different way; depending on the problem and the player’s approach for part 1, overcoming it could take anything from changing a single line to starting from scratch. Submitting the correct answer for the second part earns another gold star and completes that day’s puzzle. (The lone exception to the standard format is the Christmas Day puzzle, which differs in a way I won’t spoil). Players can use whatever language and strategy they like; some solve puzzles in absurd (or do I mean awesome) languages like Rockstar, or impose tough constraints, because they can.\nThe puzzles test a wide variety of programming techniques, from recursion to graph traversal to regular expressions. The problem statements are all “fair” - there are no hidden rules or lawyerly gotchas - but even a subtle misunderstanding can cost you hours of frustrating debugging (just like real life!). With no constraints and no expectation to write production-quality code, you’re free to tackle each problem as you see fit, limited only by your knowledge and creativity.\nI stumbled across AoC in late 2020, a pivotal time in my life. Perhaps a month before, realizing I liked programming a lot more than policy analysis, I had decided to convert my masters degree from public policy to data science. With enough experience in R to feel (over)confident in my programming skills, I dove in without hesitation and spent much of that holiday break absorbed in the puzzles. Tackling such beautifully abstract problems, with no pressure and no shame in failing, was bliss; I enjoyed even the frustration. Realizing R was ill-suited for many of the puzzles, I switched to Python, learning it as I went. I only ever solved some of the puzzles, and those in amateurish fashion (look here if you’re morbidly curious), but I became a much better programmer for it. Having had so much fun, I resolved to come back next year truly prepared.\nWhen December 2021 came, I threw myself into the puzzles. (I probably should have spent more time studying for exams instead, but this questionable time allocation thankfully didn’t hurt my GPA). The first few days came easily, aside from day 3, for which I kludged together an overcomplicated solution involving bitshifting. I switched between R and Python, preferring R for problems involving matrices and similar structures and Python where iteration was emphasized. Once again, I learned plenty along the way: queues for day 6, optimization for day 7, complex numbers as coordinates for day 11. For longer than I expected, I managed not to fall a day behind.\nBut that couldn’t last. I got badly stuck on part 2 of day 14 (which was not a hard problem, in hindsight). The end came on day 15, a tough problem involving graph traversal. I stalled out after hours of work, until a post on the subreddit pointed me toward Dijkstra’s algorithm. After writing probably the worst implementation of all time and letting my computer chug along for about an hour, I claimed both gold stars. But I had almost burnt myself out. The remaining puzzles (aside from a few easier “breather” problems) seemed impossible, and I ceased trying to keep up. Determined to keep going, I gutted my way through day 16, completing it only after spending hours looking in the wrong places for a simple bug. I knew then I had to stop.\nI had done better than I had expected; 50 stars seemed within reach. After taking a few days off, I knocked out a few of the easier puzzles, leaving thirty-odd stars secured. But then the spring semester started, depriving me of free time. Somehow, I still managed to complete the very challenging day 24, guided by a kind user on the Python discord. After that, as the holidays became a distant memory, Advent of Code fell to the bottom of my priorities.\nThat is, until I graduated. Without a job, and itching to work on something that didn’t involve complex data manipulation, I picked up where I had left off. The first puzzles fell with surprising ease: day 18, after some crude but effective string processing; day 19, after browsing the subreddit for tips; and even part 2 of day 21, completed after I spent half an hour fiddling with code I hadn’t touched for five months (when does that ever happen?). Day 22 stumped me for a while, so I asked for advice on the subreddit and followed a set-theory approach that ended up yielding a very elegant solution. That left just day 23: finding the optimal strategy for a simple puzzle that would be very, very hard to solve programatically. I dimly remembered some post on the subreddit recommending the A* algorithm. Knowing it was always smart to work out the problem with pen and paper before writing any code, I sketched out a game board, cut up some sticky notes to use as tokens, and set to work. I solved part 1 easily enough this way, so for the hell of it I tried again on part 2, which posed the same problem on an expanded game board. A few failed attempts later, I nearly gave up; I had learned the hard way how easily “just one more try” turns into a few hours of futile coding. But that time, I didn’t. When I entered my answer, I saw for the last time the familiar message:\nYou have solved part 2 of this puzzle! It provides one gold star.\nIt came as an anticlimax; I would not have to code that A* nightmare after all. Perversely, I felt cheated. Maybe I had cheated. The official description of Advent of Code entreats you to solve puzzles in “any programming language you like,” after all. Was I violating the spirit of the event by avoiding a programmatic solution entirely? Perhaps. The thought nags at me, so I suspect I’ll come back to this problem eventually, when I’m more confident in graph traversal algorithms. But still, I had all 50 stars, a feat that had seemed impossible just a year before.\nViewed one way, this is a trivial achievement: writing throwaway code to solve toy problems invented to kill time over in the weeks before Christmas. Viewed another way, it’s legitimately impressive. I solved all 25 of these puzzles in the time I could spare, just to sharpen my skills and indulge my love of the art of programming. I think it’s enough to say that grad students my age have found worse diversions. Either way, I emphasize that I had plenty of help: people on the subreddit and other forums to guide me, tutorials to consult, and above all the knowledge that many other people persevered through the same frustrations and got to 50 stars.\nI’ll probably be back next year, of course. I’ll have a lot less time to devote, since I expect to have a job by then. I don’t know if I’ll grind out all 50 stars again, now that I’ve done it already. But I do know that any time I spend on Advent of Code won’t be wasted, and I’ll be a better programmer for it.\nI just hope there aren’t as many graphs this time.\nMy repository for Advent of Code 2021: (https://github.com/ryan-heslin/AoC2021)[https://github.com/ryan-heslin/AoC2021]"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Hello, world!",
    "section": "",
    "text": "Those goals are:\n\nEngage with the wider R community.\nShare some of the R esoterica I’ve learned\nLearn a web framework, as a first step toward more sophisticated web design\nPractice writing, which I find satisfies me the same way coding does\n\nI plan to mostly write about R and other data science topics, though I might broaden scope later on. If you’ve found your way here, I hope at least you’re entertained for a little while."
  }
]