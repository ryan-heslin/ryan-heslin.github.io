[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome\nWelcome to my blog! My name is Ryan Heslin, and I recently graduated from American University with a master’s in data science. While I work in Python and a few other languages, my main (and favorite) is R. For now, I’ll devote this blog to my musings (or ramblings, if you feel uncharitable) on R and other topics related to data science.\n\n\nWhy the Name?\nNaming things really is as hard as cache invalidation. I spent a long time casting about for something to call the site before hitting on Verso. It means the reverse side of a physical document, such as the left-hand page of a book. The word attracted me because I aim to make this blog like a good anthology: you can visit any page and find something short but worth reading. As a publishing term derived from Italian, it has a pleasingly archaic resonance similar to “quarto”, the namesake of the file format this site’s documents are written in. Most important, it’s got an R.\nVerso was built with Quarto and deployed using Github Pages with a standard Quarto GitHub workflow. A previous version of this site was built with rendered Rmarkdown and the blogdown R package."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Verso",
    "section": "",
    "text": "Completing Advent of Code\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 17\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 20\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 21\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 22\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 24\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 25\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 23\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 12\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 19\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 14\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 10\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 13\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 10\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 10\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 10\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code Day 11\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 9\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 8\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 7\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 5\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 6\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 4\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 2\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2023 Day 3\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nAdvent Time Again\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT and the Advent of Code Leaderboard\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nWhy Linear Algebra is Useful\n\n\n\n\n\n\n\nMiscellaneous\n\n\nMath\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code So Far\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nRenaming Things in R\n\n\n\n\n\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nWhy Quarto?\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nThe Unexpected Pleasures of Naive Code\n\n\n\n\n\n\n\nMiscellany\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nErica Albright was Right\n\n\n\n\n\n\n\nMiscellany\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nCompleting Advent of Code\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT Has a Long Way to Go\n\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nWhat a Difference Two Years Makes\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nLexical Scope: Who Does It Best?\n\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nReflections on Advent of Code 2022\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nWhy I Use Neovim\n\n\n\n\n\n\n\nNeovim\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nIs R Hard to Learn?\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nWhat I Wish I’d Known Going into Advent of Code\n\n\n\n\n\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nThe Data Science Language of the Future\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nChasing A*: Completing Advent of Code 2021, Once and For All\n\n\n\n\n\n\n\nAdvent of Code\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nDon’t Neglect Unit Testing\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nThat Kind of a Day\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nTriumph and Travesty: Earning All 50 Stars in Advent of Code 2021\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nAdvent of Code\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nProgramatically Creating Accessor Functions for R6 Objects\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nThe Best Data Visualization of All Time\n\n\n\n\n\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nR Tricks I Wish I’d Known as a Beginner\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nGhost in the Machine: The Remnant of R’s Past That Haunts it Still\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\n  \n\n\n\n\nHello, world!\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2022\n\n\nRyan Heslin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/aoc_advice/index.html",
    "href": "posts/aoc_advice/index.html",
    "title": "What I Wish I’d Known Going into Advent of Code",
    "section": "",
    "text": "You’ve probably guessed by now that I really like Advent of Code, Eric Wastl’s annual collection of Christmas-themed programming puzzles. Even ignoring the puzzles, the experience is alluring. The silly but surprisingly elaborate narrative tying the puzzles together, the burgeoning community, the steady increase in difficulty, and the soothing blue-and-gold design of the site all combine to make the event something special. But the puzzles are the main event, so you ought to know something about them before diving in.\nWith a hundred-odd stars across three years, I thought I’d offer some advice.\nTo have an enjoyable experience, I don’t think you need anything more than familiarity with at least one programming language. You certainly don’t need formal computer science education, knowledge of math theory, or any experience with competitive programming, though all those things are helpful. You just need to know how to take a problem statement, come up with a plan to solve it, and translate that plan into code. None of that is easy, but Advent of Code is a great way to practice.\nHere are some less obvious pointers.\nBe absolutely sure you understand what is being asked.\nPuzzles often contain complicated rules with subtle but important edge cases. The text is usually careful to note these; if not, they often appear in the examples provided. Still, Wastl has remarked that, for any given sentence of the puzzle text, there is always at least one player who ignores it, to their cost. More times than I can count, I’ve missed some crucial nuance and wasted time on unworkable solutions. Don’t be like me. Don’t start solving a puzzle until you’ve resolved all your questions about how you should interpret the input, and remember that an extra minute spent skimming the puzzle text could save you an hour of fruitless coding.\nResist the temptation to pre-solve the second part.\nI’d better make this code robust to handle the second part, you tell yourself. I’ll look so clever when I solve it just by tweaking a parameter in my solution to the first part! Stop right there. The whole point of the second part is that you can’t plan for it. It could take any one of dozens of forms. It might ask you to find the highest of something instead of the lowest, remove a constraint that made the problem simpler, impose a new constraint that makes it more complicated, or even require you to use the input in a completely different way than the first part. Beyond wrapping your code in functions or classes you can reuse, you can’t prepare for all these scenarios, and you’ll only waste time if you try.\nThe exception is puzzles whose first parts tell you to ignore some part of the input. In that case, the second part is all but certain to use the full input, and the increase in scale could well break a naive but inefficient algorithm that works on the first part.\nLook out for some recurring themes\n175 puzzles in, Wastl still manages to avoid merely recycling old ones. But naturally, the same kinds of problem crop up again and again, albeit always with a fresh variation or two. In any given year, expect to see at least one appearance each of the following:\n\nEvaluating and parsing a made-up assembly language\nFinding the shortest path between a certain pair of nodes on a graph\nString manipulation, possibly involving regular expressions\nSome application of combinatorics\nA variation on Conway’s game of life\n\nPuzzles become a lot less daunting when you recognize them as new twists on familiar concepts. Moreover, these are all well-studied problems in computer science, so you can easily find advice or pseudocode for useful algorithms if you get stuck.\nBe lazy.\nEfficient code does as little work as possible. Advent of Code doesn’t expect you to aggressively optimize your code, but it does reward you for finding clever ways to avoid unnecessary computations. If you’re trying combinations of values to look for the answer, for instance, you can greatly speed up your code if you find an easy way to ignore combinations that can’t possibly yield the right answer. In a similar vein, resist the urge to just simulate a puzzle that asks for the outcome of a game or process. If you can think of a way to compute the answer directly, your code will probably run much faster. And beware: some puzzles require it. I’ve come across a few where a naive algorithm worked on the first part, but the second part changed the rules so a non-optimized approach became hopelessly slow.\nAdvent of Code isn’t intended to require dirty optimization hacks, but it does test your ability to think of subtler approaches than brute force.\nDon’t make it into work.\nAdvent of Code is, above all, fun - a chance to write code that solves silly problems about made-up Yuletide adventures. Of course, it’s also an opportunity to practice writing good code to diverse and difficult specifications, but that’s secondary. Don’t stress about efficiency, style, or refactoring; nobody is going to hound you for breaking code quality guidelines on Christmas puzzles.\nAnd if you get stuck, do something else for awhile - you’ll get that star eventually. It’s not a sign you’re dumb or bad at coding; the puzzles are devised by a clever software engineer to challenge an audience largely composed of software engineers, after all. (That doesn’t mean I’m not dumb, mind you - you just have to look elsewhere for proof of that assertion).\nIf you’re unsure whether to take part, just give it a try. I did, and the experience spurred me to learn new languages and start thinking seriously about the code I wrote. The first puzzle of each year is usually simple, and you’re not obliged to continue if you don’t enjoy it. But if you do enjoy it, there’s no telling how far your newfound interest will carry you. I hope you found this advice useful, or at least entertaining. See you in December!"
  },
  {
    "objectID": "posts/a_star/index.html",
    "href": "posts/a_star/index.html",
    "title": "Chasing A*: Completing Advent of Code 2021, Once and For All",
    "section": "",
    "text": "This post contains spoilers for the day 23 puzzle of Advent of Code 2021.\nIn an earlier post, I related my long but successful effort to obtain every last star in Advent of Code’s 2021 edition. It ended in surprising anticlimax when I solved day 23, a puzzle that looked daunting to tackle with code, using paper, pen, and some cut-up sticky notes. I could have stopped there. I should have stopped there. But good stories don’t end with an anticlimax, and the feeling that I had somehow cheated nagged at me. With plenty of free time as I hunted for a job, I decided to solve the puzzle the right way.\n\nThe Problem\nDay 23’s puzzle is a variant of the classic Towers of Hanoi. Instead of disks, the puzzle has players move different types of amphipod, in keeping with the year’s ocean theme. More importantly, the goal is to move each amphipod of each type into the correct “side room” connected to the main board, or “hall”, as efficiently as possible. The second part of the puzzle doubles the number of amphipods, making it much tougher to solve by hand.\nA post on the subreddit suggested using the A* (“A star”) algorithm. A*https://en.wikipedia.org/wiki/A*_search_algorithm) is a classic pathfinding algorithm that finds the shortest path between two given nodes of a graph. Implementations use a function called d to measure distances between nodes, and a heuristic function called h to estimate the distance between a node and the goal. The algorithm is mathematically certain to return the correct path if h never overestimates the distance to the goal.\nIn the problem at hand, the nodes were clearly game states (legal configurations of the board). Two nodes shared a connection if one could be reached from the other by a legal move (luckily, no legal moves are reversible, so the graph is acyclic — no loops are possible). The puzzle required the only the minimal cost of completing the game, not the actual sequence of moves, which further simplified things.\nStill, I faced several hard tasks:\n\nCreate a data structure capable of representing any valid game state\nImplement d (to measure distances between nodes) and h (to conservatively estimate any node’s distance from the goal).\nWrite an A_star function that used these routines to find the minimal cost\n\n\n\nInto the Fray\nAs is usual with Advent of Code, the first task was parsing the input. This was my raw input:\n#############\n#...........#\n###A#D#A#B###\n  #B#C#D#C#\n  #########\nI made the crucial decision to represent positions on the board as tuples of (x, y) coordinates. Since I was using Python, I decided to use a zero-based index, with the leftmost hall space as the x origin and the bottom side room spaces as the y origin. So the leftmost A amphipod on the board above would be located at (2, 2). I would have made my life much easier if I had used complex numbers instead of tuples of real numbers. In any case, I wrote a crude function to map each amphipod type to a set containing its positions:\n\ndef parse(inp, xmax=10):\n    stripped = inp[2 : (len(inp) - 1)]\n    ymax = len(stripped)\n    stripped.reverse()\n    stripped = list(zip(*stripped))\n    stripped = stripped[1 : (xmax + 1)]\n    mapping = {i: set() for i in range(4)}\n    for i in range(min(ends) + 2, max(ends) - 1, 2):\n        this = stripped[i]\n        for j in range(ymax):\n            val = values_map[this[j]]\n            mapping[val].add((i, j))  # Add position to set\n\n    return mapping, ymax\n\nThen I wrote a complicated function I’ll spare you. It computed, for each pair of coordinates it was legal to move between, the spaces spanning them. That way, I could allow moves only after confirming that that space wasn’t blocked.\nNext came designing an object to represent game states. It should own h and d, the second of which would take another game state as argument. I decided it should also be responsible for finding adjacent nodes and creating objects to represent them. I decided to call the class State.\nFrom here, my work only got kludgier. State ended up mapping each amphipod type to a set of the positions it occupied as well as tracking the occupants of each side room - a redundancy I couldn’t seem to avoid. From there, d and h were surprisingly simple. d would only ever be called on states that differed by the position of just one amphipod, so all I had to do was find the two coordinate tuples that disagreed in the instances’ coordinate sets, measure the distance between them, and multiply by the cost of moving the relevant amphipod type one space. h was a bit trickier, but hardly brutal — I just computed the distance from each amphipod to the target side room, a simple approach that would never underestimate the true cost.\nThe real bear turned out to be finding the valid neighbors of each instance. It took me an embarrassingly long time to figure out the rules:\n\nAmphipods in side rooms may only move out if they are in the side room of the wrong type, or if an amphipod of the wrong type is positioned behind them.\nAmphipods in side rooms that meet one or both criteria can move to any hall space to which the path is clear, or the innermost open space of their side room if it complies with rule 4.\nAmphipods in hall rooms may only move into the side room of their type, and only if a path to it is clear.\nA side room may only admit amphipods if it either contains no amphipods or only amphipods of its type.\n\nTranslating these directives into conditions was pure hell, and the result turned into pure write-only code. Here’s a representative excerpt:\n\nif coord[0] in self.sides_idx:\n    x_type = self.side_idx2type(coord[0])\n    if (x_type == k and self.sides[k][\"completed\"]) or (\n            coord[1] &lt; self.ymax - 1\n            and self.sides[x_type][\"room\"][coord[1] + 1] is not None\n            ):\n        continue\n\nSomehow, I finished it.\nThat left only the A_star function that did the real work. Translating Wikipedia’s pseudocode for the algorithm into Python was simple:\n\ndef A_star(start, goal, debug=False):\n    start_k = hash(start)\n    open_set = {start_k: start}\n\n    # For node k, node preceding it on cheapest known path to k\n    came_from = {}\n\n    # g_score[k] is cost of cheapest known path to k\n    g_score = defaultdict(lambda: inf)\n    g_score[start_k] = 0\n    # gscore[k] + k.h() - best estimate of total cost (default to infinity if node unknown)\n    f_score = defaultdict(lambda: inf)\n    f_score[start_k] = g_score[start_k] + start.h()\n\n    while open_set:\n        min_cost = inf\n        # h = hash\n        for h, node in open_set.items():\n            score = f_score[h]\n            this_cost = min(min_cost, score)\n            if this_cost &lt; min_cost and h in open_set.keys():\n                current = node\n                # print(current)\n                # print(\"\\n\")\n                if current == goal:\n                    return g_score[hash(current)]  # Cheapest cost to goal\n                min_cost = this_cost\n        current_k = hash(current)\n        current.find_neighbors()\n        if debug:\n            print(hash(current))\n            print(current)\n            print(\"-------------------\\n\")\n            for n in current.neighbors:\n                print(n)\n                print(current.d(n))\n            input(\"Continue: \")\n            print(\"\\n\\n\\n\")\n        open_set.pop(current_k)\n        for neighbor in current.neighbors:\n            # print(neighbor.neighbors)\n\n            # Distance from start to neighbor through current\n            g_score_new = g_score[current_k] + current.d(neighbor)\n            # print(f\"distance: {current.d(neighbor)}\")\n            # print(neighbor)\n            neighbor_k = hash(neighbor)\n            # This path to neighbor cheaper than any known, so record it\n            if g_score_new &lt; g_score[neighbor_k]:\n                came_from[neighbor_k] = current\n                # New estimate of cost from this neighbor\n                # Forgot this line\n                g_score[neighbor_k] = g_score_new\n                f_score[neighbor_k] = g_score_new + neighbor.h()\n                if neighbor not in open_set.values():\n                    open_set[neighbor_k] = neighbo\n\nMy only addition, naturally, was a debug mode. Then came the really hard part.\nI spent an embarrassing amount of time in the debugger getting everything to work correctly. I fell into in a dispiriting loop of scanning output for evidence of bugs, stepping through the debugger to track them down, and making painstaking changes to fix them. I came close to giving up, and several times regretted starting. Then, one fine July Monday morning, I saw the code spit out a plausible-looking answer. Not expecting success, I checked the Advent of Code website and gasped when I saw it was correct.\nI wasn’t home free; my inefficient kludge algorithm might well be too slow for the second half of the problem. I modified State to handle a larger game board, crossed my fingers, and ran the script again. It took a few minutes longer, but it spit out the correct answer for part 2. I had done it.\nI savored the feeling of blissful triumph, knowing it would not last. I might have just finished the worst implementation of A* of all time, but it was my implementation, and it solved the problem. Somehow, writing your own intricate kludge is far more satisfying then copying someone else’s elegant solution. In any case, I was at last done: I had finished all 25 puzzles for Advent of Code 2021 by myself. Perhaps an achievement in pointlessness, but an achievement nonetheless."
  },
  {
    "objectID": "posts/ghost-machine/index.html",
    "href": "posts/ghost-machine/index.html",
    "title": "Ghost in the Machine: The Remnant of R’s Past That Haunts it Still",
    "section": "",
    "text": "As programming languages go, R isn’t particularly old: its first public release came in early 2000 (see https://www.stat.auckland.ac.nz/~ihaka/downloads/Massey.pdf for more details).\nBut as many users know, its roots go back further. R was developed from the language S, created in the 1970s by a team led by John Chambers at Bell Labs. Those were the glory days of Bell Labs, when the language C and the Unix ecosystem were developed. Like a modern palace built on the foundations of an ancient one, R bears many traces of its lineage. Syntax is very similar, many features are backward-compatible, and the documentation for some functions even refers to resources about S rather than R. (Try ?sum, for one example).\n(I can’t help but pause here to relay the account the linked presentation gives of R’s origins. It all began with this hallway conversation between Ross Ihaka and Robert Gentleman in the University of Auckland around 1990):\n\nGentleman: “Let’s write some software.”\nIhaka: “Sure, that sounds like fun.”\n\nOne of those traces, harder to observe but certainly still present, is also one of R’s most unusual (and, in some quarters, derided) features: an emphasis on convenience in interactive use. Interpreted languages typically support interactivity in some way, since the ability to run a snippet of code and instantly get results is one of their greatest advantages over compiled languages. But S was designed primarily for interactive data exploration, and R has retained that capability as a design focus. In areas great and small, from core design choices to implementation quirks, R makes it as easy as possible to bang out code in the console and see what happens. That makes it a fast, flexible tool for exploring data and following hunches. It also strews mines in the path of anyone programming in the language without detailed knowledge of the its nuances.\nA few examples will make this painfully clear.\nPartial Matching, Complete Headache\nCan you spot the problem with this call? It runs correctly:\n\nrep(1:3, length = 10)\n\n--  [1] 1 2 3 1 2 3 1 2 3 1\n\n\nbut is missing something. The relevant argument of rep is actually called length.out, not length, but R’s partial argument matching saves us, since length is a shortened form of length.out.\nThis is nice to have when typing code in the console. But relying on partial argument matching in scripts is a very bad idea.\nSuppose you’re working with a package that includes some functions with annoyingly long argument names. All that typing is annoying, so you decide you may as well save some keystrokes:\n\nfoo &lt;- function(xyzabc = 0, abcxyz) {\n  rnorm(100, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n--   [1]  4.00474004 -1.27821602  1.29900596\n--   [4] -0.25402618  2.12623994 -2.07070698\n--   [7] -1.86154633 -3.59955689 -1.45326494\n--  [10]  0.42668908 -3.88395141  2.35033571\n--  [13]  2.53780029 -2.31973690 -2.15962902\n--  [16] -3.58521533 -0.49387361 -0.43639201\n--  [19]  2.37707518 -0.18366409 -2.35288133\n--  [22]  4.02017978 -0.76270196 -5.17816834\n--  [25] -1.86765373  2.18323757 -1.88491569\n--  [28]  2.11861495  1.99597302 -0.92143335\n--  [31]  2.89990969 -1.21095176 -3.24739440\n--  [34] -0.73863055 -2.13980634 -0.90604626\n--  [37] -3.28649495  0.24517599  0.57840439\n--  [40]  3.17347485  0.30699455 -0.83006642\n--  [43]  0.80333315 -0.62164082 -0.10226109\n--  [46] -0.58857382 -0.52846165  2.15047996\n--  [49] -1.99687368  3.73823522  0.05243560\n--  [52] -1.02086706 -0.47792090  3.35813506\n--  [55] -0.60337422 -0.79519643 -1.22682317\n--  [58]  0.62584765  3.13707585 -3.79529077\n--  [61]  1.97711579 -0.07767803  3.20392848\n--  [64] -6.61453906  0.84650805 -1.43290702\n--  [67] -0.58879366  0.48793729  1.58525943\n--  [70] -3.55488165 -4.82853410  0.09553368\n--  [73] -2.25119001  6.40248922 -0.92176403\n--  [76] -4.49742348  0.53547498 -0.53477097\n--  [79] -1.56163245 -2.05202139  0.07719572\n--  [82] -1.29578256  4.03163973 -0.52462044\n--  [85] -1.97150041  4.84215657 -3.99918465\n--  [88] -0.89209597 -6.06083829 -3.43076333\n--  [91]  0.97269597  0.38725233  0.54526798\n--  [94] -0.46413786  2.09924836  0.99886925\n--  [97]  1.08172962  2.04040491  3.41532473\n-- [100] -1.47850127\n\n\nAll seems well. But then a version update adds a new argument:\n\nfoo &lt;- function(abcabc = 100, xyzabc = 0, abcxyz) {\n  rnorm(abcabc, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n-- Error in foo(abc = 2): argument 1 matches multiple formal arguments\n\n\nR throws an error, unable to find an unambiguous match. (Imagine how painful this would be to debug if R defaulted to the first match instead). The way to avoid this scenario is simple: never rely on partial argument matching in permanent code. Nonetheless, many packages do. You can identify offenders yourself by setting the warnPartialMatchArgs option:\n\noptions(warnPartialMatchArgs = TRUE)\nfoo &lt;- function(xyzabc = 0, abcxyz) {\n  rnorm(100, mean = xyzabc, sd = abcxyz)\n}\nfoo(abc = 2)\n\n-- Warning in foo(abc = 2): partial argument match of\n-- 'abc' to 'abcxyz'\n\n\n--   [1] -1.54608373  4.42865773 -3.17474383\n--   [4] -1.17646384 -0.69868242  4.84537606\n--   [7] -1.24223357  4.37145363  1.68314348\n--  [10] -1.43227476  0.03528732  1.29411463\n--  [13] -0.68289925  4.36421804  2.01861546\n--  [16]  1.28721741 -2.81521034 -3.38708074\n--  [19]  0.76613105  0.03878324  2.51910218\n--  [22] -2.92955857  2.23110888  4.57614373\n--  [25]  1.53914051  1.70718975 -0.64619523\n--  [28]  0.93470631  1.75022600 -0.77017935\n--  [31]  1.15876487 -0.22535878  0.28346343\n--  [34] -4.20902826  4.96216175 -0.96275022\n--  [37]  1.69436963 -2.60114624 -0.95495529\n--  [40]  0.12017477 -0.59668282 -3.17101207\n--  [43]  1.36160906 -3.23026882 -2.35360452\n--  [46]  1.93019269 -1.09298847  0.13411490\n--  [49] -0.44234286  2.25618932 -0.09979523\n--  [52] -0.14890005  0.52002959 -0.24640900\n--  [55]  0.15160809  3.87436011  1.12535448\n--  [58]  1.98672049  3.67616975 -0.66597559\n--  [61] -1.13848192  3.84053492 -0.03255419\n--  [64]  1.94460374 -0.65360513 -0.74586357\n--  [67] -1.56281171  2.45919767 -1.45356844\n--  [70]  0.42007214  2.55341544  0.88215118\n--  [73]  0.93721472  1.03405049  0.18142759\n--  [76]  2.69026925  1.46383473 -1.37612821\n--  [79] -0.55067523  0.69859378  1.06228601\n--  [82]  0.01680672 -2.96573602 -4.38035656\n--  [85] -0.16421133 -0.65201895 -1.80580770\n--  [88] -2.02387589 -3.01723912 -0.52509132\n--  [91] -1.11499782 -0.39737547 -5.61049358\n--  [94]  3.13083857  2.03545587 -0.54159312\n--  [97]  0.78472553 -0.58132656  1.29393200\n-- [100]  2.61027762\n\n\nWhen Simplification Complicates\nR is an example of a weakly typed language with dynamic typing. That means data types are known only at runtime, not before, and that the language will try to coerce disparate types to a common type instead of throwing an error. That means the interpreter will happily run code like\n\npaste(mtcars, 1)\n\n--  [1] \"c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26, 30.4, 15.8, 19.7, 15, 21.4) 1\"                    \n--  [2] \"c(6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4) 1\"                                                                                                            \n--  [3] \"c(160, 160, 108, 258, 360, 225, 360, 146.7, 140.8, 167.6, 167.6, 275.8, 275.8, 275.8, 472, 460, 440, 78.7, 75.7, 71.1, 120.1, 318, 304, 350, 400, 79, 120.3, 95.1, 351, 145, 301, 121) 1\"                       \n--  [4] \"c(110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 205, 215, 230, 66, 52, 65, 97, 150, 150, 245, 175, 66, 91, 113, 264, 175, 335, 109) 1\"                                                     \n--  [5] \"c(3.9, 3.9, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92, 3.07, 3.07, 3.07, 2.93, 3, 3.23, 4.08, 4.93, 4.22, 3.7, 2.76, 3.15, 3.73, 3.08, 4.08, 4.43, 3.77, 4.22, 3.62, 3.54, 4.11) 1\"                  \n--  [6] \"c(2.62, 2.875, 2.32, 3.215, 3.44, 3.46, 3.57, 3.19, 3.15, 3.44, 3.44, 4.07, 3.73, 3.78, 5.25, 5.424, 5.345, 2.2, 1.615, 1.835, 2.465, 3.52, 3.435, 3.84, 3.845, 1.935, 2.14, 1.513, 3.17, 2.77, 3.57, 2.78) 1\"  \n--  [7] \"c(16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20, 22.9, 18.3, 18.9, 17.4, 17.6, 18, 17.98, 17.82, 17.42, 19.47, 18.52, 19.9, 20.01, 16.87, 17.3, 15.41, 17.05, 18.9, 16.7, 16.9, 14.5, 15.5, 14.6, 18.6) 1\"\n--  [8] \"c(0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1) 1\"                                                                                                            \n--  [9] \"c(1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1) 1\"                                                                                                            \n-- [10] \"c(4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 5, 4) 1\"                                                                                                            \n-- [11] \"c(4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2, 2, 4, 2, 1, 2, 2, 4, 6, 8, 2) 1\"\n\n\npaste just coerces everything to character, no matter how ludicrous the results. This behavior can trip you up, but it’s not truly insidious.\nUnfortunately, R sometimes changes types under your nose. Suppose we write a function, subset2. It takes as argument a data frame, and two functions that take a data frame as argument. It filters the data column-wise using col_f, then rowwise using row_f.\n\nsubset2 &lt;- function(df, col_f, row_f) {\n  df &lt;- df[, col_f(df)]\n  df[row_f(df), ]\n}\nsubset2(mtcars, \\(x) colSums(x) &gt; 500, \\(x) rowSums(x) &gt; 500)\n\n\n\n  \n\n\n\nThat seems to work. (Deadly words!) But what if my finger had slipped when I typed 500?\n\nsubset2 &lt;- function(df, col_f, row_f) {\n  df &lt;- df[row_f, col_f(df)]\n  df[row_f(df), ]\n}\nsubset2(mtcars, \\(x) colSums(x) &gt; 5000, \\(x) rowSums(x) &gt; 500)\n\n-- Error in xj[i]: invalid subscript type 'closure'\n\n\nWhat happened? Only one column of mtcars, disp, has a column sum greater than 5000. And what happens if you select a single column with array-style indexing?\n\nmtcars[, \"disp\"]\n\n--  [1] 160.0 160.0 108.0 258.0 360.0 225.0 360.0\n--  [8] 146.7 140.8 167.6 167.6 275.8 275.8 275.8\n-- [15] 472.0 460.0 440.0  78.7  75.7  71.1 120.1\n-- [22] 318.0 304.0 350.0 400.0  79.0 120.3  95.1\n-- [29] 351.0 145.0 301.0 121.0\n\n\nR helpfully simplifies to an atomic vector. We can fix our function by disabling this behavior:\n\nsubset3 &lt;- function(df, col_f, row_f) {\n  df &lt;- df[, col_f(df), drop = FALSE]\n  df[row_f(df), ]\n}\nsubset3(mtcars, \\(x) colSums(x) &gt; 5000, \\(x) rowSums(x) &gt; 500)\n\n-- numeric(0)\n\n\nor, even more sensibly, using list subsetting (single brackets, no comma), which never simplifies.\nThis behavior isn’t indefensible. It’s consistent with how subsetting works on arrays (which are usually atomic vectors). In interactive use, it’s convenient, since then you’re usually interested in the data a column contains, not the object containing it. But automatic simplification is easily missed and potentially destructive, and the way to avoid it can be found only if you carefully read the documentation.\nBrevity is the Soul of Bugs\nSuppose you have the following vector:\n\nx &lt;- c(1, 4, 7, NA, -9, NA)\n\nR is strict about missing values, but not about logical constants. T and F can be used as abbreviations for TRUE and FALSE, respectively. The following is a valid way of taking the mean:\n\nmean(x, na.rm = T)\n\n-- [1] 0.75\n\n\nLikewise, with F for FALSE:\n\nmtcars[1:5, \"cyl\", drop = F]\n\n\n\n  \n\n\n\nWhat’s the harm in this? While TRUE and FALSE are reserved words, the abbreviations aren’t. Let’s say your colleague creates a variable T, making sure to use uppercase to avoid masking the t function:\n\nT &lt;- pt(2, df = 10)\n\nThis code now fails in a confusing way:\n\nmean(x, na.rm = T)\n\n-- [1] NA\n\n\nThe reason for this feature, as before, is clear: it’s convenient in interactive use. The problem with it is equally clear: it’s suicidal in programmatic use.\nConclusion\nThe theme here is obvious: features that save a few keystrokes in interactive use can cause maddening bugs if carelessly used in production code. You need familiarity with the language and some degree of vigilance to avoid the pitfalls, and everyone slips now and again.\nThe longer I’ve spent with R, the more convinced I’ve become that R has outgrown these features. R was designed as an environment for interactive data exploration, statistical testing, and graphical displays, but today it can do so much more: serve Web apps, query remote databases, render just about any document (even this one) with Rmarkdown or Quarto, and many other uses. But to fulfill these sophisticated use cases, you have to carefully avoid traps like the ones discussed here. Some organizations have no doubt avoided the problem by switching to Python. So R’s design emphasis on interactivity may limit its growth.\nMoreover, the benefits these features deliver are scant. The three behaviors I describe - partial argument matching, logical abbreviations, and drop = FALSE save a bit of typing (or, in the last case, an extra step of data manipulation). A few key strokes saved here and there adds up quickly, and the savings may have been significant in the days when users were limited to R’s basic readline prompt. But that doesn’t balance the potential harm they can cause in production code today, especially when modern IDEs (and Vim or Emacs, of course) support autocompletion, obviating the need for abbreviated code.\nDon’t get me wrong. R remains a powerful, expressive language built on solid design principles. It’s my first choice for any kind of data manipulation, and I still find it fun and satisfying to use. But some of its behaviors are more at home in its past than its future."
  },
  {
    "objectID": "posts/hard-to-learn/index.html",
    "href": "posts/hard-to-learn/index.html",
    "title": "Is R Hard to Learn?",
    "section": "",
    "text": "R has something of a reputation for being hard to learn. Many have speculated about the reasons why. With the possible exception of C, I can’t think of another language widely regarded in the same way. R is pointer-free (at least at the user level) and has much more forgiving typing, so why the similarly bad reputation?\nI think there is some truth to these claims: some features of R do make it unusually daunting for beginners. But the more important reason is that people often must learn R with little support or without much knowledge of programming. Learning the first language is hard; tutorials or teachers that present it the wrong way make it even harder.\nWhile I have extensive experience working with R beginners, I have no hard data, and speak as a mostly self-taught programmer. So tune your skepticism level accordingly.\n\nMy Own Experience\nR was the first programming language I learned seriously. I learned some Java many years earlier in computer science class in high school, and a little Python from a college course, but in neither case did I set out to master the language and use it in my career. When I began learning R in early 2020, I planned to become a policy analyst after grad school; knowing a programming language would give me an advantage in that field. I considered learning Python instead, but I decided on R after maybe two minutes of thought because it had a reputation for being hard to learn, and I wanted a challenge.\nI had no idea what I was getting into, of course. I dived into a too-advanced tutorial that used tidyverse without even comprehending the difference between it and R itself. (I believe it’s this one, though back then it used an older version of the tidyverse API). I gave up after a few hours, hopelessly confused. Further attempts went no better. I decided programming was beyond me, and put R aside.\nThen, in the surreal early months of the pandemic, I returned to it. I worked through different tutorials, this time going more slowly this time and no longer expecting rapid progress. (Would I have given up for good if the pandemic had not unexpectedly afforded me months of free time? Perhaps.) Intrigued by R’s plotting capabilities, I started making visualizations of pandemic-related data. (It was hard to think of anything else; the circumstances had not yet come to feel normal). As I began to encounter nontrivial problems, I realized enjoyed working through them. I remember spending hours fussing over a regular expression to extract county names from a column, or trying to transform data frames so I could plot them properly. (This was in the gather-spread days, so it was not easy!).\nI would not recommend this awkward, roundabout approach to learning the language, but a few months of it made me a competent user by the time grad school began, giving me a foundation to build on when I tackled more advanced topics.\n\n\nMy Theory\nWould the learning process have been easier had I chosen Python? Some aspects of R do make it harder to learn than comparable languages. One reason is that in R, unlike in Python, there is never one right way to do something. R comes with so many powerful functions (perhaps too many) that even simple problems can reasonably solved in many different ways. Veteran R users treasure this expressiveness, but it makes the learner’s task harder. How can you learn the way to do something when there is no one way to do it? When the repurposed StackOverflow solutions you patch together use different idioms whose merits you lack the knowledge to compare? When tutorials give contradictory advice? Only through experience, and there are no shortcuts to that.\nAnother cause is R’s well-known inconsistencies and quirks, which I have discussed before. Its often unfriendly error messages, like the infamous “improper subset of object of type closure”, make working through obstacles harder than necessary.\nBut I suspect the main reason for the perceived difficulty lies not in the language itself, but in the circumstances of the learning. Effective programming takes particular skills, and R learners are unlikely to have had the experiences that develop them.\nWriting code demands a kind of focused aggression. You start (or should) by devising a plan to achieve a technical goal. As you execute it by writing, modifying, and testing code, you quickly run into unexpected obstacles: confusing error messages, problems in code construction you can’t immediately solve, and perhaps changing requirements. You fight your way through, skimming documentation or StackOverflow for advice and experimenting with code. The experience tests your knowledge of the language and ability to analyze, but also your resourcefulness and determination. With sometimes painful experience, you develop the capacity to resolve errors relatively quickly, strong attention to detail, a knack for avoiding problems that stymied you in the past, and above all the will to persevere and turn half-formed ideas into useful code.\nBut developing these abilities takes time and effort, and they don’t make you immune to frustration. I write this on a Friday, having spent much of the week trying to do a programming assignment in Racket, a language new to me. I spent more hours than I’d like to admit struggling with the syntax until suddenly, unaccountably, it started to make sense. Even though I had more than two years of experience and knew Racket had simpler syntax rules than most languages, it took time before I could write and read it fluently.\nThe problem is that many, if not most, R learners have yet to make that effort, because they are new to programming. Think of an undergrad social science student taking a methods course, or a data analyst accustomed to Excel or SAS learning a new tool for their job. Learners of this kind have little or no programming experience or computer science education. The routine I described above is completely foreign to the first group. While the second group might have experience solving technical problems in their tool of choice, they do not know the idiom of a true programming language. So asking them to write nontrivial code is like throwing raw recruits into combat without sending them through basic training. Knowing this, designers of computer science programs typically start the major off with a “how to code” class that gently introduces naive students to programming (and typically in a general-purpose language like Python, not a specialized one like R). Learning from the ground up without that kind of instruction, or at least a knowledgeable mentor, is far harder - and impossible without strong motivation.\nNor are they likely to know that making frequent errors is normal. For novices, unused to a programming language’s demands for absolute correctness, the early weeks can feel like an endless stream of mistakes: misspelled variable names, dangling parentheses, incorrect function calls. Even experienced programmers slip up now and again (literally two minutes before I wrote this, I lost points on an assignment submission because I put an extra letter in the name of a required function). For newcomers, even writing correct syntax is taxing. If not instructed otherwise, or exposed to people having the same problem, many will start to think, as I once did, it means something is wrong with them.\nIn short, compared to other languages, the people asked to learn R are unusually likely to lack the background and support that make learning a new language relatively painless. It’s no wonder, then, that R has become known for being “hard to learn.”\n\n\nThe Implications\nThis reputation threatens the language’s future. Being “hard to learn” can discourage enterprises an educators from using R or scare people away from trying to learn it themselves. Over time, as competitors like Python make usability improvements, R might decline in popularity if it fails to do the same. The situation is hardly desperate. There are almost embarrassingly many free tutorials available. The swirl package provides interactive lessons. Thousands of package developers, not to mention R core, work hard to improve existing interfaces and root out bugs.\nBut all of us who are experienced in R have a part to play. We can assist novices who come to us for help or advice, strive to write high-quality code ourselves, and above all remember that it was once very hard for us, too.\n(Caveat to all the above: a cursory search didn’t find any research comparing the difficulty of learning of different languages. I would be interested to read any if it existed, since I have no hard data to support or disprove my claims).\nSo if you’re just starting out with R? Don’t go too fast or expect instant comprehension. Find an interesting problem you think you can solve to keep up your motivation. Have faith that your skills will improve with practice. Publish your code in some form, to seek feedback and demonstrate your growing proficiency. View your work as a creative outlet and an opportunity to refine your skill in solving problems. You don’t have to like it, or even programming in general, but there’s a good chance you’ll find you do."
  },
  {
    "objectID": "posts/lesser-known/index.html",
    "href": "posts/lesser-known/index.html",
    "title": "R Tricks I Wish I’d Known as a Beginner",
    "section": "",
    "text": "R is full of quirks, some of them obscure. Getting the most out of the language takes some experience, but is well worth the effort. These techniques will be old hat to seasoned R users, but you never know: you might still learn something."
  },
  {
    "objectID": "posts/lesser-known/index.html#get-the-expressions-passed-as-function-arguments",
    "href": "posts/lesser-known/index.html#get-the-expressions-passed-as-function-arguments",
    "title": "R Tricks I Wish I’d Known as a Beginner",
    "section": "Get the Expressions Passed as Function Arguments",
    "text": "Get the Expressions Passed as Function Arguments\nR passes function arguments by value, not by reference, yet it’s possible to recover the symbol or expression passed to a function using this trick:\n\nf &lt;- function(x) {\n  x &lt;- deparse(substitute(x))\n  print(x)\n}\nf(`I'm a symbol!`)\n\n-- [1] \"I'm a symbol!\"\n\n\nsubstitute, when called in a function, replaces its argument with the expression in the promise corresponding to that argument. (Promises are internal objects that implement function arguments). deparse converts that unevaluated R code into a character vector.\nThis could be used to make a function that automatically labels plot axes:\n\ndescriptive_plot &lt;- function(x, y) {\n  x_name &lt;- deparse(substitute(x))\n  y_name &lt;- deparse(substitute(y))\n  plot(x, y, xlab = x_name, ylab = y_name)\n}\nweight &lt;- mtcars$wt\nmpg &lt;- mtcars$mpg\ndescriptive_plot(mpg, weight)\n\n\n\n\n\n\n\nWhat are your favorite R tricks?"
  },
  {
    "objectID": "posts/next_language/index.html",
    "href": "posts/next_language/index.html",
    "title": "The Data Science Language of the Future",
    "section": "",
    "text": "R, for all its warts, has most of the features I want from a data science language. It’s powerful, surprisingly versatile, and usually fun to use. But, like all languages, it is neither perfect nor likely to be widely used forever. (I doubt it will enjoy - if that is the right word - the endless afterlife of COBOL and its ilk). So I hope the (distant!) future will see statistical languages that replicate and refine R’s strengths while improving its weaknesses. What should those languages look like?\nTo discuss a next-generation language, we need to establish what makes R so great to begin with. On reflection, I identified three key ingredients:\nBut vectorization has benefits beyond mathematical convenience. (For now, let’s use Hadley Wickham’s working definition of a vectorized function: \\(f(x[[i]]) = f(x)[[i]]\\)). It abstracts away the iteration involved in operations, freeing you to think of functions as acting on each element independently. This results in compact, readable code:\npaste0(letters, \" is letter #\", seq_along(letters), \" of the alphabet\")\n\n--  [1] \"a is letter #1 of the alphabet\" \n--  [2] \"b is letter #2 of the alphabet\" \n--  [3] \"c is letter #3 of the alphabet\" \n--  [4] \"d is letter #4 of the alphabet\" \n--  [5] \"e is letter #5 of the alphabet\" \n--  [6] \"f is letter #6 of the alphabet\" \n--  [7] \"g is letter #7 of the alphabet\" \n--  [8] \"h is letter #8 of the alphabet\" \n--  [9] \"i is letter #9 of the alphabet\" \n-- [10] \"j is letter #10 of the alphabet\"\n-- [11] \"k is letter #11 of the alphabet\"\n-- [12] \"l is letter #12 of the alphabet\"\n-- [13] \"m is letter #13 of the alphabet\"\n-- [14] \"n is letter #14 of the alphabet\"\n-- [15] \"o is letter #15 of the alphabet\"\n-- [16] \"p is letter #16 of the alphabet\"\n-- [17] \"q is letter #17 of the alphabet\"\n-- [18] \"r is letter #18 of the alphabet\"\n-- [19] \"s is letter #19 of the alphabet\"\n-- [20] \"t is letter #20 of the alphabet\"\n-- [21] \"u is letter #21 of the alphabet\"\n-- [22] \"v is letter #22 of the alphabet\"\n-- [23] \"w is letter #23 of the alphabet\"\n-- [24] \"x is letter #24 of the alphabet\"\n-- [25] \"y is letter #25 of the alphabet\"\n-- [26] \"z is letter #26 of the alphabet\"\nIn base Python or most other languages, this would require a for loop that kept track of letters and indices, resulting in less readable code and a greater likelihood of mistakes. Better still, R features convenience functions like colMeans that operate at a higher level of abstraction: data frames or arrays, which are versatile generalizations of simple atomic vectors. These capabilities let you ignore implementation details of iteration and write nicely abstract code.\nVectorization is hardly unique to R, but I don’t know of another language\nas fundamentally vector-oriented. Our ideal successor language should emulate R in this area.\nToo often, the actual “science” of data science, like dessert after a big feast, is dwarfed by what came before: data tidying, missing value imputation, transformation, and\neverything else required to get messy input into a form that can be analyzed. If a data pipeline doesn’t exist, this can become far more daunting than the analysis itself. No language is better suited for the job than R. A skilled user can achieve even elaborate transformations in ten or twenty lines. With practice, the feeling of power becomes almost addictive. Using another language feels like putting on heavy gloves before tying your shoelaces.\nR’s expressive, powerful data manipulation interface grants it this power. It also makes R hard to learn. You can often find five or six obvious, correct ways to do even a simple task, like obtaining the fourth element of the mtcars columns cyl.\nmtcars$cyl[[4]]\n\n-- [1] 6\n\nmtcars[[c(2, 4)]]\n\n-- [1] 6\n\nmtcars[4, \"cyl\"]\n\n-- [1] 6\n\nmtcars[[\"cyl\"]][[4]]\n\n-- [1] 6\n\nmtcars[rownames(mtcars)[[4]], \"cyl\"]\n\n-- [1] 6\nA successor to R might develop a smaller set of operators, and smooth out some oddities (like drop = FALSE). But it should not go too far in this Emphasizing readability and separating tasks into different functions, as dplyr has done, would make code more readable and easier to debug, but also more verbose. Too radical a departure from R’s approach would fail to replicate what makes it special.\nThe other two areas I identify are widely cited as strengths of R. This one, though, is esoteric. While almost all R users take advantage of the features that power metaprogramming, many without knowing it, few use them extensively. It’s easy (and sometimes advisable) even for experienced users to avoid invoking it directly. Still, it distinguishes R from most other languages, and rests on bold design decisions made long before the language’s inception.\n“Metaprogramming”, as used in the R community, means writing programs that treat R code as data - programming on programs, in other words. It utilizes R’s highly developed capabilities for partial expression substitution, controlled evaluation, and environment manipulation. Books could be written about this topic, and Advanced R covers it in detail.\nAs a basic example, have you ever wondered why most calls to library in R scripts look like library(package), not library(\"package\")? The latter is legal, but seldom used. Most functions will throw an error if passed the name of a nonexistent object:\nc(\"a\", \"b\", \"c\", d)\n\n-- Error in eval(expr, envir, enclos): object 'd' not found\nBut certain functions capture their inputs directly, without evaluating them, and then evaluate them in a different context. This is called “quoting”, since it captures the syntax of code while ignoring the semantics the way quoting natural language does. The implementation, known as non-standard evaluation, powers much of R’s interface. One prominent example is formulas: a compact mini-language for specifying a statistical relationship to modeling functions. Because the formula is quoted and evaluated in the context of a data frame, the user can provide bare variable names, making for a clean, simple interface:\nlm(mpg ~ wt + cyl * disp, data = mtcars)\n\n-- \n-- Call:\n-- lm(formula = mpg ~ wt + cyl * disp, data = mtcars)\n-- \n-- Coefficients:\n-- (Intercept)           wt          cyl  \n--    49.55195     -2.73695     -3.00543  \n--        disp     cyl:disp  \n--    -0.08670      0.01107\nThe tidyverse takes this idea much further. Its functions rely on tidy evaluation, an elaborate framework for selecting and modifying variables within “data masks.” In the end, R is really a statistics-oriented descendant of Lisp with more conventional syntax. Many of these ideas - expressions as data, expression substitution, and even optional prefix syntax - come from that immortal language.\n`+`(2, 2)\n\n-- [1] 4\nAll this power comes with serious drawbacks - serious enough that it can be reasonably argued that non-standard evaluation is a bad paradigm. Manipulating expressions means code loses referential transparency (evaluating the same if variable names are changed). Controlled evaluation requires programmers to think about environment inheritance, creating the potential for a host of subtle bugs. Functions that quote some of their arguments but not all, or accept quoted and nonquoted forms of the same argument (like library), are harder to use. In the end, all this indirection makes code harder to write and reason about (hence the need for a vignette on simply programming with dplyr). I think the tradeoff is worthwhile; the convenience and flexibility of non-standard evaluation are too valuable to abandon. But unlike the other two characteristics I outlined above, a strong case can be made otherwise.\nIn short, a successor to R should contain R’s most powerful features: vector types and vectorized functions, a terse but expressive subsetting syntax, and support for expression manipulation and controlled evaluation."
  },
  {
    "objectID": "posts/next_language/index.html#finicky-interface",
    "href": "posts/next_language/index.html#finicky-interface",
    "title": "The Data Science Language of the Future",
    "section": "Finicky Interface",
    "text": "Finicky Interface\nR’s user interface, in places, in harder to learn and use than necessary. It uses conventions inconsistently, exposes too much detail to the user, and contains too many “gotchas” that cause confusing errors you can only avoid with experience.\nOne of the unwritten rules of programming is that inconsistency should not exist without reason. If you write a class Foo with methods called bar_bar, baz_baz, and quxQux, your users will wonder why you used camelCase for just one method every time they try to call the logically expected but nonexistent qux_qux. If you put a data frame argument at the head of one function’s argument list but the tail of another’s, they will wonder why every time they forget which is which. Only constant attention in design can avoid inconsistencies like these, but the best designs do so.\nR violates the principle in many places. One trivial but well-known example is the way S3 methods are written generic.class (e.g., mean.default), yet dots are used all the time in the names of functions, including S3 generics. The many exceptions (t.test, all.vars, …) thwart a potentially useful convention. Unlike the other functionals, mapply has the function as the first argument, not the second, and the simplify and use.names arguments are\nactually SIMPLIFY and USE.NAMES (not without reason, but good luck remembering). ave and tapply do similar things, but ave uses ... for grouping factors, while tapply reserves it for arguments to the FUN argument. Once you notice one of these seams in the design, you can’t unsee it.\nR sometimes contains unnecessary complexity. Interfaces often have complicated semantics, and functions sometimes feature multiple operating modes. For instance, there are two slightly different functions for doing principal components analysis, differing in the algorithm used. The function diag has four distinct uses (five, if you count diag&lt;- as part of the same interface). Most troubling to me are the heavily overloaded arguments of certain functions. Consider this passage from the help for get:\nThe ‘pos’ argument can specify the environment in which to look\n     for the object in any of several ways: as a positive integer (the\n     position in the ‘search’ list); as the character string name of an\n     element in the search list; or as an ‘environment’ (including\n     using ‘sys.frame’ to access the currently active function calls).\n     The default of ‘-1’ indicates the current environment of the call\n     to ‘get’. The ‘envir’ argument is an alternative way to specify an\n     environment.\nI count three possible types for pos, all with different meanings, a default value with a special meaning, and another argument that does exactly the same thing for one type. (Plus a suggestion to use call stack introspection, which I’ll leave to braver programmers than me).\nTrying to memorize the intricacies of an interface like this is a fool’s errand: at some point, you’ll get it wrong and cause a nasty bug. That leaves no recourse but referring to the documentation each time you use the function, and nothing makes an interface more annoying to use.\nAnother offender is factors. Factors represent categorical variables by mapping integer codes to levels. Simple idea, but so many potential errors come from this fact. Something as simple as naively concatenating a factor causes disaster:\n\nx &lt;- factor(c(\"a\", \"b\", \"c\"))\ny &lt;- factor(c(\"x\", \"y\", \"z\"))\nc(x, \"d\")\n\n-- [1] \"1\" \"2\" \"3\" \"d\"\n\n\nAttempting to do factor arithmetic only triggers a warning, despite being nonsense (Note also that the factor warning preempts the “mismatched object lengths” warning this would normally trigger):\n\nx + 3:6\n\n-- [1] NA NA NA NA\n\n\nWorst of all, and not widely known: R’s lexical sort order differs by system locale. (See here for an example). When creating a factor, R defaults to ordering the levels lexically. Good luck with that reproducible research!\nIndividually, these criticisms are trivial. I don’t mean to cast them as evidence of incompetence or carelessness by the language designers. I have written much worse interfaces to far simpler programs, so I know from experience how hard it is to implement and maintain a good one. But our successor language can do better by following the tidyverse and making “design for human users” a core principle."
  },
  {
    "objectID": "posts/next_language/index.html#very-weak-typing",
    "href": "posts/next_language/index.html#very-weak-typing",
    "title": "The Data Science Language of the Future",
    "section": "Very Weak Typing",
    "text": "Very Weak Typing\nOur new language should have dynamic typing. Static typing makes code easier to reason about and debug, especially in large applications, but it would be awkward to explore or transform data without quick, easy type conversions that can be done interactively. In its present form, I think R makes these conversions a little too easy. R is a weakly typed language: instead of disallowing operations with objects of disparate types, it casts them to a common type. Sometimes the result is predictable:\n\nc(TRUE, \"abc\")\n\n-- [1] \"TRUE\" \"abc\"\n\nTRUE + 3\n\n-- [1] 4\n\n\nBut sometimes R will allow operations that have no sensible result:\n\npaste0(mtcars, \"abc\")\n\n--  [1] \"c(21, 21, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26, 30.4, 15.8, 19.7, 15, 21.4)abc\"                    \n--  [2] \"c(6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8, 8, 8, 8, 4, 4, 4, 8, 6, 8, 4)abc\"                                                                                                            \n--  [3] \"c(160, 160, 108, 258, 360, 225, 360, 146.7, 140.8, 167.6, 167.6, 275.8, 275.8, 275.8, 472, 460, 440, 78.7, 75.7, 71.1, 120.1, 318, 304, 350, 400, 79, 120.3, 95.1, 351, 145, 301, 121)abc\"                       \n--  [4] \"c(110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 205, 215, 230, 66, 52, 65, 97, 150, 150, 245, 175, 66, 91, 113, 264, 175, 335, 109)abc\"                                                     \n--  [5] \"c(3.9, 3.9, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92, 3.07, 3.07, 3.07, 2.93, 3, 3.23, 4.08, 4.93, 4.22, 3.7, 2.76, 3.15, 3.73, 3.08, 4.08, 4.43, 3.77, 4.22, 3.62, 3.54, 4.11)abc\"                  \n--  [6] \"c(2.62, 2.875, 2.32, 3.215, 3.44, 3.46, 3.57, 3.19, 3.15, 3.44, 3.44, 4.07, 3.73, 3.78, 5.25, 5.424, 5.345, 2.2, 1.615, 1.835, 2.465, 3.52, 3.435, 3.84, 3.845, 1.935, 2.14, 1.513, 3.17, 2.77, 3.57, 2.78)abc\"  \n--  [7] \"c(16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20, 22.9, 18.3, 18.9, 17.4, 17.6, 18, 17.98, 17.82, 17.42, 19.47, 18.52, 19.9, 20.01, 16.87, 17.3, 15.41, 17.05, 18.9, 16.7, 16.9, 14.5, 15.5, 14.6, 18.6)abc\"\n--  [8] \"c(0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1)abc\"                                                                                                            \n--  [9] \"c(1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1)abc\"                                                                                                            \n-- [10] \"c(4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5, 5, 4)abc\"                                                                                                            \n-- [11] \"c(4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2, 2, 4, 2, 1, 2, 2, 4, 6, 8, 2)abc\"\n\n\nMoreover, R has no equivalent of Python’s type hinting system. If you want to enforce a specific type for function arguments, you have to do it manually:\n\nfoo &lt;- function(x, y, z) {\n  if (!is.character(x)) {\n    stop(\"x must be character\")\n  }\n}\n\nMany of the type-checking helpers like is.character have surprisingly complex behaviors that make them dangerous to rely on.\nR functions also do not always have stable return types. sapply, for example, can return a list, an array, an atomic vector, or even an empty list, depending on the input. Programming guides often recommend lapply or vapply instead, since they enforce stable return types, but many unwary users (including me, at various times) who did not know this have written subtly buggy code.\nR’s very weak typing accounts for much of the unpredictable behavior that makes it challenging to use in large applications. I think strict typing like Python’s would be excessive; operations like paste(1:10, letters[1:10]) are too convenient to part with. But our successor language will dispense with some of the crazier implicit coercions R allows."
  },
  {
    "objectID": "posts/next_language/index.html#string-manipulation",
    "href": "posts/next_language/index.html#string-manipulation",
    "title": "The Data Science Language of the Future",
    "section": "String Manipulation",
    "text": "String Manipulation\nR’s string manipulation facilities leave something to be desired. In other languages, strings are array types or feature array-like subsetting. R, however, handles strings (i.e., the raw character data that make up the elements of character vectors) with an internal type. You can’t extract string elements the way you can in Python:\n\nx = \"A typical string\"\nx[0]\n\n-- 'A'\n\n\nYou have to use substr or substring (barely distinguishable functions again!)\n\nx &lt;- \"A typical string\"\nsubstr(x, 1, 1)\n\n-- [1] \"A\"\n\n\nThe rationale is obvious - the unpalatable alternative would be to implement character vectors as list-like recursive vectors - but it has annoying consequences for the interface, such as strsplit returning a list:\n\nx &lt;- c(\"This is a typical\", \"character vector\")\nstrsplit(x, split = \"\\\\s\")\n\n-- [[1]]\n-- [1] \"This\"    \"is\"      \"a\"       \"typical\"\n-- \n-- [[2]]\n-- [1] \"character\" \"vector\"\n\n\nBut these are quibbles. The real problem is the regular expression interface. This is the only part of base R I actively dislike. There are too many functions with terse, barely distinguishable names. (If you can remember the difference between gregexpr and regexec without looking it up, please teach me your secrets). Functions don’t use PCRE by default, a fact I never remember until it causes an error. They return match data in awkward formats; gregexpr, for instance, returns a list of match start positions and lengths, making it difficult to extract the actual match data.\nPut together, these issues make working with regular expressions much more verbose and painful than necessary. The convoluted snippet below, copied from the documentation, does nothing more than create a matrix with the text from two capture groups. For comparison, Python’s re module contains a groupdict method that stores matches in an appropriate data structure automatically.\n\nnotables &lt;- c(\n  \"  Ben Franklin and Jefferson Davis\",\n  \"\\tMillard Fillmore\"\n)\n# name groups 'first' and 'last'\nname.rex &lt;- \"(?&lt;first&gt;[[:upper:]][[:lower:]]+) (?&lt;last&gt;[[:upper:]][[:lower:]]+)\"\n(parsed &lt;- regexpr(name.rex, notables, perl = TRUE))\n\n-- [1] 3 2\n-- attr(,\"match.length\")\n-- [1] 12 16\n-- attr(,\"index.type\")\n-- [1] \"chars\"\n-- attr(,\"useBytes\")\n-- [1] TRUE\n-- attr(,\"capture.start\")\n--      first last\n-- [1,]     3    7\n-- [2,]     2   10\n-- attr(,\"capture.length\")\n--      first last\n-- [1,]     3    8\n-- [2,]     7    8\n-- attr(,\"capture.names\")\n-- [1] \"first\" \"last\"\n\ngregexpr(name.rex, notables, perl = TRUE)[[2]]\n\n-- [1] 2\n-- attr(,\"match.length\")\n-- [1] 16\n-- attr(,\"index.type\")\n-- [1] \"chars\"\n-- attr(,\"useBytes\")\n-- [1] TRUE\n-- attr(,\"capture.start\")\n--      first last\n-- [1,]     2   10\n-- attr(,\"capture.length\")\n--      first last\n-- [1,]     7    8\n-- attr(,\"capture.names\")\n-- [1] \"first\" \"last\"\n\nparse.one &lt;- function(res, result) {\n  m &lt;- do.call(rbind, lapply(seq_along(res), function(i) {\n    if (result[i] == -1) {\n      return(\"\")\n    }\n    st &lt;- attr(result, \"capture.start\")[i, ]\n    substring(res[i], st, st + attr(result, \"capture.length\")[i, ] - 1)\n  }))\n  colnames(m) &lt;- attr(result, \"capture.names\")\n  m\n}\nparse.one(notables, parsed)\n\n--      first     last      \n-- [1,] \"Ben\"     \"Franklin\"\n-- [2,] \"Millard\" \"Fillmore\"\n\n\n\nimport re \n\nnotables = [\"Ben Franklin and Jefferson Davis\",  \"\\tMillard Fillmore\"]\n[re.match(\".*(?P&lt;first&gt;[A-Z][a-z]+).*(?P&lt;last&gt;[A-Z][a-z]+)\", x).groupdict() for x in notables]\n\n-- [{'first': 'Jefferson', 'last': 'Davis'}, {'first': 'Millard', 'last': 'Fillmore'}]\n\n\n(Is Fillmore’s inclusion a sly joke? He is chiefly notable for being a bottom-tier president).\nThe excellent stringr package provides functions that fix all of these problems. But R users shouldn’t have to choose between taking a major dependency and foregoing easy string processing."
  },
  {
    "objectID": "posts/R6-active/index.html",
    "href": "posts/R6-active/index.html",
    "title": "Programatically Creating Accessor Functions for R6 Objects",
    "section": "",
    "text": "One nice feature of R6 objects is active fields. Normally, to expose a field to the user, you have pass it to the public argument of the R6Class constructor. That makes it accessible, but also permits the user to meddle with it.\n\nlibrary(R6)\n\nunprotected &lt;- R6Class(\n  classname = \"unprotected\",\n  public = list(foo = 1, bar = 2, baz = 3)\n)\nexample &lt;- unprotected$new()\nexample$foo\n\n-- [1] 1\n\nexample$foo &lt;- 2\nexample$foo\n\n-- [1] 2\n\n\nFields can be protected by sending them to private instead, but that blocks the user from accessing them. The solution is to create an active field. This creates an active binding: a special form of R function that can be used to return a value if called with no arguments and to bind a value if called with one. We can use this capability to create an accessor function that blocks users from changing values:\n\nprotected &lt;- R6Class(\n  classname = \"example\",\n  public = list(\n    bar = 2, baz = 3,\n    initialize = function(foo) private$.foo &lt;- foo\n  ),\n  private = list(.foo = NULL),\n  active = list(foo = function(value) {\n    if (missing(value)) {\n      return(private$.foo)\n    } else {\n      stop(\"Hands off!\")\n    }\n  })\n)\n\nexample &lt;- protected$new(foo = 1)\nexample$foo\n\n-- [1] 1\n\nexample$foo &lt;- 2\n\n-- Error in (function (value) : Hands off!\n\n\n(See chapter 14 of Advanced R for more details).\nThis is all simple enough, but there’s an obvious problem: what if we have a lot of attributes to protect? We could dodge the problem by combining them into a single list attribute, or just copy-paste the same function with different attribute names. But those options aren’t always attractive. I recently confronted this problem while working on an elaborate subclass of torch::dataset, which organizes data for neural networks. I decided to rifle through my bag of functional programming tricks in search of a solution.\nFirst Attempt: Function Factory\nSince each active field requires a function, a function factory was an obvious approach. It’s simple to implement:\n\naccessor_factory &lt;- function(field) {\n  force(field)\n  function(value) {\n    if (missing(value)) {\n      return(private[[\"field\"]])\n    } else {\n      stop(\"Hands off \", field, \"!\")\n    }\n  }\n}\n\n(The real version used a less jocular error message, but I need to have my fun somehow). Because R has lexical scope, field is bound in the manufactured function’s enclosing environment, so when executed it should look there and find it.\nBut it doesn’t work.\n\nprotected &lt;- R6Class(\n  classname = \"example\",\n  public = list(\n    bar = 2, baz = 3,\n    initialize = function(foo) private$.foo &lt;- foo\n  ),\n  private = list(.foo = NULL),\n  active = list(foo = accessor_factory(\".foo\"))\n)\nexample &lt;- protected$new(1)\nexample$foo\n\n-- NULL\n\nexample$foo &lt;- 2\n\n-- Error in (function (value) : object 'field' not found\n\n\nEither R core sneaked support for dynamic scope into the last major version, or the R6Class constructor was doing something funny. Checking the source code found the offending line:\n\ngenerator_funs &lt;- assign_func_envs(generator_funs, generator)\n\nThe constructor modified the environments of function fields (a trick I also resorted to while writing a different subclass, but that’s another story). Relying on scope wouldn’t help, but what would?\nSecond Attempt: as.function\n\nMy next idea was to use R’s obscure but powerful function constructor, as.function. It has a strange implementation: it takes a list, interpreting all elements except the last as name-value pairs for arguments (with an empty value slot designating an argument with no default). The last element should be an expression defining the function body. This is what I wrote:\n\naccessor_factory &lt;- function(field) {\n  force(field)\n  code &lt;- substitute(\n    {\n      if (missing(value)) {\n        return(private[[field]])\n      } else {\n        stop(sQuote(field), \" is read-only\")\n      }\n    },\n    list(field = field)\n  )\n  as.function(eval(substitute(\n    alist(value = , code),\n    list(code = code)\n  )),\n  envir = globalenv()\n  )\n}\n\nThis code demands some explanation. The idea is to return a function with the value of field already substituted, not set at runtime. The first step uses substitute to replace the symbol field with the value passed to the function (i.e., the name of the target attribute). The result forms the body of the manufactured function. I have to call substitute again to substitute this expression into the call to alist passed to as.function, because alist quotes its arguments. That expression actually creates the function we need. (See why most people consider me weird for liking metaprogramming?).\n\nprotected &lt;- R6Class(\n  classname = \"example\",\n  public = list(\n    bar = 2, baz = 3,\n    initialize = function(foo) private$.foo &lt;- foo\n  ),\n  private = list(.foo = NULL),\n  active = list(foo = accessor_factory(\".foo\"))\n)\nexample &lt;- protected$new(1)\nexample$foo\n\n-- [1] 1\n\nexample$foo &lt;- 2\n\n-- Error in (function (value) : '.foo' is read-only\n\n\nThis works. But can we do better?\nThird Attempt: Body Substitution\nR features assignment functions to modify all three parts of a closure: formal arguments, body, and environment. We’re interested in creating a set of functions with slightly different bodies, so pairing body&lt;- with substitute is a natural approach. It’s a lot more readable than my last attempt, too. The classic double-substitute trick for substituting the result of an expression comes from Advanced R.\n\nsubstitute_body &lt;- function(fn, mapping) {\n  body(fn) &lt;- eval(substitute(substitute(temp, mapping), list(temp = body(fn))))\n  fn\n}\n\ntemplate &lt;- function(value) {\n  if (missing(value)) {\n    return(private[[field]])\n  } else {\n    stop(sQuote(field), \" is read-only\")\n  }\n}\nsubstitute_body(template, mapping = list(field = \"test\"))\n\n-- function (value) \n-- {\n--     if (missing(value)) {\n--         return(private[[\"test\"]])\n--     }\n--     else {\n--         stop(sQuote(\"test\"), \" is read-only\")\n--     }\n-- }\n\n\nVictory! Well, almost. To make this truly useful, we need a wrapper function to create a list of accessors from field names. Thankfully, that’s much easier than figuring out the substitution.\n\nset_active_fields &lt;- function(fields) {\n  out &lt;- lapply(fields, function(x) {\n    substitute_body(\n      fn = template,\n      mapping = list(field = x)\n    )\n  })\n  names(out) &lt;- gsub(\"^\\\\.\", \"\", fields)\n  out\n}\n\nA bog-standard use of lapply does the job, with the annoying complication of removing leading dots from the names of private fields.\nWe can even go one step further and write a wrapper to R6Class to automatically create accessors from a list of private attributes.\n\nwith_accessors &lt;- function(classname = NULL,\n                           public,\n                           private,\n                           inherit = NULL, lock_objects = TRUE,\n                           class = TRUE,\n                           portable = TRUE, lock_class = FALSE,\n                           cloneable = TRUE,\n                           parent_env = (function() parent.frame())()) {\n  force(parent_env)\n  active &lt;- set_active_fields(names(private))\n  R6Class(\n    classname = classname, public = public,\n    private = NULL, active = active,\n    inherit = inherit, lock_objects = lock_objects,\n    class = class,\n    portable = portable,\n    lock_class = lock_class,\n    cloneable = cloneable,\n    parent_env = parent_env\n  )\n}\n\n\npublic &lt;- list(initialize = function(foo) {\n  private$.foo &lt;&lt;- foo\n})\nprivate &lt;- list(.foo = NULL, .bar = 2, .baz = 3)\nprotected &lt;- with_accessors(\"example\", public = public, private = private)\n\nexample &lt;- protected$new(foo = 1)\nexample$foo\n\n-- [1] 1\n\nexample$bar\n\n-- [1] 2\n\nexample$baz\n\n-- [1] 3\n\nexample$foo &lt;- 2\n\n-- Error in (function (value) : '.foo' is read-only\n\nexample$baz &lt;- 5\n\n-- Error in (function (value) : '.baz' is read-only\n\n\nNote that because of the indirection, I have to use &lt;&lt;- in initialize. I also have to make parent_env the execution environment of the wrapper, which is the caller environment of R6Class here. There may also be other nasty surprises buried in this use of reference semantics. Still, this was a fun diversion, and proof of how much power R grants the user over environments and evaluation."
  },
  {
    "objectID": "posts/that_kind_of_a_day/index.html",
    "href": "posts/that_kind_of_a_day/index.html",
    "title": "That Kind of a Day",
    "section": "",
    "text": "The lintr package, a widely used linter for R code, prefers that you use snake case for object names. “You” includes R core:\n\nIn fairness, is.numeric ought to be called is_numeric, because dots are supposed to be reserved for S3 methods. R breaks this rule all the time, leading to names like as.data.frame.data.frame. R updates are named after Peanuts strips; getting dinged by the linter for using a base function is something that would happen to Charlie Brown if he ever took up programming. Apparently, whatever method lintr uses to exempt base function names from linting doesn’t work when those function names are arguments to another function.\nAs you might guess from the traceback thirty calls deep on the right of the screen, this wasn’t a great day. But I can’t help but smile when I see a linter commit heresy."
  },
  {
    "objectID": "posts/triumph-travesty/index.html",
    "href": "posts/triumph-travesty/index.html",
    "title": "Triumph and Travesty: Earning All 50 Stars in Advent of Code 2021",
    "section": "",
    "text": "This post contains spoilers for some 2021 Advent of Code puzzles. Proceed at your own risk.\nIf you haven’t heard of Advent of Code, it’s well worth your time to check out. Created and maintained by software engineer Eric Wastl, Advent of Code (AoC for short) is an annual event involving an advent calendar of Christmas-themed programming challenges. Anyone can participate for free, anonymously if they like. A new puzzle is released on each of the first 25 days of December. They start simple and gradually increase in difficulty. Elite players compete for spots on the official leaderboard of the fastest solutions, but most (myself included) just aim to solve the puzzles. Each puzzle(with one exception) awards two “gold stars” when completed, providing a way to track your progress.\nThe puzzles themselves take the form of well-posed problems, connected through a whimsical yuletide narrative. This year’s edition sent players to the ocean depths in a submarine to retrieve the lost keys to Santa’s sleigh. Along the way, they encountered treacherous currents, labyrinthine caves, and a whole lot of obstreperous sea creatures - all of which could only be overcome with some creative programming (Half the fun is recognizing the classical computer science problems underneath the intentonally silly presentation). Each puzzle consists of two parts. The first part states the problem, with any necessary rules, and offers a plaintext input (randomly generated for each player) to work from. If the player submits the correct answer, they receive a gold star…and updated instructions with a new version of the problem to solve. It usually adds a new constraint or asks the player to interpret the input in a different way; depending on the problem and the player’s approach for part 1, overcoming it could take anything from changing a single line to starting from scratch. Submitting the correct answer for the second part earns another gold star and completes that day’s puzzle. (The lone exception to the standard format is the Christmas Day puzzle, which differs in a way I won’t spoil). Players can use whatever language and strategy they like; some solve puzzles in absurd (or do I mean awesome) languages like Rockstar, or impose tough constraints, because they can.\nThe puzzles test a wide variety of programming techniques, from recursion to graph traversal to regular expressions. The problem statements are all “fair” - there are no hidden rules or lawyerly gotchas - but even a subtle misunderstanding can cost you hours of frustrating debugging (just like real life!). With no constraints and no expectation to write production-quality code, you’re free to tackle each problem as you see fit, limited only by your knowledge and creativity.\nI stumbled across AoC in late 2020, a pivotal time in my life. Perhaps a month before, realizing I liked programming a lot more than policy analysis, I had decided to convert my masters degree from public policy to data science. With enough experience in R to feel (over)confident in my programming skills, I dove in without hesitation and spent much of that holiday break absorbed in the puzzles. Tackling such beautifully abstract problems, with no pressure and no shame in failing, was bliss; I enjoyed even the frustration. Realizing R was ill-suited for many of the puzzles, I switched to Python, learning it as I went. I only ever solved some of the puzzles, and those in amateurish fashion (look here if you’re morbidly curious), but I became a much better programmer for it. Having had so much fun, I resolved to come back next year truly prepared.\nWhen December 2021 came, I threw myself into the puzzles. (I probably should have spent more time studying for exams instead, but this questionable time allocation thankfully didn’t hurt my GPA). The first few days came easily, aside from day 3, for which I kludged together an overcomplicated solution involving bitshifting. I switched between R and Python, preferring R for problems involving matrices and similar structures and Python where iteration was emphasized. Once again, I learned plenty along the way: queues for day 6, optimization for day 7, complex numbers as coordinates for day 11. For longer than I expected, I managed not to fall a day behind.\nBut that couldn’t last. I got badly stuck on part 2 of day 14 (which was not a hard problem, in hindsight). The end came on day 15, a tough problem involving graph traversal. I stalled out after hours of work, until a post on the subreddit pointed me toward Dijkstra’s algorithm. After writing probably the worst implementation of all time and letting my computer chug along for about an hour, I claimed both gold stars. But I had almost burnt myself out. The remaining puzzles (aside from a few easier “breather” problems) seemed impossible, and I ceased trying to keep up. Determined to keep going, I gutted my way through day 16, completing it only after spending hours looking in the wrong places for a simple bug. I knew then I had to stop.\nI had done better than I had expected; 50 stars seemed within reach. After taking a few days off, I knocked out a few of the easier puzzles, leaving thirty-odd stars secured. But then the spring semester started, depriving me of free time. Somehow, I still managed to complete the very challenging day 24, guided by a kind user on the Python discord. After that, as the holidays became a distant memory, Advent of Code fell to the bottom of my priorities.\nThat is, until I graduated. Without a job, and itching to work on something that didn’t involve complex data manipulation, I picked up where I had left off. The first puzzles fell with surprising ease: day 18, after some crude but effective string processing; day 19, after browsing the subreddit for tips; and even part 2 of day 21, completed after I spent half an hour fiddling with code I hadn’t touched for five months (when does that ever happen?). Day 22 stumped me for a while, so I asked for advice on the subreddit and followed a set-theory approach that ended up yielding a very elegant solution. That left just day 23: finding the optimal strategy for a simple puzzle that would be very, very hard to solve programatically. I dimly remembered some post on the subreddit recommending the A* algorithm. Knowing it was always smart to work out the problem with pen and paper before writing any code, I sketched out a game board, cut up some sticky notes to use as tokens, and set to work. I solved part 1 easily enough this way, so for the hell of it I tried again on part 2, which posed the same problem on an expanded game board. A few failed attempts later, I nearly gave up; I had learned the hard way how easily “just one more try” turns into a few hours of futile coding. But that time, I didn’t. When I entered my answer, I saw for the last time the familiar message:\nYou have solved part 2 of this puzzle! It provides one gold star.\nIt came as an anticlimax; I would not have to code that A* nightmare after all. Perversely, I felt cheated. Maybe I had cheated. The official description of Advent of Code entreats you to solve puzzles in “any programming language you like,” after all. Was I violating the spirit of the event by avoiding a programmatic solution entirely? Perhaps. The thought nags at me, so I suspect I’ll come back to this problem eventually, when I’m more confident in graph traversal algorithms. But still, I had all 50 stars, a feat that had seemed impossible just a year before.\nViewed one way, this is a trivial achievement: writing throwaway code to solve toy problems invented to kill time over in the weeks before Christmas. Viewed another way, it’s legitimately impressive. I solved all 25 of these puzzles in the time I could spare, just to sharpen my skills and indulge my love of the art of programming. I think it’s enough to say that grad students my age have found worse diversions. Either way, I emphasize that I had plenty of help: people on the subreddit and other forums to guide me, tutorials to consult, and above all the knowledge that many other people persevered through the same frustrations and got to 50 stars.\nI’ll probably be back next year, of course. I’ll have a lot less time to devote, since I expect to have a job by then. I don’t know if I’ll grind out all 50 stars again, now that I’ve done it already. But I do know that any time I spend on Advent of Code won’t be wasted, and I’ll be a better programmer for it.\nI just hope there aren’t as many graphs this time.\nMy repository for Advent of Code 2021: https://github.com/ryan-heslin/AoC2021"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Hello, world!",
    "section": "",
    "text": "Welcome to my new blog! My name is Ryan Heslin, and I’ve just graduated with a master’s degree in data science from American University. As I do for most of my side projects, I’ve defined goals for this one. Naturally, I’ve made those goals sound just compelling enough that I can plausibly deny that working on this thing is procrastination.\nThose goals are:\n\nEngage with the wider R community.\nShare some of the R esoterica I’ve learned\nLearn a web framework, as a first step toward more sophisticated web design\nPractice writing, which I find satisfies me the same way coding does\n\nI plan to mostly write about R and other data science topics, though I might broaden scope later on. If you’ve found your way here, I hope at least you’re entertained for a little while."
  },
  {
    "objectID": "posts/use-neovim/index.html",
    "href": "posts/use-neovim/index.html",
    "title": "Why I Use Neovim",
    "section": "",
    "text": "Introduction\nRight now, I do almost all my writing and programming using Neovim. It’s not a typical choice for R users, who largely use RStudio, an excellent R-specialized IDE. I started with RStudio, but moved to Neovim after a bit more than a year, for reasons I’ll explain below.\nMy experience with Neovim was overdue for a full-length post. I spend most of each working day using it, and I’ve devoted much of my free time to maintaining and improving my setup. More than any of my other projects, it reflects my interests and creative abilities. So I’ll start from the beginning and describe the long, strange journey that brought me here.\n\n\nTaking the Plunge\nI first encountered Vim through MIT’s Missing Semester, a free online course in programming tools. One of those tools was Vim. Vim is a free text editor, developed and maintained since 1991 by Bram Moolenar. I had no experience with it, but knew it by reputation: very powerful and very hard to learn. I also considered Vim’s main competitor, Emacs, another free text editor with an equally distinguished history and a very different design philosophy. “Nemesis” might be a better word: partisans of each editor have spent the last three decades arguing about which is better. At the time, I was neutral in that endless conflict, but I decided to try using Vim. With practice, Vim was supposed to make you much more productive, and who wouldn’t want that?\nSo I installed Vim and fumbled my way through Vimtutor, the built-in tutorial mode that demonstrates basic motion and editing commands. (If you’re new to Vim, activate it with :Tutor). Then I tried doing my usual work in Vim. It went poorly. Using Vim for the first time feels like a nightmare. The complex, unfamiliar interface paralyzes you; even moving is hard. You try to escape, desperate to return to the familiarity of your old editor. You try every key combination you can think of, but nothing works; even quitting is a challenge. The experience is almost enough to scare you off from trying again\nA gentler introduction was necessary, I concluded. So I returned to my usual IDE, RStudio. RStudio delivers first-rate support for R coding, but it also has an unusually good set of Vim keybindings. So I turned them on and forced myself to use them. It was hard at first, but before long I got the hang of the commands. As I learned to delete whole words, move lines, and change visual selections, my productivity began to improve. But soon I craved more power than the keybindings gave me. I couldn’t execute complex commands or use Vim’s powerful regex implementation, because I was really just using a Vim-like interface for RStudio. I needed the real thing.\nThat meant finding a plugin (what Vim calls user-created extensions) that supported an R REPL like RStudio’s. I settled on Nvim-R, which I still use. But Nvim-R was not designed for compatibility with Vim. Instead, it required a variant of Vim: Neovim. Neovim, I learned, was a relatively new fork of Vim, promising an improved API, web embedding, Lua support, and other features that sounded exciting, though I hardly understood them Almost all Vim features were supported, but it implemented them by a different approach software.\nI tried setting up Neovim on my Windows machine, but I could not get it to work properly. It is possible to run Neovim on Windows machines, but I became so annoyed trying and failing to configure it that I turned to radical measures. It would be easier just to set up Neovim on a non-Windows machine, and the simplest way to do that was to install a virtual machine on my physical PC and then install Neovim there. So I downloaded VirtualBox and fumbled my way through setting up an Ubuntu VM, choosing Ubuntu for its reputation as the most noob-friendly Linux distribution. I had no clue what I was doing, but somehow I got everything configured correctly.\n\n\nFun and Games\nOver the next few months, I slowly came to grips with my new editor. I installed the R packages I needed and got an R REPL working. I got a basic language server running, too, mostly by blindly copying from the relevant plugin. (My current configuration, in sharp contrast, largely consists of judicious copying from the plugin). I kept practicing the Vim motions, trying to avoid anti-patterns like using the arrow keys, until they became familiar. It was hard going. Many times, something would break and send me back to RStudio. But I persevered and began expanding my configuration to new languages, such as Python and Lua. In time, my original workflow seemed as alien as this new one once had.\nWith time, Vim’s interface becomes familiar, then indispensable. Once the motion commands become muscle memory, it becomes easy to execute even complex movements in just a few keystrokes. You decide what to do - say, “go three lines down, jump to the end of the sentence, go back two words, and yank eveything inside the parentheses” - and express it (3j)2byi(). The interface is fast, versatile, and useful for all kinds of text editing. Going back to the vim keybindings available in other editors feels like drinking diet soda instead of the regular kind - a passable imitation, but clearly not the real thing.\nNeovim is making the classic system even more powerful by integrating TreeSitter, a program that parses the Abstract Syntax Tree of code in various languages. This makes it possible to use elements of language grammar as a basis of motion: you can select the expressions inside a function definition, or advance to the next branch of an if-else block.\nOne of Neovim’s headline features is a Lua API for VimScript, the specialized language Moolenar created for modifying and extending Vim. Vimscript does that job well enough, but is full of idiosyncrasies and has no applications outside Vim. I spent a little time learning it, but found it hard going. Lua, by contrast, was a joy to program in. Python is often billed as “interpretable pseudocode,” but I think Lua comes even closer to that ideal. (Well, at least JIT-compiled pseudocode). I found that its minimalist design and simple syntax ideally suited to customizing Neovim’s behavior. I had to stop myself from writing functions in my configuration files to solve interesting problems I encountered. The basic limits on Neovim’s capabilities, I came to realize, were not any design constraints, but my own programming skill and imagination.\n\n\nTaking Stock\nMy decision to try a new editor had escalated into a months-long quest to create a new working environment and customize my workflow. Put that way, it seemed almost absurd, as if I had resolved to go running on weekends and ended up becoming a semi-pro triathlete. It had been fun, it had been frustrating, it had been an irresponsible use of time I should have devoted to planning my career, and I could not honestly say I regretted any of it. Nor was it wholly wasted. Without really intending to, I had learned a complex API and used it to write useful code. I almost completely replaced my old GUI-based workflow with the terminal. And Vim itself has its uses. In my last semester, I found myself, as practically every programmer has at some point, working on a mysterious server via SSH, with no access to my usual editor. I typed which vim hopefully, and had all I needed.\nVim really did improve my productivity, I concluded. Using the fluid motion commands, powerful support for linewise transformations, and ease of integration with other shell tools, I could quickly carry out tasks I wouldn’t have even thought to attempt with a conventional IDE. The feeling is intoxicating - in fact, too intoxicating. After a few months, I came to another realization: I spent much more time editing my configuration than I saved using it to do actual work. I am the type who would rather spend an hour finding an interesting way to automate a boring task than ten minutes actually doing it. And if that’s procrastination, isn’t it the same impulse that spurs us to invent new technologies? You be the judge.\n\n\nAside: Should I Have Gone with Emacs?\nThe idea isn’t ridiculous. I know little of Emacs, but I have heard it described as even more versatile and configurable than Vim. I think Vim’s interface for text editing is superior (and so do a good number of Emacs users, considering the popularity of EVIL mode), but Emacs might have allowed me to indulge my configuration addiction even more shamelessly than Neovim. And, just as a McDonald’s manager would probably concede that Burger King’s onion rings are better, I have to admit that writing Emacs Lisp sounds more appealing than Lua-interfaced Vimscript. But I had to pick one editor, and I can’t say I regret my choice.\n\n\nConclusion\nAs I write this, I’m toying with five or six half-formed ideas for improving my Neovim configuration, some of which I might actually get around to implementing. It’s strangely reassuring to have a project I can never finish. Of course, getting too attached to my elaborate configuration might prevent me from jumping ship if an even better editor appeared. But if this unending adventure has taught me anything, it’s that adopting superior tools, striving to master them, and customizing them to your heart’s content is worth the investment. Still, for now, I’m happy where I am."
  },
  {
    "objectID": "posts/testthat/index.html",
    "href": "posts/testthat/index.html",
    "title": "Don’t Neglect Unit Testing",
    "section": "",
    "text": "Bad habits are hard to break. When I finish a first version of a function, I sometimes just make up an input, call it interactively, and move on if it seems to work, promising myself I’ll get around to properly testing it later. If someone asks me about it, I can only reply honestly: “I haven’t tested it thoroughly, but I’m pretty sure it works.” I do use and appreciate unit testing: when I wrote my first package, I tested it thoroughly, probably too thoroughly. Still, I often yield to the temptation of informal testing, and just as often regret it.\nI relearned this lesson recently while writing tests for the image classifier I’ve been working on for several months. It uses the torch package, which contains R bindings for the torch machine learning library. The classifier uses a customized subclass of a class called torch_dataset to implement a pretrained neural network. torch_dataset instances organize data for model training and evaluation. The subclass, called candidate_image_dataset, does this with file paths to images and class labels. Sampling an instance directly creates a tensor containing that image’s data; it also exposes an attributed called metadata, a data frame with each image’s file path and class label. The subclass can also be configured to return a randomly selected image when indexed. But if sampling is disabled, indexing the \\(i\\)th image directly should yield the image found in row \\(i\\) of metadata.\nAlmost as an afterthought, I added a test of the subclass’ .getitem method to confirm all this worked as described All it did was check that, for an instance \\(x\\) that did not use sampling, x$getitem(1) returned the image whose file path was stored in x$metadata[1,].\nI tend to be pessimistic, but I was still caught off guard when this trivial check failed. Unnerved, I looked through my code for the origin of the bug. The logical flag controlling whether an instance sampled indices did check whether the user had specified sampling weights, as expected:\n\nif (n_weights != 0) {\n  private$.sample_weights &lt;- self$compute_sample_weights(\n    images,\n    sample_weights\n  )\n  private$.uses_sample &lt;- TRUE\n}\n\nThat left only one possible culprit: a wrongly set default value. Sure enough, in the list of private attributes:\n\nprivate$.uses_sample &lt;- TRUE\n\nSo the class was sampling whether the user indicated it or not. I changed that TRUE to FALSE and the test passed. Had I not taken a few minutes to construct this seemingly unnecessary unit test, I would not have noticed the bug. I would only have caught it at a less convenient time, possibly after it compromised results obtained from the classifier. Verifying the assumptions you make about your code, even obvious ones, is never wasted time.\nUnit testing, at least for me, has another advantage: it’s tremendously motivating. When the console fills up with a traceback and a cryptic error message, I know I made a mistake somewhere. That makes me responsible for correcting it. I become very dogged when I have a clear goal in mind, so I seldom fail to track down and fix the bug. Pinning down the cause holds some interest, too, since it’s typically a surprise: anything from a misnamed variable to a subtle misunderstanding that compromises the whole algorithm.\nThen, having hopefully learned my lesson, it’s on to the next test case."
  },
  {
    "objectID": "posts/aoc-2022/index.html",
    "href": "posts/aoc-2022/index.html",
    "title": "Reflections on Advent of Code 2022",
    "section": "",
    "text": "Overview\nAdvent of Code 2022 is over. After a few frantic weeks, I’ve completed both parts of all 25 puzzles. And, like most years, it was a wild ride, full of frustration and triumph in equal measure. Eric Wastl and his team have once again delivered. When I wasn’t coding, I was scanning documentation or trawling the subreddit for hints. To my surprise, I managed to finish the whole calendar before the New Year. The year felt a little easier than 2021, though of course I have much more experience than I did then.\nThis year’s narrative had the player assisting the Elves’ expedition deep into the jungle, in search of a rare fruit that powers the reindeer’s flight. This premise has lower stakes than past years, which had you scrambling to prevent some catastrophe from ruining Christmas, but the change of pace is welcome.\nWe all know the plot is just a device to string puzzle premises together, but I make special note of it because I think it’s the secret to Advent of Code’s popularity. Problems are just more enjoyable when framed by a silly scenario. “Find the path on this unidirected graph maximizing this dynamic weighting function” is a work specification; “Find which valves to open release as much pressure as possible so you and the herd of elephants following you can escape the erupting volcano” is fun.\n(One other amusing consequence of the narrative: if you follow the common style advice to name variables and functions in terms of the problem you are modeling, your code will teem with names like ModulusMonkey and nearest_elf).\nI set out to use as many languages as possible, even those I lacked confidence in. For the first week or so, I found this easier than expected. Even JavaScript, which I hadn’t used seriously in a long time, I found simple enough to use. But it couldn’t last. As the puzzles got tougher past day 10 or so, I found myself retreating to the familiarity of Python. Maybe I’ll do better in this regard next year.\nHere’s a plot showing my choice of language for each day with the cumulative line count:\n\nsource_env &lt;- new.env()\nsource_env$years &lt;- 2022\nsource_env$csv &lt;- \"../../data/line_counts.csv\"\nsource(\"../../scripts/plot_lines.R\", local = source_env)\n\n\n\n\n\n\n\nMethods\nIf this year had a theme, it was Cartesian coordinates. They show up in at least a few puzzles each year, but they showed up again and again this year. The concept often appeared with a clever twist, such as modeling falling shapes (day 17) or computing three-dimensional surface areas (day 18).\nFor these types of problems, I like to use a sparse map that stores only coordinates that matter for the problem. For day 18, for example, this was coordinates covered by a falling shape. I once used tuples to represent even two-dimensional coordinates, but now I use a tried-and-true hack: complex numbers. Not only is this trick effective, it gives me the guilty thrill of using a workaround it would be reckless to use in production code.\nAs always, a few problems frustrated me. Day 9 caught me off guard. It’s tricky, but nothing terrible: each knot on the rope follows simple movement rules that you can infer by studying the example input. Instead, I developed a complicated solution for part 1 that wouldn’t generalize to part 2. The problem became easy when I took a break and realized all I had to do was keep each knot adjacent to its leading knot.\nDay 11 stymied me for a different reason: it could be solved a simple trick that I just couldn’t see. Part 2 introduces a rule change that allows certain values to grow without bound. You have to recognize that they follow a cycle whose period is the lowest common multiple of divisors specified in the problem, then modulo divide by that value. This isn’t too hard to figure out, but I missed it because I attacked the problem without thinking it through. I ended up finding the correct method only after trying almost everything else. A little number theory would have saved me a lot of trouble.\nI also got badly stuck on day 16, but that one was horrible for everyone, so there’s nothing to analyze.\nDifficulty\nI think the puzzle design this year was perhaps the best yet. Many puzzles were hard, some very hard, but none were complicated. I think good puzzles test two things: modeling the important features of a problem and to to adapting a standard algorithm to a novel situation. Those with complicated rules introduce an additional challenge: correctly implementing every detail of the specification. This rewards diligence more than insight and means even tiny mistakes will yield the wrong answer. Complication isn’t _unfair per se, and most real software specifications are very complicated. But few people consider attending to minutiae the best part of programming; consider the mixed reception to 2018 day 15, which I’m not even brave enough to attempt.\nThis year, the hard puzzles were hard because a straightforward approach could not solve them. Days 16 and 19, probably the hardest, presented tough pathfinding scenarios. You had to wrangle each input into a graph structure (possibly implicit), then find a way to reduce the problem space to a manageable size that brepth-first search could handle. Without some clever optimization, any solution would probably take much too long Both tasks involve lateral thinking and creativity; implementing Wikipedia’s Dijkstra pseudocode wouldn’t cut it. Some puzzles involved simulation, but not with complicated rules; the difficulty instead came from understanding the subtleties in the interaction of simple rules (e.g., Day 9). Overall, the puzzles have become leaner but no less challenging. I appreciate the work it must have taken to refine a successful formula and make it still better.\nI also encountered, or implemented for the first time, several other useful techniques: cycle detection, bit-encoding subsets, flood-filling, and more. I never come out of a year without at least a few new tricks.\nMy favorite puzzle was day 21. I solved it in R. Part 1 is fairly simple, requiring you to parse and evaluate a large expression recursively. Part 2 introduces a nasty escalation: one of the names referenced in the expression (the one labeled “humn”, naturally, since the others refer to monkeys) is bound to an unknown value, not a constant, and you have to solve for it.\nI’ve always liked expression manipulation. R, with its strong LISP influence, makes it easy to convert between R code, objects representing the code’s abstract syntax tree, and S-expression-like lists of functions and arguments. So I attacked part 2 by converting the equation to be solved into a nested list of S-expressions. Since the right side of the equation resolved to a constant, I could solve it by simply inverting each operation on the left-hand side until I had isolated the unknown. I learned later that several other approaches worked well, such as finding the solution by binary search or using complex-number hackery. It was fun and rewarding to work through, and the part 2 twist is a classic.\nClosing Thoughts\nOverall, it was a fun and rewarding year, and I’m mostly pleased with my performance. I approached puzzles strategically instead of instinctively. I mostly picked the right data structures, and avoided overengineering my solutions. I knew enough about pathfinding algorithms to get through the tougher puzzles. Don’t get me wrong: I’m a long way from the people who can crank out an idiomatic Python solution that runs in 50 microseconds any given day. But I’m improving, and that pleases me.\nI had better learn the Chinese Remainder Theorem before December 2023, though. Can’t be too hard, right?"
  },
  {
    "objectID": "posts/lexical-scope/index.html",
    "href": "posts/lexical-scope/index.html",
    "title": "Lexical Scope: Who Does It Best?",
    "section": "",
    "text": "Introduction\nMost programming languages have some notion of scope: a frame of names bound to values. A program will involve many interlocking scopes, all associated with constructs like functions and (in some languages) blocks. Each language has its own rules for managing scope. These rules determine how references to variable names are resolved at runtime. They typically organize scopes into a hierarchy and determine which scope “wins” when the same name exists in multiple scopes visible at a given time. An important part of scoping rules is what happens when a name is looked up in a scope where it is not bound to any object. As we will see, different languages have different ways of handling this case.\nA common pattern is called lexical scope. This means that objects, particularly functions, consider their parent scope to be the scope where they were defined. In a lexically scoped language, if a function can’t find an object called x in its execution environment, the interpreter or compiler would look next in whichever environment the function was defined in (often called the enclosing environment). This behavior has the advantage of consistency, since a given function’s enclosing environment is the same no matter where the function is called.\nLanguages with lexical scope often allow assignments in a function’s enclosing environment. If a function is defined in the global environment, this is usually bad idea, since it violates the principle that functions should avoid unnecessary side effects. But side effects are no issue if that enclosing environment is another function’s execution environment. Suppose f is a function that returns a function g when called. g’s enclosing environment, the place it was defined, is f’s execution environment. So any binding g makes in its enclosing environment when it is called is visible only to subsequent calls to g! This fact makes it possible for functions to read and write a private cache, enabling a host of powerful programming techniques.\nThis post compares the syntax several languages use for assignments in enclosing scope. We’ll implement a common pattern in each and contrast the languages’ very different means of supporting the same basic idea.\nThat pattern is called memoization. It can be used to write functions that “remember” their result for a given input after computing it the first time. This approach trades performance for memory; used well, it can speed up code by saving the results of expensive computations instead of repeating them.\nWe’ll compare how several popular languages approach lexical scope using a simple application called neighbors. neighbors is a function that returns a memoized function that finds the four non-diagonal neighbors of a point on a two-dimensional Cartesian grid. Each time this function is called on a new point, it computes that point’s neighbors and stores them in an associative data structure in its enclosing environment. Subsequent calls with the same argument find the cached value and return it, preventing repeated computation. While trivial, this technique might be useful on certain Advent of Code problems, where it might be necessary to check a point’s neighbors many times.\nFor simplicity, I make no attempt to verify that inputs as two-dimensional coordinates, which a real implementation would have to do.\nPython\nWe may as well start with good old Python. One of Python’s core design principles is “explicit is better than implicit.” So it is with lexical scope. If you want to modify a variable in the global environment from within a function (which you probably shouldn’t), you have to use the global keyword:\n\nx = 5\n\ndef modify():\n    global x\n    x = 6\nmodify()\nprint(x)\n\n-- 6\n\n\nThere is a different keyword, nonlocal, that should be used when the enclosing environemnt is a function’s evaluation environment. Here is neighbors in Python:\n\ndef neighbors():\n    memo = {}\n    def inner(x):\n        nonlocal memo\n        if x not in memo.keys():\n            memo[x] = ((x[0] - 1, x[1]),\n            (x[0] + 1, x[1]),\n            (x[0], x[1] - 1),\n            (x[0], x[1] + 1))\n        return memo[x]\n    return inner\n\nget_neighbors = neighbors()\nget_neighbors((2, 3))\n\n-- ((1, 3), (3, 3), (2, 2), (2, 4))\n\nget_neighbors((1, 4))\n\n-- ((0, 4), (2, 4), (1, 3), (1, 5))\n\nget_neighbors((2, 3))\n\n-- ((1, 3), (3, 3), (2, 2), (2, 4))\n\n\nThis implementation shows off one of my favorite Python features: any immutable object can serve as a dict key, not just strings. We can just use coordinates as indices. The use of keywords makes it impossible to use global data unless you really want to, but it’s hardly elegant.\nI could have avoided most of the work above by just writing a function to compute neighbors and adding the cache decorator from the functools module. (In Python, decorators are functions that modify other functions. A special syntax exists for applying them). That would have memoized the function automatically. But I think it’s instructive to demonstrate the concept in pure Python.\nBash\nSpeaking of elegance, we’ll now consider a language about as elegant as an A-10: Bash. Bash is designed for interactively managing filesystems and operating systems, or writing scripts that do the same thing. Its syntax isn’t pretty, but it’s brutally effective in its role.\nBut this pattern can’t be implemented in Bash, so far as I know. Bash is dynamically scoped, not lexically scoped. That means a function that fails to find a variable in its local scope will next look in its caller’s scope, then the caller’s caller’s scope, and so on, instead of the scope where it was defined. If we tried to implement a Python-style neighbors in Bash like this:\n\n\nneighbors(){\n    declare -A memo\n    local code='get_neighbors(){\n    local x=\"$1\"\n    local y=\"$2\"\n    local hash = \"$x,$y\"\n\n    if [ ! -v memo[\"$hash\"] ]; then\n        local west=\"$(($x-1)),$y\"\n        local east=\"$(($x+1)),$y\"\n        local south=\"$x,$(($y -1))\"\n        local north=\"$x,$(($y +1))\"\n        memo+=([\"$hash\"]=\"$north-$east-$south-$west\")\n    fi\n}'\nexec \"$code\"\n}\n\nthe function it returned (to be exact, the function generated by the code string it evaluated ) would not “remember” the memo array, because bindings in the scope where the array was defined would not be preserved. Even if we ignore that fact, Bash is a bad choice for this problem because it doesn’t support multidimensional arrays, so I had to resort to some ugly code to represent the coordinates as a string. Bash is indispensable in many situations, but not this one.\nLua\nLua is a fast, lightweight scripting language. Though its design is minimalist, with simple syntax and few data structures, it does support lexical scope, enabling the usual functional programming tricks.\n\npresent_table = function(x)\n    result = {}\n    for i, tab in ipairs(x) do\n        result[i] = \"{\" .. table.concat(tab, \", \") .. \"}\"\n    end\n    return \"{\" .. table.concat(result, \", \") .. \"}\"\nend\n\nneighbors = function()\n    local memo = {}\n    return function(x)\n        local hash = table.concat(x, \",\")\n        if memo[hash] == nil then\n            memo[hash] = {\n                { x[1] - 1, x[2] },\n                { x[1] + 1, x[2] },\n                { x[1], x[2] - 1 },\n                { x[1], x[2] + 1 },\n            }\n        end\n        return memo[hash]\n    end\nend\n\nget_neighbors = neighbors()\nprint(present_table(get_neighbors({ 2, 3 })))\nprint(present_table(get_neighbors({ 1, 4 })))\nprint(present_table(get_neighbors({ 2, 3 })))\n\n-- {{1, 3}, {3, 3}, {2, 2}, {2, 4}}\n-- {{0, 4}, {2, 4}, {1, 3}, {1, 5}}\n-- {{1, 3}, {3, 3}, {2, 2}, {2, 4}}\n\n\nLua automatically finds the memo table (what Lua calls its all-purpose record data structure) in the enclosing scope and modifies it. I had to write my own function to print tables, however, because Lua only prints a table’s memory address by default. The only other wrinkle is that Lua makes variables global by default. You have to use the local keyword to bind variables in a function’s execution scope (or make them local to a script). Since creating global variables from functions is usually a bad idea, I think it should be the other way around, as in Python, but that’s a matter of preference.\nThis snippet demonstrates another neat feature of Lua: looking up a nonexistent table index or unbound variable returns nil instead of an error, making it safe to test for index existence.\nJavaScript\nLike Bash, JavaScript is respected for its usefulness and ubiquity rather than its elegance. Say what you will about JavaScript’s loose typing, design inconsistencies, and erratic syntax, it isn’t Python that powers the modern Web.\nAs in Lua, there’s no need to use special syntax to write in the enclosing scope. But I do have to use JavaScript’s declaration keywords, which have characteristically intricate rules. I need to use let inside the function instead of const to avoid creating global variables; var has yet another assignment behavior. Still, it’s easy enough to express the functional idiom:\n\nfunction neighbors(){\n    let memo = {};\n\n    return function(x){\n        let hash = x.toString();\n        if(!(hash in memo)){\n            memo[hash] = [[x[0] -1, x[1]],\n            [x[0] + 1, x[1]],\n            [x[0], x[1] - 1],\n            [x[0], x[1] + 1]];\n        }\n        return memo[hash];\n    }\n}\nconst get_neighbors = neighbors();\nconsole.log(get_neighbors([ 2, 3 ]));\nconsole.log(get_neighbors([ 1, 4 ]));\nconsole.log(get_neighbors([ 2, 3 ]));\n\n-- [ [ 1, 3 ], [ 3, 3 ], [ 2, 2 ], [ 2, 4 ] ]\n-- [ [ 0, 4 ], [ 2, 4 ], [ 1, 3 ], [ 1, 5 ] ]\n-- [ [ 1, 3 ], [ 3, 3 ], [ 2, 2 ], [ 2, 4 ] ]\n\n\nSQL\nSurprisingly, SQL provides the most elegant interface for functional programming. Strange as it sounds, its expressive, if finicky, declarative syntax enables powerful idioms. With creative use of GROUP BY clauses and certain window functions, it is possible to -\nJust kidding. But it probably is possible, albeit silly, to implement neighbors in procedural SQL.\nR\nAs in many things, I’m biased toward R because it was my first language. But I think R wins this comparison on the merits.\nR has the most elegant solution to writing in the enclosing environment. The regular assignment operator, &lt;-, only ever binds in its caller environment (and yes, it technically is a function, so it’s correct to speak of its caller environment). Instead, R implements assignment in enclosing environments using a _super_assignment operator, &lt;&lt;-. (In R, environments are first-class objects that record their parent environments). This operator checks the parent of the caller environment for a binding with the same name as the one being used, and overwrites that binding if it exists. If the name is not bound in the parent environment, it repeats this process for each parent of that environment, and finally binds the name in the global environment if it is not defined anywhere.\nThis power can be used for evil, as in this snippet that modifies a global variable:\n\nf1 &lt;- function() {\n  x &lt;&lt;- 5\n}\nf1()\nx\n\n-- [1] 5\n\n\nOr for good, as in this version of neighbors. It works because closures, the type R uses to implement most functions, record the environment where they were defined.\n\nneighbors &lt;- function() {\n  memo &lt;- new.env(hash = TRUE)\n  function(x) {\n    hash &lt;- paste(as.character(x), collapse = \"\")\n    result &lt;- tryCatch(get(hash, memo), error = function(e) {\n      result &lt;- list(\n        c(x[[1]] - 1, x[[2]]),\n        c(x[[1]] + 1, x[[2]]),\n        c(x[[1]], x[[2]] - 1),\n        c(x[[1]], x[[2]] + 1)\n      )\n      memo[[hash]] &lt;&lt;- result\n      result\n    })\n    result\n  }\n}\nget_neighbors &lt;- neighbors()\nget_neighbors(c(2, 3))\n\n-- [[1]]\n-- [1] 1 3\n-- \n-- [[2]]\n-- [1] 3 3\n-- \n-- [[3]]\n-- [1] 2 2\n-- \n-- [[4]]\n-- [1] 2 4\n\nget_neighbors(c(1, 4))\n\n-- [[1]]\n-- [1] 0 4\n-- \n-- [[2]]\n-- [1] 2 4\n-- \n-- [[3]]\n-- [1] 1 3\n-- \n-- [[4]]\n-- [1] 1 5\n\nget_neighbors(c(2, 3))\n\n-- [[1]]\n-- [1] 1 3\n-- \n-- [[2]]\n-- [1] 3 3\n-- \n-- [[3]]\n-- [1] 2 2\n-- \n-- [[4]]\n-- [1] 2 4\n\n\nNotice how &lt;&lt;- still works correctly, even embedded in an anonymous function passed as an error handler to tryCatch. The one annoyance is that numeric vectors can’t be used as names.\nConclusion\nAt the end of the day, broad techniques like memoization transcend any individual language. All the languages above (except Bash) make it relatively easy to use memoization. As with most programming concepts, translating the principles into any given language is much easier than understanding them in the first place. Still, the comparison here highlighted each language’s character: Python’s explicitness, Lua’s simplicity, and R’s lovable blend of elegance and jankiness. (Also worth noting: the languages featured each call their associative data structure something different).\nI like R’s approach best. A special version of the assignment operator clearly signals that something unusual is being done, and it is visually distinct yet hard to type by accident. (RStudio, the premier R IDE, has a keyboard shortcut for &lt;- that makes mistakenly typing &lt;&lt;- very unlikely). But which is your favorite?"
  },
  {
    "objectID": "posts/chatGPT-test/index.html",
    "href": "posts/chatGPT-test/index.html",
    "title": "ChatGPT Has a Long Way to Go",
    "section": "",
    "text": "Like everyone else, I’ve spent some time over the past few months toying with ChatGPT, the new chatbot trained on the latest publicly available version of OpenAI’s large language model.\nYou may already have heard about ChatGPT’s impressive capabilities. It can write code to specifications, answer factual questions, and even write coherent essays.\nThe second capability interests me the most. I served as a TA in graduate school, and I spend a good deal of my free time helping people with R problems. Both roles, involve helping people debug or improve their code and answering their questions about the subject. ChatGPT can do both tasks, but is it good enough to render me obsolete?\nFor now, I think the answer is no. ChatGPT’s limitations become obvious with experience. It refuses to take a stance on anything remotely controversial. A typical response will give one viewpoint, say “On the other hand…”, then express the other. Its prose is, well, mechanical: it composes long, monotonous sentences with bland, inexact diction. Writers for content farms will soon become obsolete, but anyone skilled is safe for now.\nThese are are mere annoyances. One quirk, however, is dangerous to naive users: ChatGPT will confidently give you false information. Like an arrogant party guest who will invent an answer to any question you ask them rather than admit, “I don’t know,” it always seems to have a coherent response - just not necessarily the correct one.\nI fed ChatGPT a few technical questions to see if I could catch it in an error. To my surprise, it knew how to derive the number \\(e\\) from a Taylor series, which takes some knowledge of calculus:\n\nI followed up with an outright trick question:\n\nGotcha. Actually, Python tuples can contain references to mutable objects, even though tuples themselves are immutable. It would be silly ever to do this, but you can:\n\nx = [1, 2, 3]\ny = (x, 4, (1, 2))\ny[0].append(1)\nprint(y)\n\n([1, 2, 3, 1], 4, (1, 2))\n\n\nI could have declared victory there, but I pressed on. People are often nonplussed when I tell them I enjoyed linear algebra class. Hence the next question: what are the possible numbers of solutions to a system of linear equations?\nChatGPT replied with this:\n\nThis is, at best, a distortion of the truth. We may as well do this rigorously. Let \\(A\\) be the coefficient matrix and of the equation \\(Ax=b\\). A system with more unknowns than equations has either no solution or infinitely many solutions. A system with more equations than unknowns may have zero solutions, one solution, or infinitely many solutions. A system with exactly as many unknowns as equations may also have zero, one, or infinitely many solutions.\nIn each case, at least one solution exists if and only if \\(b\\) lies entirely within the image of \\(A\\) (the linear space spanned by its column vectors). In the first case, a unique solution is impossible because the column vectors of \\(A\\) must be linearly dependent. \\(n\\) linearly independent vectors span a vector space of dimension \\(n\\), such that any vector within it corresponds to a unique linear combination of the vectors. But more unknowns than equations means more than \\(n\\) vectors, meaning at least one must be redundant. So in this case, there are infinitely many ways of forming vectors in \\(A\\)’s image from its columns, and hence either zero or infinitely many solutions.\nIn the other two cases, at most one solution exists if the rank of the matrix (the number of linearly independent column vectors, or, equivalently, row vectors) is at least the number of unknowns. Then the column vectors form a basis for the image of \\(A\\), so any solutions must be unique. This is not a sufficient criterion for a matrix with more unknowns than equations: \\(b\\) may not be in the image of \\(A\\), in which case no solution exists. If the numbers of equations and unknowns are equal, of course, the rank equaling the number of columns means it also equals the number of rows, ensuring a solution because the columns of \\(A\\) form a basis for a \\(n\\)-dimensional vector space. In general, one or more solutions must exist if and only if the rank of \\(A\\) is at least the number of rows, and a solution is unique (assuming it exists) if and only if the rank is at least the number of columns. Only if the rank is equal to both quantities is a unique solution guaranteed.\nThe reference to determinants also misleads, since they are only defined if the number of equations and the number of unknowns are equal and \\(A\\) is square.\nHow many unwitting linear algebra students have asked this innocent question and received a response this misleading? Probably at least some.\nIn the unlikely event you’re still reading this, I was tempted to ask ChatGPT for more information about its fascinating theory of heterodox linear algebra. But instead I corrected it.\n\nHere we see a common pattern: ChatGPT confidently says something wrong. The user pushes back. ChatGPT meekly agrees. It spouts garbled nonsense, like a student faking understanding. The illusion that the bot is actually reasoning collapses.\nChatGPT is a major advance, perhaps a milestone. It casts a convincing illusion of intelligence. Unlike earlier chatbots, it is smart and reliable enough to have real uses. But anyone approaching it as an oracle is bound to be disappointed. None of its factual claims, especially about technical subjects, should be trusted without verification. But if you have to do your own research anyway to confirm what it tells you, why ask it for help to begin with?\nAs a programmer, I suspect ChatGPT or its successors will begin automating parts of my job soon enough. I feel a twinge of unease at the prospect. But the day AI replaces us outright, if it will ever come, is not nigh."
  },
  {
    "objectID": "posts/two-years/index.html",
    "href": "posts/two-years/index.html",
    "title": "What a Difference Two Years Makes",
    "section": "",
    "text": "This post contains spoilers for day 19 of Advent of Code 2017.\nIn the past few week, I’ve gone back and completed some Advent of Code puzzles I never got around to finishing. While working through 2017, I came across Day 19. It asks you to traverse a line that extends across a map, with several changes of direction. In typical Advent of Code fashion, everything is presented as as a file of ASCII characters. (The title is also a quality reference.)\nI first attempted the puzzle in the summer of 2021, when I had no clue what I was doing. I plunged in without a plan, and the code got messier and messier. I thought it made sense to represent the map as a matrix, but it made keeping track of position complicated. I hadn’t yet learned that nothing makes a puzzle harder than the wrong choice of data structure. I eventually hacked my way to a correct solution, but it was kludgy and slow. I felt exhausted rather than triumphant.\nI won’t subject you to the whole script. The first lines, containing the helper functions I wrote, give enough of an impression:\n\nlibrary(magrittr)\n\nindex2coords &lt;- function(X, index) {\n  c(\n    ifelse(index %% nrow(X) == 0, nrow(X), index %% nrow(X)),\n    index %% ncol(X) + 1\n  )\n}\ncoords2index &lt;- function(X, coords) {\n  coords[1] + nrow(X) * coords[2]\n}\n\nnext_direction &lt;- function(input, coords, last_dir, passed, steps) {\n  candidates &lt;- surrounds(input, coords)\n  excludes &lt;- c(\n    l = \"r\",\n    r = \"l\",\n    u = \"d\",\n    d = \"u\"\n  )\n  candidates &lt;-\n    candidates[rownames(candidates) != excludes[last_dir], ]\n  next_dir &lt;-\n    rownames(candidates[input[candidates] != \" \", , drop = FALSE])\n  print(coords)\n  traverse(input,\n    coords,\n    dir = next_dir,\n    passed = passed,\n    steps = steps\n  )\n}\nsurrounds &lt;- function(input, coords) {\n  base &lt;- c(1, -1, 0, 0)\n  adds &lt;- cbind(base, rev(base))\n  out &lt;- sweep(adds,\n    FUN = `+`,\n    STATS = c(coords),\n    MARGIN = 2\n  )\n  rownames(out) &lt;- c(\"d\", \"u\", \"l\", \"r\")\n  out[validate_coords(input, coords), ]\n}\nvalidate_coords &lt;- function(input, coords) {\n  dims &lt;- dim(input)\n  apply(coords, MARGIN = 1, function(x) {\n    all(x &gt; 0 && x &lt;= dims)\n  })\n}\n\nVerbose and complicated.\nWondering how I could do better, I found a concise Python solution to the puzzle. It cleverly used a dict of complex numbers to represent the map - a classic hack I had never though to use. I remembered the trick and moved on to to other problems.\nA few months shy of two years passed. Then, one morning, I came back to the puzzle. In about twenty leisurely minutes, I came up with this:\n\nfrom operator import attrgetter\n\n\ndef parse(lines):\n    result = {}\n    for j, line in enumerate(lines):\n        for i, char in enumerate(line):\n            if char != \" \":\n                result[complex(i, j)] = char\n    return result\n\n\ndef neighbors(point):\n    return {\n        complex(point.real - 1, point.imag): -1 + 0j,\n        complex(point.real + 1, point.imag): 1 + 0j,\n        complex(point.real, point.imag - 1): 0 - 1j,\n        complex(point.real, point.imag + 1): 0 + 1j,\n    }\n\n\nwith open(\"inputs/day19.txt\") as f:\n    raw_input = f.read().splitlines()\n\ngrid = parse(raw_input)\nposition = min(grid.keys(), key=attrgetter(\"imag\"))\ndirection = 0 + 1j\nfound = \"\"\ntraversed = 0\nchar = \"-\"\n\nwhile char:\n    last = position\n    position += direction\n    char = grid.get(position, \"\")\n    traversed += 1\n    if char.isalpha():\n        found += char\n    elif char == \"+\":\n        this_neighbors = neighbors(position)\n        for dest, dir in this_neighbors.items():\n            if grid.get(dest) and dest != last:\n                direction = dir\n                break\n\nprint(found)\nprint(traversed)\n\nThis is merely workmanlike, but compared to my original solution it may as well be the source for the Apollo Guidance Computer. I implemented the complex-number approach, as I now know to do, and the code practically wrote itself. After parsing the map, I simply have to keep track of the position and direction of motion and update the motion one unit at a time. Changing directions at one of the junctions on the road the puzzle simulates just means checking neighbors for the road’s continuation. I use the empty string to represent the end of the road and terminate the while loop.\nLooking back, I realized this puzzle wasn’t really hard, at least for Advent of Code. (For an example of “hard”, see here. I struggled the first time from inexperience. With experience, it came easily. So easily that it seems amazing the problem ever seemed hard."
  },
  {
    "objectID": "posts/aoc-complete/index.html",
    "href": "posts/aoc-complete/index.html",
    "title": "Completing Advent of Code",
    "section": "",
    "text": "At long last, I’ve earned all 400 stars available in Advent of Code. I have implemented every possible variation of Dijkstra’s algorithm. I have spent hours scouring the subreddit for help threads tagged “[20XX Day YY].” I have become a lot more acquainted with the Python debugger than I would like. I don’t regret that considerable investment of time, so I thought it would be worthwhile to reflect on it.\nI started participating in Advent of Code in 2020, shortly after entering a data science master’s program. Since then, I’ve found myself returning to it whenever I have free time. The appeal of the puzzles never seems to dull.\nWhen I began the final effort to “finish Advent of Code”, most of my backlog consisted of 2017 through 2019, plus some unfinished puzzles from 2020. At first, I did the puzzles roughly in order, year by year. Midway through 2019, when the difficulty began to ramp up, I diverted to 2018. I did the final, hardest puzzles out of sequence.\nThis was a long journey, undertaken in fits and starts the time I could spare over the past two and a half years. Some puzzles I tore through in a few minutes, almost without thinking; many took hours of effort. A few took days of attempts to crack. While I sometimes felt frustration, I enjoyed most of the process, and nothing dimmed my determination to finish.\nNaturally, a few puzzles became my favorites:\n\n2018 Day 23. It presents a simple but very hard problem: find the nearest point in a vast three-dimensional space that falls within the greatest number of octohedra (the shape formed by defining a center and a radius measured by Manhattan distance). After some research, I solved it using octary search - a three-dimensional generalization of binary search.\n2020 Day 20. This puzzle, the notorious “Jurassic Jigsaw,” involves reassembling a grid of tiles into an image and finding a complex pattern within it. I found it very hard, but quite satisfying, to recover the image using a variation of depth-first search.\n2018 Day 22. A fun variation on the well-worn graph traversal format. In this puzzle, the graph represents an unexplored cave the player has to navigate using their equipment.\n\nI had one clear least favorite: 2019 Day 22. Like many people, I found this puzzle almost impossible. It asks you to simulate shuffling a deck of trillions of cards trillions of times. Solving it before the end of our civilization requires subtle modular arithmetic. I could only solve it by following this tutorial. The experience makes me wish I’d taken a course in number theory.\nIntcode\nI ought to weigh in on the most controversial puzzle element in the series: Intcode. Featured in the 2019 iteration, Intcode is a made-up CPU language that relies on integer parameters and opcodes. Intcode programs take the form of lists of comma-separated integers. About half of 2019’s puzzles were distributed in this format.\nAn early puzzle challenges the player to create an Intcode virtual machine and execute a simple program. Later puzzles complicate the Intcode specification and feature more elaborate programs. The player has to update their VM in order to solve them. The final puzzle, in most years a simple exercise, became something special: an interactive adventure game written entirely in Intcode.\nIntcode seems to have polarized the community. Some laud it for adding a new type of challenge in requiring the player to design and upgrade a VM. They point out that it powered an exciting new kind of interactive puzzle that allowed the player to control an Intcode program directly. Others object that Intcode was complicated, finicky, and difficult to understand for anyone who hadn’t taken a compilers course (such as yours truly). And the cumulative requirements for the player’s VM made it easy to fall behind.\nI agree with both viewpoints. I found it difficult to implement the Intcode VM; the end result worked, but was brittle and complicated. One subtlety within the instructions (the distinction between “relative” and “absolute” parameter modes) cost me a few hours before I found a Reddit post explaining the difference. But it was worth it. I found the later Intcode puzzles fun and inventive. They include simplified versions of Frogger and Breakout - a welcome change of pace from variations on the Game of Life or graph pathfinding. I can see why the experiment wasn’t repeated, but I think Intcode was a success overall, and certainly something that set 2019 apart.\nBefore I wrap up, here are plots showing cumulative lines of code by language for each year.\n\nsource_env &lt;- new.env()\nsource_env$years &lt;- 2015:2022\nsource_env$csv &lt;- \"../../data/line_counts.csv\"\nsource(\"../../scripts/plot_lines.R\", local = source_env)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLast Thoughts\nI learned a good deal in the process. I got to practice applying Dijkstra’s algorithm, depth-first search, and (yes) modular arithmetic. Intcode, as noted, was a sometimes harsh lesson in how to meet an evolving specification. But I think the most important skill Advent of Code teaches is how to translate convoluted problem statements into code. Much of the fun comes from uncovering the well-posed problem buried in the elaborate elf-related scenario presented by the narrative text. That experience comes in handy in real life, where requirements are rarely so clear.\nAbove all, I was left with an appreciation for the hard work it must have taken to create Advent of Code. The puzzles often return to the same concepts, but each is unique. And the puzzle text itself is always both a precise problem statement and a continuation of the charmingly silly narrative that ties each year together. The whole experience remains cohesive and rewarding, even hundreds of puzzles in.\nWhat next? I could certainly improve my solutions; some contain dubious hacks or take an uncomfortably long time to run. But I think I’ll find other projects to occupy me - at least until next December, that is."
  },
  {
    "objectID": "posts/social-network-china/index.html",
    "href": "posts/social-network-china/index.html",
    "title": "Erica Albright was Right",
    "section": "",
    "text": "Zuckerberg: Did you know there are more people with genius IQs living in China than there are people of any kind living in the United States?\nAlbright: That can’t possibly be true.\nZuckerberg: It is.\nAlbright: What would account for that?\n\n\n\nAlbright’s intuition is right, though Zuckerberg derails the conversation before she can challenge him further. Some calculations show that Zuckerberg’s claim is ridiculous.\nEstablishing Numbers\nAccording to the World Bank, in 2003 China had 1.29 billion people and the United States had 290,108,000 (rounding to the nearest hundred, not that it matters).\nWhile there are many ways to define genius, I’ll use intelligence quotient, a well-known measure that is easy to work with. IQ is typically described as normally distributed, with mean 100 and standard deviation about 15. There is no universally accepted cutoff for a “genius” IQ. To be charitable, I’ll define it as 130 or greater, a mere two standard deviations above the mean.\nIf we assume each country has the same theoretical IQ distribution, how many geniuses would we expect in each?\nCalculations\n\nus_pop <- 290108000\nchina_pop <- 1290000000\nmu <- 100\nsigma <- 15\ngenius <- 130\ncutoff <- pnorm(genius, mu, sigma, lower.tail = FALSE)\nus_geniuses <- floor(cutoff * us_pop)\nchina_geniuses <- floor(cutoff * china_pop)\n\nThe China-US ratio is 4.45, so under these assumptions China has that many times more geniuses. But the ratio of U.S. population to Chinese geniuses is 9.89. There would need to be almost 10 times more Chinese geniuses than there are under these assumptions for Zuckerberg to be correct. If we use a higher IQ score as the genius cutoff, the ratio gets even worse.\nThere are infinitely many implausible IQ distributions that would justify Zuckerberg’s claim. For simplicity, let us pose an optimization problem. Given China’s actual population and a U.S. mean IQ of 100, how high must China’s mean IQ be for Zuckerberg’s claim to be correct? That is, what is the lowest mean IQ value for which the right tail area of the normal CDF is greater than the ratio of US to Chinese population?\n\nvalidate_mean <- function(mu) {\n  floor(pnorm(genius, mu, sigma, lower.tail = FALSE) * china_pop) > us_pop\n}\ngrid <- seq(100, 150, 0.01)\nminimum <- min(grid[validate_mean(grid)])\nminimum\n\n-- [1] 118.67\n\n\nZuckerberg’s claim would require the average Chinese to have an IQ more than one standard deviation above the U.S. average. I couldn’t find reliable IQ data for China, and intelligence is notoriously hard to measure. But for the claim to be true, the average Chinese would be more intelligent than about 89.3% of Americans, which is unbelievable.\nSupposing instead China has the same IQ distribution as the United States, how many Chinese would there have to be for Zuckerberg’s claim to be correct? This can be solved algebraically:\n\nfloor(us_pop / cutoff + 1)\n\n-- [1] 12751926040\n\n\nThat is about 10 times the actual 2003 population of China.\nConclusion\nIn fairness to the real Zuckerberg, this conversation is fictional. As a computer science sophomore at an elite university, he likely understood the properties of normal distributions well enough not to make this ridiculous claim. Or maybe he’s trying to impress a listener he thinks is naive with nonsense. That’s how I’ve always interpreted Han Solo’s nonsensical claim in Star Wars that the Millennium Falcon can make the Kessel Run in less than twelve parsecs. Either way, Albright is clearly right.\nOne other note: During the hacking montage that follows the tavern scene, Zuckerberg says (in voiceover), “So it’s definitely necessary to break out Emacs and modify that Perl script.” As a Neovim user, I’m required to remind you it is never necessary to break out Emacs."
  },
  {
    "objectID": "posts/erica-albright/index.html",
    "href": "posts/erica-albright/index.html",
    "title": "Erica Albright was Right",
    "section": "",
    "text": "The Social Network, the classic David Fincher film about the founding of Facebook, opens with the following exchange. It’s a Tuesday night in autumn 2003. Mark Zuckerberg, a computer science sophomore at Harvard, is at a tavern on a date with the fictional Erica Albright:\n\n\n\n\n\n\nZuckerberg: Did you know there are more people with genius IQs living in China than there are people of any kind living in the United States?\nAlbright: That can’t possibly be true.\nZuckerberg: It is.\nAlbright: What would account for that?\n\n\n\nAlbright’s intuition is right, though Zuckerberg derails the conversation before she can challenge him further. Some calculations show that Zuckerberg’s claim is ridiculous.\nEstablishing Numbers\nAccording to the World Bank, in 2003 China had 1.29 billion people and the United States had 290,108,000 (rounding to the nearest hundred, not that it matters).\nWhile there are many ways to define genius, I’ll use intelligence quotient, a well-known measure that is easy to work with. IQ is typically described as normally distributed, with mean 100 and standard deviation about 15. There is no universally accepted cutoff for a “genius” IQ. To be charitable, I’ll define it as 130 or greater, a mere two standard deviations above the mean.\nIf we assume each country has the same theoretical IQ distribution, how many geniuses would we expect in each?\nCalculations\n\nus_pop &lt;- 290108000\nchina_pop &lt;- 1290000000\nmu &lt;- 100\nsigma &lt;- 15\ngenius &lt;- 130\ncutoff &lt;- pnorm(genius, mu, sigma, lower.tail = FALSE)\nus_geniuses &lt;- floor(cutoff * us_pop)\nchina_geniuses &lt;- floor(cutoff * china_pop)\n\nThe China-US ratio is 4.45, so under these assumptions China has that many times more geniuses. But the ratio of U.S. population to Chinese geniuses is 9.89. There would need to be almost 10 times more Chinese geniuses than there are under these assumptions for Zuckerberg to be correct. If we use a higher IQ score as the genius cutoff, the ratio gets even worse.\nThere are infinitely many implausible IQ distributions that would justify Zuckerberg’s claim. For simplicity, let us pose an optimization problem. Given China’s actual population and a U.S. mean IQ of 100, how high must China’s mean IQ be for Zuckerberg’s claim to be correct? That is, what is the lowest mean IQ value for which the right tail area of the normal CDF is greater than the ratio of US to Chinese population?\n\nvalidate_mean &lt;- function(mu) {\n  floor(pnorm(genius, mu, sigma, lower.tail = FALSE) * china_pop) &gt; us_pop\n}\ngrid &lt;- seq(100, 150, 0.01)\nminimum &lt;- min(grid[validate_mean(grid)])\nminimum\n\n-- [1] 118.67\n\n\nZuckerberg’s claim would require the average Chinese to have an IQ more than one standard deviation above the U.S. average. I couldn’t find reliable IQ data for China, and intelligence is notoriously hard to measure. But for the claim to be true, the average Chinese would be more intelligent than about 89.3% of Americans, which is unbelievable.\nSupposing instead China has the same IQ distribution as the United States, how many Chinese would there have to be for Zuckerberg’s claim to be correct? This can be solved algebraically:\n\nfloor(us_pop / cutoff + 1)\n\n-- [1] 12751926040\n\n\nThat is about 10 times the actual 2003 population of China.\nConclusion\nIn fairness to the real Zuckerberg, this conversation is fictional, and I’m skeptical he would really have made the claim this post dissects. As a computer science sophomore at an elite university, he likely understood the properties of normal distributions well enough to know it was implausible. Or maybe the script means to imply that he’s trying to impress a listener he thinks is naive with nonsense, as I’ve always interpreted Han Solo’s nonsensical boast in Star Wars that the Millennium Falcon can make the Kessel Run in less than twelve parsecs. That would also explain why Zuckerberg keeps changing the subject whenever Albright presses him on the point. Either way, Albright is clearly right.\nOne other note: During the hacking montage that follows the tavern scene, Zuckerberg says (in voiceover), “So it’s definitely necessary to break out Emacs and modify that Perl script.” As a Neovim user, I’m required to remind you it is never necessary to break out Emacs. Editing bliss is always just a which vim away."
  },
  {
    "objectID": "posts/naive-code/index.html",
    "href": "posts/naive-code/index.html",
    "title": "The Unexpected Pleasures of Naive Code",
    "section": "",
    "text": "As a moderator of a technical Discord server, I’ve encountered a lot of “naive” code. In computer science, a “naive” solution to a problem is an obvious, straightforward approach someone with no special knowledge might offer as a first attempt. Textbooks often present one before revealing a much more efficient, if less obvious, alternative. Bubble sort is simple enough that a novice programmer might discover it themselves. Merge sort is much harder to understand (at least if you’re not John von Neumann), but it’s much more efficient on large inputs.\nFor our purposes, naive code comes from an enthusiastic but inexperienced user trying to solve a problem at the edge of their abilities. Naive code is fascinating. It is rarely good, since its authors know little of sound practices or language conventions. It contains strange design choices and sometimes outright errors. But these same defects make it oddly charming, as we’ll see.\nAn Analogy\nObviously, the best way to illustrate this is to analyze some ABBA lyrics. Here are the opening lines of “Dancing Queen” (here, if somehow you haven’t heard it before):\nYou can dance\nYou can jive\nHaving the time of your life\nOoh, see that girl\nWatch that scene\nDigging the dancing queen\nIt’s easy to miss on a first listen, but these lyrics don’t sound natural in any widely spoken form of English. “Jive” isn’t typically used as a verb. “Scene” isn’t typically used to mean “situation”; you would say “check it out” or “look over there,” not “watch that scene.” Who is the singer addressing, anyway? It seems like the first three lines refer to the “dancing queen” herself, but the last three address an observer who is “digging” her.\nThe reason for this is simple. ABBA hail from Sweden; they are songwriters whose first language is Swedish trying hard to sound like Americans, but not quite passing. But it’s hard to notice this beneath the song’s amazing arrangement and vocal performances. What’s more, I think the song would be worse if it used more idiomatic English. Laden with dated slang they may be, the lyrics feel oddly timeless, because they don’t sound like any common form of English. Songs written in authentic 1970s American English usually aren’t so lucky. The words somehow convey the song’s message perfectly even though, taken literally, they barely make sense.\n(Related: it bothers me more than it should that another ABBA song incorrectly claims Napoleon surrendered at Waterloo. His army was routed there, but he actually surrendered to a British warship about a month later. Then again, I doubt even ABBA could make “At Waterloo, Napoleon was decisively defeated” scan).\nNaive Code\nNaive code is compelling in quite a similar way. I could give examples from people I’ve helped, but I won’t; they didn’t volunteer to be included in my ramblings. Instead, I’ll use some very naive code I wrote long ago. Here is coivd_19_us.R, a snippet of R that does some simple data processing.\nLike everyone else in April 2020, I made some visualizations of COVID-19 data (though unlike most others, I managed to misspell the name of the disease). (If you don’t know R, the problems with the code below will still be obvious if you have any programming experience).\n\nstate_pop &lt;- read_xlsx(\"states_pop_data.xlsx\", col_names = F, skip = 9)[1:51, ] %&gt;%\n  select(...1, ...13) %&gt;%\n  mutate(...1 = str_remove(...1, \".\"), ...13 = ...13 / 1000) %&gt;%\n  rename(\"state\" = ...1, \"state_pop\" = ...13)\n\ncounty_pop &lt;- read_xlsx(\"county_pop_data.xlsx\", col_names = F, skip = 5)[1:3142, ] %&gt;%\n  select(...1, ...12) %&gt;%\n  separate(...1, into = c(\"county\", \"state\"), sep = \",\\\\s\") %&gt;%\n  rename(county_pop = ...12) %&gt;%\n  mutate(county = str_replace(str_replace(county, \"\\\\s[A-Z][a-z]*\\\\s*[A-Z]*[a-z]*$\", \"\"), \"^\\\\.\", \"\"), county_pop = county_pop / 1000)\n\n\ncombined_pop &lt;- left_join(county_pop, state_pop, by = \"state\")\ncombined_pop &lt;- nest_join(combined_pop, us_county, by = c(\"county\", \"state\"))\n\nRegrettably, I have a vague memory of what this code was supposed to do. It reads .csvs containing some state and county population data and does some basic cleaning. The structure of the code is fine, but the implementation is a mess. Magic numbers crop up everywhere (note the obscure ...n syntax for selecting the nth column). The column transformations are convoluted, especially the double regex replacement applied to county_pop. The last line uses a nested join, a special dplyr function that groups the matched rows in the right-hand data frame in a list column. I remember thinking that was an appropriate way to handle many-to-many relationships.\nIt’s easy to tell the author of this snippet was blissfully unaware anyone would have to read or maintain his code. (Don’t snicker - everyone has had Past You pull that that on Present You at some point). Not only did he not know his language’s conventions, he did not know of them.\nThe code is bad, no doubt. But its innocent sincerity is charming. It reminds me of the days when there seemed nothing wrong with dashing off a chunk of with no clue how it fit into the project it was part of. Naive code can contain other unexpected delights, too. I’ve seen attempts to add factors (an R class for categoricals that cannot be added), heroic attempts to do with for loops what could easily be done with vectorized functions, and every nonstandard variable naming scheme you could think of. Novice programmers so often write logic experienced ones would never even think to try, and I can only respect them for it.\nConclusion\nThis analogy is strained, I admit. ABBA’s unique brand of lyrics may have got them to #1, but naive code as I describe it above more often leads to frustration and searching for an experienced person to debug it. Helping can be a lot of work, but if you find yourself asked to help, do so. It might have weeks ago, it might have been decades, but you once wrote naive code, too. We all did."
  },
  {
    "objectID": "posts/why-quarto/index.html",
    "href": "posts/why-quarto/index.html",
    "title": "Why Quarto?",
    "section": "",
    "text": "When I started planning this blog, I initially intended to use R Markdown with Blogdown. But right before I committed, I learned about Quarto, a new Markdown variant with several new features. I decided to go with Quarto, and it proved the right decision. This post is about how Quarto extends R Markdown’s already formidable capabilities to produce all kinds of documents.\nR Markdown\nR Markdown is a file type based on Markdown. It is designed for writing documents that combine “chunks” of R code with plain text. An R Markdown document can be “rendered” into several different types of output, most commonly .html or .pdf. Rendering executes R code contained in chunks and displays the outputs in the resulting document alongside text.\nR Markdown is well suited for typical data science work products. When writing a report or article, it’s easy to combine code with prose that explains what it does. You write the document like this:\nThis important plot shows that two principal components capture almost all of the variance within mtcars ```{r} plot(princomp(mtcars)) ```\n\n\n\n\n\n\n\n\nand the output is shown when it’s rendered.\nThe format also renders LaTeX, which means you can use all kinds of math symbols, like this set identity.\n\\[\n|A \\cup B| = |A| + |B| - A \\cap B|\n\\]\nI used R Markdown all the time in grad school. I learned the format before my first semester began, and I used it from the start to make reports to my assistantship supervisor. I became more familiar with it after changing to a data science degree. Since the data science department at my school mostly used R, it was the required format for most assignments. Its LaTeX capabilities also came in handy for linear algebra homework.\nBut a few pain points emerged over time. There was no built-in shell command to render a document from the terminal; you had to run R -e 'rmarkdown::render(\"thefile.Rmd\")' or similar. Many people used R Markdown to write blogs, but no built-in support existed, so most relied on external packages like Blogdown. Most obviously, it was fundamentally _R_ markdown. While the format can render an impressive number of non-R languages, like SQL or Bash, it was designed and intended for R, not other ecosystems. If you want to render an R Markdown file as a Jupyter notebook, you’ll need to use a third-party tool.\nDrawn and Quartoed\nQuarto does all these things and more. It features a command-line interface that can render files, preview outputs, create projects, and more. It supports Jupyter notebook outputs and offers a cleaner way to set chunk options than R Markdown. Most importantly for my use case, Quarto was designed to support blog editing. The docs even have a step-by-step guide.\nAll that aside, Quarto also supports the usual R Markdown features. You can run code from various languages, customize chunks to your heart’s content, and use LaTeX freely.\nThese facts make Quarto ideal for a blog like this. The workflow is simple. When I want to write a new post, I change directory to the project root and create posts/important-content/index.qmd, where important-content is the post title. (But I repeat myself, for all my content is important). Then I write index.qmd, the file that will be turned into an HTML document containing the post text. Once done, I use quarto render posts/important-content to create the HTML output, then quarto preview to find out how it would look once uploaded. I make any needed tweaks, then repeat the process as needed. Once satisfied, I git commit and push to the repository where the blog lives.\nI’m sure I’ll discover some pain points with Quarto eventually, but so far it’s been a smooth experience. And when I do, I’m pretty confident they’ll be resolved with the successor file format."
  },
  {
    "objectID": "posts/r-names/index.html",
    "href": "posts/r-names/index.html",
    "title": "Renaming Things in R",
    "section": "",
    "text": "Naming things is one of the two hard problems of computer science. It seems like it should be easy. Then you start coding, try to come up with names that are consistent, succinct, precise, and evocative, and fail.\nR adds another layer of difficulty: internal names. Vectors (which most R objects are) can store names for each element, including recursively. This makes it easier to understand objects’ structure and allows you to subset by name rather than position:\n\nx &lt;- list(a = 4, b = \"q\", c = c(a = 1, b = 2), 1:5)\nx$a\n\n-- [1] 4\n\nx$c[[\"a\"]]\n\n-- [1] 1\n\n\n(You can’t use $ to subset atomic vectors).\nR also has a system for altering the names of existing objects - or so it seems. That system is the subject of this post.\nYou can do this with the syntax names(x) &lt;- new_names. If the new vector of names is shorter than the old one, R will just fill the new names with NA. You can also use subassignment to change only some names, subject to the same rules. Thus:\n\nnames(x)[c(1, 3)] &lt;- c(\"w\", \"z\")\nx\n\n-- $w\n-- [1] 4\n-- \n-- $b\n-- [1] \"q\"\n-- \n-- $z\n-- a b \n-- 1 2 \n-- \n-- [[4]]\n-- [1] 1 2 3 4 5\n\n# Legal but silly\nnames(x) &lt;- \"a\"\nx\n\n-- $a\n-- [1] 4\n-- \n-- $&lt;NA&gt;\n-- [1] \"q\"\n-- \n-- $&lt;NA&gt;\n-- a b \n-- 1 2 \n-- \n-- $&lt;NA&gt;\n-- [1] 1 2 3 4 5\n\n\nThis is awkward but workable. It also creates the misleading impression that you are changing an object’s names in place. R experts will know that R generally copies objects on modification, except environments. The code above isn’t really setting the names to a new value. Rather, you’re calling the names&lt;- function to return a new object with changed names. (You can also call it directly in that form, if you hate whoever has to maintain your code).\nAs shown in Hadley Wickham’s Advanced R, what really happens is this:\n\n`*tmp*` &lt;- x\nx &lt;- `names&lt;-`(`*tmp*`, `[&lt;-`(names(`*tmp*`), c(1, 3), c(\"w\", \"z\")))\nrm(`*tmp*`)\n\nThis spooky process happens behind the scenes, and the deletion of *tmp* (yes, that’s a legal variable name) destroys the evidence. The R user is none the wiser - at least until they see something like the second line in the above chunk in an error traceback and investigate.\nYou can even create your own functions that work this way. Just add &lt;- to the end of the function name, call the first argument “x” and the last “value”, and you have a working “replacement function.” Presumably, the parser gives them special treatment, in one of R’s several “magic” syntax rules. Here is a JavaScript-esque replacement function that fails silently if the index passed is invalid:\n\n`nth&lt;-` &lt;- function(x, n, value) {\n  n &lt;- as.integer(n)\n  if (n &gt; 0 && n &lt;= length(x)) {\n    x[[n]] &lt;- value\n  }\n  x\n}\nnth(x, 2) &lt;- 3\nx\n\n-- $a\n-- [1] 4\n-- \n-- $&lt;NA&gt;\n-- [1] 3\n-- \n-- $&lt;NA&gt;\n-- a b \n-- 1 2 \n-- \n-- $&lt;NA&gt;\n-- [1] 1 2 3 4 5\n\n\nIf you really want to see how the sausage is made, you can read the relevant R source code here.\nIs this the most elegant way to handle the delicate problems of naming and renaming? Maybe not. But it works, and sometimes that’s enough."
  },
  {
    "objectID": "posts/regression-algebra/index.html",
    "href": "posts/regression-algebra/index.html",
    "title": "Why Linear Algebra is Useful",
    "section": "",
    "text": "Linear algebra has a bad reputation. The subject has a tough combination of fiddly computation and abstract reasoning, one that isn’t always taught well. STEM majors often have bitter memories of spending hours on row reduction and QR decomposition. But sometimes linear algebra can make your life much, much easier. One such situation is expressing the formula for computing linear regression coefficients.\nWhat does Linear Regression Actually Do?\nFormally, ordinary least squares regression takes a data matrix \\(X\\) and a response vector \\(Y\\), and finds a vector of coefficients \\(\\beta\\) that minimize the equation \\(\\sum_{i=1}^n (Y_i - \\hat Y)^2\\), where \\(\\hat Y = X \\beta\\). This is called the sum of squared errors (\\(SSE\\)). It can also be viewed as the sum of the squares of the Euclidean distance between each response value \\(Y_i\\) and the corresponding fitted value \\(\\hat Y_i\\). (Why squared distance instead of absolute value? See below). The individual errors are called residuals.\nSimple linear regression is the special case where there is only one variable, \\(X_1\\), and hence two coefficients: the constant term \\(\\beta_0\\) and \\(\\beta_1\\), the coefficient for \\(X_1\\). (\\(\\beta_0\\) isn’t strictly necessary, but it’s rarely a good idea to omit it). In simple regression, \\(\\hat Y_i = \\beta_0 + \\beta_1 X_{i1}\\)\nThis is equivalent to plotting the data and finding the line that minimizes the sum of squared errors. \\(\\beta_0\\) provides the intercept of the line.\nHere is a model with the residuals shown:\n\nlibrary(ggplot2)\n\nX &lt;- rnorm(100, 10, 5)\nY &lt;- X * 7 + 4 + rnorm(100, 20, 4)\nmodel &lt;- lm(Y ~ X)\nggplot(mapping = aes(x = X, y = Y)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\", linewidth = 0.7, se = FALSE) +\n  geom_point(alpha = .3) +\n  geom_segment(aes(x = X, y = model$fitted.values, xend = X, yend = Y), color = \"grey\", linewidth = 0.5) +\n  theme_minimal()\n\n-- `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow we have to know how to find these coefficients \\(\\beta\\). There the trouble starts.\nElementary Algebra\nStatistics textbooks usually introduce the equations for the estimators in simple linear regression using these formulae (or algebraically equivalent expressions): \\[\n\\displaylines{\\beta_0 = \\bar Y -\\beta_1 \\bar X \\\\\n\\beta_1 = \\frac{\\sum_{i=1}^n(X_i - \\bar X)(Y_i - \\bar Y)}{\\sum_{i = 1}^n (X_i - \\bar X)^2}}\n\\]\nProfessors often throw these complicated formulae at unsuspecting students in the last weeks of intro stats courses. I first encountered them that way, and I remember the shock of fear I felt. Nobody with mathematical maturity is scared of a summation symbol, but back then I had no clue what I was doing, and many other students are in the same position.\nThere’s also the classic assignment of deriving these formulae by minimizing the normal equation \\(SSE = \\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1 x_i)^2\\). You have to take first derivatives, solve the equations for each estimator simultaneously, do some algebra tricks, and finish off with a second-derivative test to make sure the estimators really do minimize the loss function. It’s one of those tough but brutally satisfying assignments, like computing a median with standard SQL or implementing iterative least squares to estimate a generalized linear model. (I appreciate the professor who suggested doing it as an exercise; it made me a lot less mathematically naive!).\nLinear Algebra\nIn linear algebra terms, linear regression orthogonally projects the response vector \\(Y\\) into the linear subspace formed by the data matrix \\(X\\). (Linear algebra texts usually call a generic matrix \\(A\\), but statistics texts use \\(X\\) for a data matrix, so I will do so here). In other words, it finds the closest vector to \\(Y\\) by Euclidean distance that can be formed by a linear combination of \\(X\\). In matrix algebra, the normal equation is \\(SSE = (Y-\\beta X)^T(Y- \\beta_ X)\\). (Multiplying a row vector by its transpose just sums the squares of each element). Some basic calculus and rearrangement turns this into \\(X^TX \\beta = X^Ty\\). By inverting \\(X^TX\\) (a matrix operation roughly analogous to turning a number \\(n\\) into \\(1/n\\)), we get a simple formula for \\(\\beta\\):\n\\[\n\\beta = (X^TX)^{-1}X^Ty\n\\]\nThis is a variation on the formula for a projection matrix, \\(P = X(X^TX)^{-1}X^T\\). For any vector \\(y\\), \\(Py\\) is the closest vector to \\(y\\) by Euclidean distance that is a linear combination of the vectors in \\(X\\). This is much more revealing than the elementary-algebra version of the formula. For one thing, it’s a lot simpler. It’s fully general, too: it holds for all coefficients for a data matrix of any size, not just simple linear regression. (What about \\(\\beta_0\\), the constant term? Just add a column of ones to your data matrix, and you have it!). And it implies the important fact that no unique least-squares solution exists if \\((X^TX)\\) fails to be invertible, which happens if the rank of \\(X\\) is less than \\(p\\), the number of variables. In that case, there are infinitely many least-squares solutions.\nConclusion\nAs this example shows, matrix notation is more abstract and concise than scalar notation. It makes the underlying concepts of an expression like this far easier to see and understand. It’s the difference between implementing a function in Python and implementing it in C, except without the loss in efficiency.\nThis is just one example of how linear algebra makes it easy to express ideas that would be hard to convey with elementary algebra. Those weeks of row-reducing matrices really were worth it."
  },
  {
    "objectID": "posts/aoc-overview/index.html",
    "href": "posts/aoc-overview/index.html",
    "title": "Advent of Code So Far",
    "section": "",
    "text": "Introduction\nI really like Advent of Code, as you probably can guess. I’ve solved every one of the 200 puzzles available at the time I write this (in late summer 2023). With the ninth edition coming in a few months, it’s a good time to review the eight existing years of Advent of Code and offer my impressions.\n\n\n2015\nThe first installment of a long-running series is usually the strangest. Rules of the setting are looser or unestablished. The writing varies from the formula later installments will establish. So it is with Advent of Code’s first year. According to Eric Wastl, the sole author of Advent of Code, the original idea was just to entertain his friends with Chirstmas-themed puzzles. The first year accordingly has more informal feel than what would come later. The difficulty spikes and troughs as with the surprisingly easy Aunt Sue puzzle. Some puzzles lack the creative twists that became the series’ hallmark. Day 9, for instance, is just a traveling salesman problem small enough to brute-force. Overall, the year is probably the easiest of all; perhaps Wastl underestimated just how determined people would be to get those gold stars.\nThe narrative style is different, too. Later years would take the player on a convoluted adventure to save Christmas, but 2015 instead offers a series of disconnected Christmas scenarios. This might seem like mere flavor, but for me it gives Advent of Code much of its charm. Instead of a straightforward problem specification, you’re told to compute how long a lost Elf would take to wander through a snowstorm, or [emulate Tetris for some angry elephants](https://adventofcode.com/2022/day/17. Uncovering the programming problem wrapped in the silly plotline is half the fun, so I missed the absence of one to tie this year together.\nBut the charms of the series shine through nonetheless. Most of the classic puzzle topics appear: Conway’s Game of Life variations, made-up assembly languages, string manipulation, number theory, combinatorics, and deductive logic. The wry tone of the series establishes itself, as do the clever part 2 twists. (My favorite is day 13. In part 1, you have to seat people with strong opinions of each other at a dinner table in a way that maximizes their total happiness. Part 2 reveals that you forgot to reserve a place for yourself, but it’s not too hard, since you’re indifferent to who you sit next to). And the website had its classic blue-and-gold look from the start.\nIt’s easy to see in hindsight how Advent of Code became far more popular than anyone expected. A new edition for 2016 had to happen. But how would it improve on what came before?\n\n\n2016\nWith a fancier narrative and tougher, more complex puzzles, naturally. As day 1 opens, you are airdropped a short distance from the Easter Bunny’s headquarters. He has stolen all 50 of Santa’s stars, and you need to retrieve them in time to save Christmas. This hilarious scenario (and my favorite from the seven years with proper plotlines) introduces the standard Advent of Code plot: some calamity has befallen Santa and scattered his stars, and you have to recover them before Christmas is ruined.\nThe puzzles start off harder than before. 2016’s day 1, which involves tracking the player’s position over a two-dimensional grid, is in my opinion the hardest day 1 yet. The first week or so of puzzles are tough but reasonable, but then comes day 11. This one requires you to find the most efficient way of moving a set of microchips and radioisotope thermoelectric generators through several floors of a building. The obvious method is to find a way to represent states and do breadth-first search, but this gets complicated and finicky. Part 2 is nigh impossible without doing a subtle optimization. Nothing about the puzzle is unfair, but for a second-week puzzle, its difficulty is extreme.\nThat aside, the puzzles provided a welcome challenge after the relatively straightforward ones of 2015. The only days I disliked required the player to compute MD5 hashes. They resulted in unavoidably slow solutions that just required the player to call their language’s standard library function for MD5.\n\n\n2017\nBy 2017, Advent of Code had found its classic formula. MD5 hashes were gone, as were no-frills puzzle designs and sudden difficulty spikes. Everything this year was executed smoothly.\n2017 turned down the difficulty from 2016. Many puzzles were simpler than the last year, and nothing anywhere near as hard as 2016’s day 11 appeared. Perhaps Wastl wanted to broaden the series’ appeal to inexperienced programmers. At any rate, the easier challenges provided a welcome change of pace.\nStill, plenty of the puzzles offered a good challenge, such as day 23. This one asked you to implement a hideously inefficient program in a made-up assembly language. To solve part 2 in a reasonable amount of time, you had to reverse-engineer it. It was tough but fun to unravel the assembly code and discover a simple program for counting composite numbers within a numeric range.\nOverall, I found 2017 fun and engaging. I should also note the ASCII art for the year has stunning animation, in my view the best in the series.\n\n\n2018\nAfter something of a breather in 2017, 2018 brought perhaps the toughest set of puzzles yet. This year’s plot involved traveling through time to recover stars. Naturally, that adventure ran through some fiendishly complex puzzles.\nSeveral of the later puzzles pushed my skills to the limit. Day 17 is a notoriously hard physics simulation. Day 23 is meant to be solved by octary search, basically binary search in three dimensions. It was as hard (but fun) as it sounds. Day 15 requires the player to implement a combat video game; it’s probably the most complicated puzzle ever. The high difficulty was exhausting at times, but always rewarding.\nOverall, the year demonstrated that Advent of Code would emphasize unconventional puzzle designs that rewarded a creative approach. The only hiccup was day 6. For the only time in series history, a bug with some people’s inputs made the puzzle unsolvable for them before it was fixed. All scores for this puzzle were canceled as a result. It’s mostly notable for being the one time there actually was a bug with the website rather than users’ code. Aside from that incident, this year featured some of the best puzzles yet.\n\n\n2019\n2019 was the most ambitious year yet, and also the most controversial. For the first and only time, Wastl changed the formula. Almost half the puzzles in 2019 would require players to work with Intcode: an invented programming language that uses integer opcodes to manipulate data and memory. To solve them, players would have to write an Intcode interpreter or compiler and use it to execute the Intcode program provided as each puzzle’s input.\nThe Intcode puzzles started simple, but got more and more complex. By late December, players found themselves using Intcode to implement simplified versions of Frogger and Breakout.Then came day 25. Most years have an easy problem on the last day, one meant to give you a relaxing diversion on Christmas morning. This year, it was an Intcode program that implemented an interactive adventure game. It has the genre’s classic elements: unfair deaths, a confusing layout, and an item-swapping puzzle at the end. You could either solve it interactively or write a script to do so automatically - a much richer challenge than a typical day 25 puzzle.\nIntcode divided the community. If you search the official subreddit, you can find several threads debating its merits. Those in favor argued that the Intcode puzzles rewarded players for writing reusable code and offered a progressive challenge the classic puzzle format did not. Those against responded that Intcode was convoluted and hard to understand, locking out players who couldn’t keep up with the puzzles.\nI think both claims are right. As original and interesting as Intcode was, it was also finicky and unforgiving. I remember wasting a few hours because I misunderstood a minor point in the specification. Overall, I think it succeeded in giving 2019 some unique puzzles, but I don’t blame people for disliking the concept. It probably would have been a lot easier if I’d taken a compilers class, as many participants probably have. All the same, I think it’s good to shake up a well-established formula. I’d like to see an experiment like Intcode again.\n\n\n2020\n2020 is special. It was the first year I took part in while it was ongoing. I stumbled across a link to a puzzle near the end of my fall semester of graduate school, and I decided to try solving it. It was day 12, a fairly easy puzzle that asks you to simulate moving a navigation beacon according to instructions. I had fun solving it, though I wasted a lot of time trying to implement a general rotation matrix to model switching orientation. (I had yet to learn that Advent of Code only requires integral computations, so I only had to deal with multiples of 90 degrees).\nI spent the rest of that winter break solving as many puzzles as I could, learning Python as I went. I had found Advent of Code at exactly the right time in my life. I had just switched from pursuing a master’s in public policy to a master’s in data science, and I knew I would have to learn to write effective code fast. Doing the puzzles increased my confidence that I could do that and confirmed to me that I would enjoy it.\nSentimentality aside, this was a good year, one that returned to the classic format after the Intcode experiment of 2019. There were yet more Game of Life variations, this time in four dimensions or with hexagonal coordinates. It was a fitting tribute to John Conway, who died that year. While the overall difficulty was reduced from the past two years, one puzzle broke the trend. (If you took part in this year, you already know which it is). Day 20, “Jurassic Jigsaw”, requires you to assemble dozens of tiles into an image by matching the patterns on their edges. Part 2 asks you to scan the image for the Loch Ness Monster, which can be recognized by a telltale pattern. It ranks among the very hardest puzzles, but I had fun designing a very complex depth-first search to solve it.\nThe year’s plot, which involves the player frantically trying to salvage a vacation plan gone awry, is the most realistic so far. Aside from the part with the Loch Ness Monster, anyway.\n\n\n2021\nI began 2021 with far more experience than I had in 2020. And I needed that experience, because this year cranked the difficulty back up to near-2018 levels. I kept up for the first week or so, but I began to fall behind when the puzzles began requiring graph traversal. I knew nothing about graphs or pathfinding algorithms back then, and had to learn fast. I fumbled my way through Dijkstra’s algorithm well enough to solve some of the early puzzles, but I soon fell several days behind.\nI persevered, making steady progress as 2022 began. I had to put Advent of Code aside when grad school resumed, but by late spring I had earned all 50 stars. It was the first year I finished completely, and by far the most satisfying to complete.\nIt took weeks to crawl through days 21 through 24, one of the toughest stretches of puzzles in the whole series. Most of these puzzles demand creativity to pare down huge problem spaces. Day 24, for instance, asks you to find the highest number that meets a condition from trillions of possibilities. I solved it only after painfully reverse-engineering the made-up assembly langueage provided as the input. Day 23, to me, is the hardest puzzle in the entire series. It asks you to solve a variation on the Towers of Hanoi puzzle. The secret is that it’s pretty easy to solve by hand. (I used cut-up Post-it notes as game pieces). But I couldn’t resist the challenge of writing a program to solve it, one that would work on a general input. It took a week of focused effort to do so. It was painful at time, but I grew so much as a programmer from these puzzles that it was all worth it.\n\n\n2022\nThe most recent year, as of this writing, has the player accompany a jungle expedition searching for star fruit, which the reindeer apparently eat. The more relaxed narrative suits the puzzles, which are a bit easier than the previous year’s. The puzzle design seemed more thoughtful than ever; most of this year’s puzzles had simple rules that lacked accidental complexity. Instead, the difficulty emerged organically from the constraints of the problems.\nDay 16, perhaps the hardest puzzle, is a good example. It asks you to open a series of pressure valves in rooms connected by tunnels in a way that releases as much pressure as possible. Representing the situation as a simple graph isn’t enough; the number of possible states quickly becomes unmanageable. After asking for advice, I solved the problem by creating a simpler graph that represented distances between pairs of valves, then writing a traverse algorithm that discarded paths that had no chance of releasing the most pressure. This complex design process emerged from a simple problem statement, and I found it more satisfying than trying to meet a complicated specification.\n\n\nConclusion\nAdvent of Code continues to grow more popular. In all likelihood, it will do so for many years to come. Looking back over the eight years and 200 puzzles so far, it’s easy to see why. The puzzles offer endless variations on classic computer science problems, often with an inventive “part 2 twist.” The story tying each year together, while inessential, keeps the mood light. The design of the website makes it easy to check answers, and the carefully worded puzzle descriptions provide enough guidance to find a solution. It success reminds me how many computer science discoveries came from just playing around with interesting problems. I’ll probably keep participating as long as I can, because it never stops being fun."
  },
  {
    "objectID": "posts/aoc-chatGPT/index.html",
    "href": "posts/aoc-chatGPT/index.html",
    "title": "ChatGPT and the Advent of Code Leaderboard",
    "section": "",
    "text": "When is it cheating to use ChatGPT?\nMany professions have confronted this question, such as teachers and lawyers. So too have some hobbies. That brings me to Advent of Code.\nI write this as someone with no hope of ever placing on the leaderboard. I can’t code fast enough, and what I love Advent of Code fort the art of designing solutions, not writing them as fast as possible.\nBut I have earned all 400 stars to date, so I believe my opinion holds some weight.\n\nThe Situation\nChatGPT became publicly available on November 30, 2022 - the day before that year’s Advent of Code began. Even so, people immediately saw its potential to solve puzzles. Some began using it to write solutions. If fed a puzzle description, it could churn out a working solution in seconds, fast enough to beat even the quickest humans on the leaderboard. For the first few days, ChatGPT solutions bested their human competitors.\nAs December wore on, I saw the frequency of ChatGPT posts diminish. The model struggled with the later puzzles, which often feature complicated rules and inputs large enough to frustrate naive solutions. While ChatGPT’s success on the early days still impresses me, it clearly could not take on subtler problems. The emerging consensus on ChatGPT’s usefulness as a coding assistant - passable for churning out boilerplate to access an unfamiliar API, useless for serious problems - seemed to confirm my impressions.\n\n\nIs It Cheating?\nThe subreddit debated these events, and a majority seemed to think using ChatGPT was unfair. I think they’re right.\nIn one sense, this debate is beside the point. The leaderboard isn’t the main feature of Advent of Code. It’s a special arena reserved for the small fraction of players with the speed and competitive drive to place on it. Then again, for many players it provides much of Advent of Code’s appeal. They stay up until midnight (or, if in Europe, crawl out of bed at five or six in the morning) and hack together a solution as fast as they can. It’s a low-stakes but exhilarating competition, one the best players describe as deeply rewarding.\nBut fun is the point of Advent of Code. It’s meant to deliver the exciting parts of programming - variations on algorithms, optimization problems, input parsing - in an enjoyable package. Churning out an alogrithm as fast as possible is rarely useful in the real world, but it makes for a fun competition. Beating the leaderboard with ChatGPT is no fun.\nHow could ChatGPT be banned, though? By design, the site has no way of verifying how users get their answers. Previously, cheating (if we consider using code you didn’t write cheating) required a collaborator - a fast friend who got on the leaderboard legitimately and gave you their code to run on your input. Now, there is nothing stopping anyone from using ChatGPT.\nOne option would be setting a secret cutoff time for each puzzle that represented the theoretical minimum time it would take for a human to solve. Any correct answer submitted before this time elapsed would be rejected as probably illegitimate. I doubt this would be workable in practice. If the allotted time was too great, some fast players might have legitimate scores rejected. If too small, then a cheater could easily defeat it by solving the puzzle instantly and waiting to submit. The winners would be those who used ChatGPT and guessed the cutoff score most accurately. This approach would just make things worse.\nSo the days of a human leaderboard may be ending. But I don’t think it will hurt Advent of Code. The puzzles have always offered endless opportunities to get creative. Every year sees solutions in esoteric languages, ones that observe strict or bizarre constraints, compelling visualizations, and in-depth discussion of the puzzles and the concepts needed to solve them. It is these pursuits, more so than the leaderboard, that make Advent of Code special. ChatGPT doesn’t threaten any of them.\nSo first place will no longer belong to the quickest and cleverest, but to whoever uploads their ChatGPT output the quickest. Humans may hold out on the harder puzzles, but more advanced versions of GPT may be able to tackle them, or at least give human solvers an easier time. There is something sad about watching technology overtake human skill like this. I’m reminded of the military elites that became superfluous when the societies that once revered them obtained mass-produced guns.\nChatGPT certainly will not kill Advent of Code, any more than the invention of rifles killed fencing. But the leaderboard will never be what it was, and something will be lost with it."
  },
  {
    "objectID": "posts/aoc-2023/index.html",
    "href": "posts/aoc-2023/index.html",
    "title": "Advent Time Again",
    "section": "",
    "text": "It’s Advent of Code time again!\nOnce again, thousands of programmers around the world are rearranging their lives to solve a series of Christmas-themed puzzles. As I write this, only one puzzle has been released, but this already promises to be an interesting year. This Day 1 puzzle was unexpectedly hard.\nPart 1 is a simple string manipulation problem. The input consists of a file where each line has lowercase letters and single digits. You just have to read the first and last integers on each line of a file, combine them, read the resulting integers, and sum them.\n```{python}\nimport re\n\nfrom utils.utils import split_lines\n\nraw = split_lines(\"inputs/day1.txt\")\n# No zeroes\nnaturals = r\"[1-9]\"\nstripped = [re.sub(\"[a-z]+\", \"\", line) for line in raw]\npart1 = sum(int(line[0] + line[-1]) for line in stripped)\nprint(part1)\n```\nPart 2 reveals a nasty surprise. You no longer have to just search for digits, but written numbers. one and 1 now both mean 1. I created an awkward regular expression to handle this, then confidently submitted my answer. Wrong. I checked my code on the provided test case, and it was correct.\nThis happens to me several times each year, but rarely on day 1! I racked my mind for a pathological edge case that would break my code. Immediately, I thought of overlapping matches. By the rules, a string like eightwo should be parsed as 82, but a conventional pattern would match eight, move on to w, and fail to match any more substrings from the string. I needed an overlapping pattern to get the correct answer.\nAs with most problems, the answer to this one turned out to lie in a purple StackOverflow link. I tweaked my pattern, reran my code, and submitted a new, slightly higher answer. Correct!\n```{python}\nnums = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\nn = len(nums)\nparts = \"|\".join(nums + [naturals])\npattern = f\"(?=({parts}))\"\nchars = list(map(str, range(1, 1 + n)))\nkeys = dict(zip(nums, chars))\n\nmatches = [re.findall(pattern, line) for line in raw]\npart2 = sum(\n    int(keys.get(r[0], r[0]) + keys.get(r[-1], r[-1])) if r else 0 for r in matches\n)\nprint(part2)\n```\nAll in all, it was a typical Day 7 or 8 puzzle, which made it a nasty surprise as a Day 1 puzzle. So Day 2 should be easy by comparison - right?"
  },
  {
    "objectID": "posts/best_viz/index.html",
    "href": "posts/best_viz/index.html",
    "title": "The Best Data Visualization of All Time",
    "section": "",
    "text": "Most data visualizations are simple and utilitarian. This one is complicated and utilitarian:\n\nThe chart was the work of Charles Joseph Minard, a French civil engineer who spent his retirement years making innovative visualizations. The chart in question, “Carte figurative des pertes successives en hommes de l’Armée Française dans la campagne de Russie 1812–1813,” shows what its title describes: “successive manpower losses in the French Army’s campaign in Russia, 1812-1813.”\nA broad strip, representing Napoleon’s army, stretches across the map before doubling back. You realize it represents the progress of his army. Then you understand why it gets thinner as it meanders across the map: French soldiers dying, day after day. You realize that the plot on the bottom shows the recorded low temperature on selected days.\nThe chart does what all great visualizations do: highlight an important pattern in a morass of data. It shows plainly why Napoleon lost the campaign. From the start, disease, starvation, and Russian resistance ate away at his army. The Russian army refused to oblige his plan to destroy it. By the time his diminshed forces marched unopposed into Moscow, their defeat was sealed. Most of those who remained succumbed to cold or Cossacks on the long retreat.\nThe ever-narrowing strip tells this brutal story more vividly than pages of tables or narrative prose could. (It also neatly dispels the misconception that the winter cost Napoleon the campaign. It just turned a fiasco into a catastrophe). Few modern analysts (me included) could come up with such an original and striking way of displaying the data. Minard’s work still stands as the best data visualization of all time."
  },
  {
    "objectID": "posts/aoc-2023-2/index.html",
    "href": "posts/aoc-2023-2/index.html",
    "title": "Advent of Code 2023 Day 2",
    "section": "",
    "text": "Day 2, fortunately, proved easier than day 1. The puzzle gives results of draws of red, green, and blue items from bags. Part 1 asks to find which draws were possible given certain numbers of each item. Part 2 asks to find the minimum number of items to make each draw valid.\nThe computations proved trivial. Most of the challenge came in parsing the input, which looked like this:\nGame 1: 3 blue, 4 red; 1 red, 2 green, 6 blue; 2 green\nGame 2: 1 blue, 2 green; 3 green, 4 blue, 1 red; 1 green, 1 blue\nGame 3: 8 green, 6 blue, 20 red; 5 blue, 4 red, 13 green; 5 green, 1 red\nGame 4: 1 green, 3 red, 6 blue; 3 green, 6 red; 3 green, 15 blue, 14 red\nGame 5: 6 red, 1 blue, 3 green; 2 blue, 1 red, 2 green\nI used R for vectorized arithmetic. I ended up using several rounds of string splitting to extract the data:\nparse_pairs &lt;- function(pairs) {\n    splits &lt;- strsplit(pairs, \" \") |&gt; unlist()\n    n &lt;- length(splits)\n    even &lt;- seq(2, n, 2)\n    # Since values come in value-color pairs\n    result &lt;- colors\n    present &lt;- splits[even]\n    result[present] &lt;- strtoi(splits[even - 1])\n    result\n}\n\nparse_line &lt;- function(line) {\n    line &lt;- sub(\"^Game \\\\d+:\\\\s+\", \"\", line)\n    # browser()\n    draws &lt;- strsplit(line, \";\\\\s+\") |&gt; unlist()\n    pairs &lt;- strsplit(draws, \",\\\\s+\")\n    lapply(pairs, parse_pairs) |&gt;\n        do.call(what = cbind)\n}"
  },
  {
    "objectID": "posts/aoc-2023-4/index.html",
    "href": "posts/aoc-2023-4/index.html",
    "title": "Advent of Code 2023 Day 4",
    "section": "",
    "text": "Day 4, thankfully, simplifies the input parsing. Each line of the input contains winning numbers and drawn numbers for a lottery game. Part 1 simply asks you to compute a score from the quantity\nof winning numbers on each card:\nCard 1: 41 48 83 86 17 | 83 86  6 31 17  9 48 53\nCard 2: 13 32 20 16 61 | 61 30 68 82 17 32 24 19\nCard 3:  1 21 53 59 44 | 69 82 63 72 16 21 14  1\nCard 4: 41 92 73 84 69 | 59 84 76 51 58  5 54 83\nCard 5: 87 83 26 28 32 | 88 30 70 12 93 22 82 36\nCard 6: 31 18 13 56 72 | 74 77 10 23 35 67 36 11\nPart 2 switches up the rules in an interesting way. Now, if card \\(n\\) has \\(k\\) winning numbers, you get copies of cards \\(n+1\\) through \\(n+k\\) inclusive. After simulating this process, you count the total number of cards. The difficulty comes from the fact that multiple copies of card \\(n\\) may exist. Since each card has an identical result, I can just multiply by the number of cards. The resulting loop runs quickly.\ndraw &lt;- function(cards, won) {\n    n &lt;- length(cards)\n    held &lt;- rep(1, length.out = n)\n\n    for (current in seq_len(n - 1)) {\n        draws &lt;- held[[current]]\n        result &lt;- won[[current]]\n        if (result &gt; 0) {\n            received &lt;- seq(current + 1, current + result, 1)\n            # Clamp to number of cards\n            received &lt;- received[received &lt;= n]\n            held[received] &lt;- held[received] + draws\n        }\n    }\n    sum(held)\n}"
  },
  {
    "objectID": "posts/aoc-2023-3/index.html",
    "href": "posts/aoc-2023-3/index.html",
    "title": "Advent of Code 2023 Day 3",
    "section": "",
    "text": "Day 3 seemed to confirm this would be a tough year. The puzzle gives a grid with numbers and punctuation characters like this:\n467..114..\n...*......\n..35..633.\n......#...\n617*......\n.....+.58.\n..592.....\n......755.\n...$.*....\n.664.598..\nPart 1 asks you to sum the numbers adjacent to one or more non-\".\" characters. Parsing was nontrivial. I eventually decided to create two dicts, one mapping coordinates to symbols and one mapping each coordinate with a digit to the number’s value. I used complex numbers as Cartesian coordinates. So 1+0j maps to 467.\nPart 2 adds a wrinkle: you now must find each space with \"*\". For each that has exactly two adjacent numbers, multiply those numbers and sum up the products.\nI wrote a complicated function to do both at once:\ndef solve(numbers, symbols, neighbors):\n    gear = \"*\"\n    counted = set()\n    part1 = part2 = 0\n\n    for coord, symbol in symbols.items():\n        gear_neighbors = set()\n        for neighbor in neighbors(coord):\n            if neighbor in numbers:\n                number = numbers[neighbor]\n                if neighbor not in counted:\n                    part1 += number.val\n                    counted.update(number.coords)\n                if symbol == gear:\n                    gear_neighbors.add(frozenset(number.coords))\n        if len(gear_neighbors) == 2:\n            part2 += prod(numbers[next(iter(c))].val for c in gear_neighbors)\n\n    return part1, part2\nThe difficulty is higher than usual, but manageable - so far."
  },
  {
    "objectID": "posts/aoc-2023-1/index.html",
    "href": "posts/aoc-2023-1/index.html",
    "title": "Advent Time Again",
    "section": "",
    "text": "It’s Advent of Code time again!\nOnce again, thousands of programmers around the world are rearranging their lives to solve a series of Christmas-themed puzzles. As I write this, only one puzzle has been released, but this already promises to be an interesting year.\nThis year, I’ll write a brief post explaining how I solved each puzzle. We start with day 1.\nThis year’s first puzzle was unexpectedly hard. Part 1 is a simple string manipulation problem. The input consists of a file where each line has lowercase letters and single digits. You just have to read the first and last integers on each line of a file, combine them into a string, read the resulting integer, and sum them.\n```{python}\nimport re\n\nfrom utils.utils import split_lines\n\nraw = split_lines(\"inputs/day1.txt\")\n# No zeroes\nnaturals = r\"[1-9]\"\nstripped = [re.sub(\"[a-z]+\", \"\", line) for line in raw]\npart1 = sum(int(line[0] + line[-1]) for line in stripped)\nprint(part1)\n```\nPart 2 reveals a nasty surprise. You no longer have to just search for digits, but written numbers. one and 1 now both mean 1. I created an awkward regular expression to handle this, then confidently submitted my answer. Wrong. I checked my code on the provided test case, and it was correct.\nThis happens to me several times each year, but rarely on day 1! I racked my mind for a pathological edge case that would break my code. Immediately, I thought of overlapping matches. By the rules, a string like eightwo should be parsed as 82, but a conventional pattern would match eight, move on to w, and fail to match any more substrings from the string. I needed an overlapping pattern to get the correct answer.\nAs with most problems, the answer to this one turned out to lie in a purple StackOverflow link. I tweaked my pattern, reran my code, and submitted a new, slightly higher answer. Correct!\n```{python}\nnums = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\nn = len(nums)\nparts = \"|\".join(nums + [naturals])\npattern = f\"(?=({parts}))\"\nchars = list(map(str, range(1, 1 + n)))\nkeys = dict(zip(nums, chars))\n\nmatches = [re.findall(pattern, line) for line in raw]\npart2 = sum(\n    int(keys.get(r[0], r[0]) + keys.get(r[-1], r[-1])) if r else 0 for r in matches\n)\nprint(part2)\n```\nAll in all, it was a typical Day 7 or 8 puzzle, which made it a nasty surprise as a Day 1 puzzle. So Day 2 should be easy by comparison - right?"
  },
  {
    "objectID": "posts/aoc-2023-5/index.html",
    "href": "posts/aoc-2023-5/index.html",
    "title": "Advent of Code 2023 Day 5",
    "section": "",
    "text": "Today saw a harsh difficulty spike. The puzzle described a series of integer intervals (e.g., [2, 5]) that each shifted input values by a constant if they fell in the interval, but otherwise left them alone. The input consisted of several layers of intervals. Part 1 the smallest output value possible from a few input values. Since there were only a few inputs, each could be manually checked by computing the interval transformations.\nPart 2 “revealed” that the input values were actually intervals. Now a naive solution would have billions of values to check. I instead reversed the interval data and checked each possible output value. Mercifully, the answer ended up being less than 10 million, so my brute-force approach took only a few minutes.\n(I’ll spare you the class that actually does the interval checking.)\n\ndef solve_part2(data, seeds):\n    current = 0\n    mappings = Mappings(data, [], True)\n    zipped = list(zip(*seeds))\n    lower = min(zipped[0])\n    upper = max(zipped[1])\n\n    while True:\n        result = mappings.verify_seed(current)\n        for number in result:\n            if not (lower &lt;= number &lt;= upper):\n                continue\n            for rnge in seeds:\n                if rnge[0] &lt;= number &lt;= rnge[1]:\n                    return current\n        current += 1\nI found out later that the “smart” way of solving it relied on finicky interval splitting. All in all, a wakeup call."
  },
  {
    "objectID": "posts/aoc-2023-6/index.html",
    "href": "posts/aoc-2023-6/index.html",
    "title": "Advent of Code 2023 Day 6",
    "section": "",
    "text": "Another breather day!\nThis puzzle describes a boat race. To win, you must get the boat at least \\(d\\) distance within \\(t\\) seconds. You power the boat by pressing a button for \\(b\\) seconds; the boat then travels at \\(b\\) units per second for the remaining \\(b-t\\) seconds.\nI saw immediately that this could be represented in a quadratic equation \\(-b^2 + bt - d = 0\\), where \\(t\\) is time, \\(d\\) is distance, and \\(b\\) is the length of the button press. All I had to do was find the roots of this equation, compute the lowest and highest integers in that interval, and subtract.\nBrute force probably would have worked, but math is so much more satisfying.\nlocal quadratic = function(a, b, c)\n    local discrim = math.sqrt((b ^ 2) - (4 * a * c))\n    local denom = 2 * a\n    return { (-b + discrim) / denom, (-b - discrim) / denom }\nend\n\nlocal intercepts = function(time, dist)\n    return quadratic(-1, time, -dist)\nend\n\nlocal num_bool = function(e)\n    return (e and 1) or 0\nend\n\nlocal solve = function(data)\n    local result = 1\n    for i, _ in ipairs(data.time) do\n        local points = intercepts(data.time[i], data.distance[i])\n        -- Need to handle case where intercepts are integers and therefore invalid\n        local left_i = math.min(points[1], points[2])\n        local left = math.max(1, math.ceil(left_i + num_bool(left_i % 1 == 0)))\n        local right_i = math.max(points[1], points[2])\n        local right = math.min(\n            math.floor(right_i - num_bool(right_i % 1 == 0)),\n            data.time[i] - 1\n        )\n        result = result * (right - left + 1)\n    end\n    return result"
  },
  {
    "objectID": "posts/aoc-2023-7/index.html",
    "href": "posts/aoc-2023-7/index.html",
    "title": "Advent of Code 2023 Day 7",
    "section": "",
    "text": "An easy-ish day, suprisingly. The puzzle asks you to simulate hands in a simplified version of poker. Hands are ranked by card composition (one pair, two pair, three of a kind, etc.). Ties are broken by directly comparing card ranks.\nPart 1 simply asked to rank a series of hands. While the details were fiddly, it wasn’t too hard to write a class to do so:\nclass Hand:\n    joker = \"J\"\n    ranks = {\n        (5,): 7,\n        (4, 1): 6,\n        (3, 2): 5,\n        (3, 1, 1): 4,\n        (2, 2, 1): 3,\n        (2, 1, 1, 1): 2,\n        (1, 1, 1, 1, 1): 1,\n    }\n    n_cards = 5\n\n    def __init__(self, cards, suits, joker=False):\n        # if joker:\n        #     breakpoint()\n        self.cards = tuple(suits[c] for c in cards)\n        self._count = Counter(cards)\n        self.rank = self.ranks[tuple(sorted(self._count.values(), reverse=True))]\nPart 2 “reveals” that the \"J\" cards are actually jokers, not jacks. For purposes of ranking hands, jokers can assume whichever card value results in the greatest rank. When comparing hands of the same rank, they have the lowest card value.\nBrute-force would have sufficed to find the best joker substitutions, but I thought of a more elegant way. Given the hand type and number of jokers, the best possible hand value after the substitution could always be computed. For example, a two pair with a pair of jokers converts into a four of a kind by replacing the jokers with the card used in the other pair:\nif joker and (jokers := self._count[self.joker]) &gt; 0:\n    match (self.rank, jokers):\n        case (1, 1):\n            self.rank = 2\n        case (2, 1) | (2, 2):\n            self.rank = 4\n        # two pair\n        case (3, 1):\n            self.rank = 5\n        case (3, 2):\n            self.rank = 6\n        # should be impossible\n        case (3, 3):\n            print(\"check\")\n            self.rank = 7\n        # 3 of a kind\n        case (4, 1) | (4, 3):\n            self.rank = 6\n        # 4 of a kind, full house\n        case (5, _) | (6, _):\n            self.rank = 7\n        case _:\n            pass"
  },
  {
    "objectID": "posts/aoc-2023-8/index.html",
    "href": "posts/aoc-2023-8/index.html",
    "title": "Advent of Code 2023 Day 8",
    "section": "",
    "text": "At last Advent of Code offers its starkest choice: math or pain. Today could be solved in a split second using the right formula, or in weeks with brute force.\nThe input consists of a graph with nodes, plus instrucitons to move left or right. Each step moves to the node in the direction indicated on by the current instruction index. After each step, the index is advanced 1, wrapping around where needed.\nPart 1 simply asks for the distance from the start node to the \"ZZZ\" node. This is easy to compute. Part 2 “establishes” that you actually track the movements of several ghosts. You have to find the fewest steps until the time that each ghost is in a node ending in \"Z\". (This ghost-convergence stuff was obviously inspired by Pac-Man).\nI guessed that brute force would be unworkable, but a mathematical shortcut would solve the puzzle quickly. Since the ghosts were independent, I hit upon computing each cycle length and computing the least common multiple. I implemented the classic algorithm to do this, and it worked.\nconst fs = require('fs');\n\nfunction parse_lines(lines) { \n    let result = {};\n    for(line of lines) { \n        let nodes = [...line.matchAll(/[A-Z]{3}/g)];\n        result[nodes[0][0]] = [nodes[1][0], nodes[2][0]];\n    }\n    return result;\n}\n\nfunction find(directions, indices, start, test){ \n    let current = start;\n    let goal = \"ZZZ\";\n    let n_indices = indices.length;\n    let steps = 0;\n    \n    while (!(test(current))){ \n        let choice = indices[steps % n_indices];\n        let next = directions[current][choice];\n        current = next;\n        steps ++;\n    }\n    return steps;\n}\n\nfunction gcd(a, b){ \n    while (b != 0){ \n        let t = b;\n        b = a % b;\n        a = t;\n    }\n    return a;\n}\n\nfunction lcm(a, b){ \n    return Math.abs(a) * (Math.abs(b) / gcd(a, b));\n}\n\nfunction solve_ghost(directions, indices, targets){ \n    let periods = targets.map((x) =&gt; find(directions, indices, x, (x) =&gt; x.substr(2, 3) == \"Z\"))\n    return periods.reduce(lcm);\n}\n\nconst raw_input = fs.readFileSync('inputs/day8.txt', 'utf-8').toString().split(\"\\n\\n\");\nlet indices = raw_input[0].split(\"\").map((x) =&gt; x == \"R\" ? 1 : 0);\nlet directions = parse_lines(raw_input[1].replace(/\\n+$/, \"\").split(\"\\n\"));\nlet start = \"AAA\";\nconst part1 = find(directions, indices, start, (x) =&gt; x == \"ZZZ\");\nconsole.log(part1);\n\n\nlet starts = [...Object.keys(directions)].filter((x) =&gt; x.substring(2, 3) == \"A\");\nconst part2 = solve_ghost(directions, indices, starts)\nconsole.log(part2)\nI realized at the last moment that this simple method would only work if each ghost visited only one node in a repeating cycle. I had no idea if every input obeyed that rule, but it seems they do. On the subreddit, some are objecting to\nthis, since a general solution is much, much harder."
  },
  {
    "objectID": "posts/aoc-2023-9/index.html",
    "href": "posts/aoc-2023-9/index.html",
    "title": "Advent of Code 2023 Day 9",
    "section": "",
    "text": "To my surprise, the weekend difficulty spike failed to materialize. Today’s puzzle just requires you to compute successive differences of vectors and add the last term of each iteration. (I mean the operation that takes subtracts from every element the previous one, turning (1, 3, 6) into (2, 3)). Part two requires the process in reverse on the first element of each iteration.\nAfter some tinkering, I got both on a single pass:\n\nimpute &lt;- function(sequence) {\n  modulus &lt;- 1\n  part1 &lt;- sequence[[length(sequence)]]\n  part2 &lt;- sequence[[1]]\n\n  while (any(sequence)) {\n    # To avoid numeric(0)\n    sequence &lt;- diff(sequence)\n    part1 &lt;- part1 + sequence[[length(sequence)]]\n    part2 &lt;- part2 + (sequence[[1]] * (1 - (2 * modulus)))\n    modulus &lt;- (modulus + 1) %% 2\n  }\n  c(part1, part2)\n}\n\nI felt pretty slick until I learned the “slick” way to solve the problem is to use Lagrange interpolation."
  },
  {
    "objectID": "posts/aoc-2023-11/index.html",
    "href": "posts/aoc-2023-11/index.html",
    "title": "Advent of Code Day 11",
    "section": "",
    "text": "Today was a classic “naive solution of part 1 dooms you for part 2” problem. You’re given a grid of galaxies. You have to double each empty column and sum the Manhattan distance between each pair of galaxies.\n...#......\n.......#..\n#.........\n..........\n......#...\n.#........\n.........#\n..........\n.......#..\n#...#.....\nNaturally, I just counted the blank rows and columns between each pair and added the sum to the distance.\nPart 2, predictably, asks you to do this for 1000000 iterations. Solving it is as simple as multiplying the counts of blank rows and columns by 999999. (An\nobvious trap for an off-by-one error).\n\ndistance &lt;- function(pair, empty_rows, empty_cols, iterations = 1) {\n  # browser()\n  pair &lt;- rbind(pair[[1]], pair[[2]])\n  # if (all(sort(pair[, 1]) == c(1, 9))) browser()\n  xes &lt;- sort(pair[, 2])\n  ys &lt;- sort(pair[, 1])\n  sum(abs(pair[1, ] - pair[2, ])) + (iterations * (sum(empty_cols &gt; xes[[1]] & empty_cols &lt; xes[[2]]) + sum(empty_rows &gt; ys[[1]] & empty_rows &lt; ys[[2]])))\n}\n\nsolve &lt;- function(grid, iterations = 1) {\n  pairs &lt;- which(grid, arr.ind = TRUE)\n  rows &lt;- which(rowSums(grid) == 0)\n  cols &lt;- which(colSums(grid) == 0)\n  asplit(pairs, MARGIN = 1) |&gt;\n    combn(m = 2, FUN = \\(x) distance(x, rows, cols, iterations = iterations)) |&gt;\n    sum()\n}"
  },
  {
    "objectID": "posts/aoc-2023-10/index.html",
    "href": "posts/aoc-2023-10/index.html",
    "title": "Advent of Code 2023 Day 10",
    "section": "",
    "text": "After a few days’ delay, I’ve finally solved this vexing puzzle. You are given a large grid strewn with characters - -, |, J, F, 7, and L - that represent pieces of pipe that point in different directions. The pipe contains a large loop that starts at a given point. Part 1 simply asks for the size of the loop.\nPart 2 “reveals” that you actually want to find the area enclosed by the pipe loop, including disconnected pipe pieces lying around. Just one problem: it’s possible to squeeze between pipes, (e.g., two parallel lines of pipe) meaning an area completely surrounded by pipe tiles might be accessible.\nI struggled to deal with this before finding the obvious solution: convert each grid square into nine, so squeezing between pipes can be directly modeled. Then use flood-fill to find all coordinates enclosed by the grid, convert them back to the original coordinates by integer-dividing by 3, then count them.\nAfter more debugging than I would like, I got it right:\n\ndef flood_fill(grid, xmin, xmax, ymin, ymax, get_neighbors, traversed):\n    exposed = set()\n    enclosed = set()\n    for coord, el in grid.items():\n        if (el and downscale(el, 3) not in traversed) or (coord in enclosed or coord in exposed):\n            continue\n        queue = deque()\n        queue.append(coord)\n        visited = set()\n        open = False\n\n        while queue:\n            new = queue.pop()\n            visited.add(new)\n            open = open or (\n                new in exposed\n                or (\n                    new.real == xmin\n                    or new.real == xmax\n                    or new.imag == ymin\n                    or new.imag == ymax\n                )\n            )\n            new_neighbors = get_neighbors(new)\n\n            for neighbor in new_neighbors:\n                # if neighbor in exposed:\n                #     open = True\n                if not ((grid[neighbor] and downscale(neighbor, 3) in traversed) or neighbor in visited):\n                    queue.append(neighbor)\n        if open:\n            exposed.update(visited)\n        else:\n            enclosed.update(visited)\n    return enclosed"
  },
  {
    "objectID": "posts/aoc-2023-13/index.html",
    "href": "posts/aoc-2023-13/index.html",
    "title": "Advent of Code 2023 Day 13",
    "section": "",
    "text": "Today brought another breather puzzle. You’re given a series of matrices like this:\n#.##..##.\n..#.##.#.\n##......#\n##......#\n..#.##.#.\n..##..##.\n#.#.##.#.\n\n#...##..#\n#....#..#\n..##..###\n#####.##.\n#####.##.\n..##..###\n#....#..#\nYou have to find the vertical and horizontal line of symmetry in each. In the first example above, the vertical line is after the fourth column. Part two adds the twist that each matrix has a hidden line of symmetry, one that holds if just one element is altered. This is easy to compute by converting the character matrices to numeric, adding the reflected elements and checking if the sum is one."
  },
  {
    "objectID": "posts/aoc-2023-14/index.html",
    "href": "posts/aoc-2023-14/index.html",
    "title": "Advent of Code 2023 Day 14",
    "section": "",
    "text": "So much for the breather. Today asks you to simulate the motion of round rocks in a grid. Each round rock should move upward until it hits another rock or the edge of the space. Part 1 is a little fiddly, but simple enough to solve using a dict of complex numbers to represent coordinates, with an enum type for the different types of rock.\nPart 2 asks you to simulate a full “cycle” of rock movements - up, left, down, then right - 1000000000 times. When Advent of Code asks you to simulate something a billion times, it’s really asking you to simulate a few times until you detect a cycle, then do the arithmetic to determine the outcome after a billion iterations. I messed up some of the fiddly math at first, but got it eventually:\n\ndef total_load(grid, offset):\n    grid = dict(grid)\n    rounded = Coordinate.ROUND\n    closed = {rounded, Coordinate.CUBE}\n    load = 0\n    keys = {\n        1j: lambda x: (-x.imag, x.real),\n        -1: lambda x: (x.real, x.imag),\n        -1j: lambda x: (x.imag, x.real),\n        1: lambda x: (-x.real, x.imag),\n    }\n\n    for coord in sorted(grid.keys(), key=keys[offset]):\n        el = grid[coord]\n\n        if el == rounded:\n            current = coord\n            grid[current] = Coordinate.OPEN\n            while True:\n                new = current + offset\n                if new not in grid or grid[new] in closed:\n                    grid[current] = rounded\n                    load += current.imag + 1\n                    break\n                current = new\n\n    return grid, int(load)\n\n\ndef predict(grid):\n    results = OrderedDict()\n    offsets = (1j, -1, -1j, 1)\n    load = 0\n\n    while True:\n        for offset in offsets:\n            grid, load = total_load(grid, offset)\n        hash = frozenset(zip(grid.keys(), grid.values()))\n        if hash in results:\n            print(load)\n            return results, tuple(i for i, k in enumerate(results.keys()) if k == hash)\n        results[hash] = load\n\n2022 day 17 was a harder version of this puzzle. And day 17 is in 3 days - on a weekend, no less…"
  },
  {
    "objectID": "posts/aoc-2023-15/index.html",
    "href": "posts/aoc-2023-15/index.html",
    "title": "Advent of Code 2023 Day 10",
    "section": "",
    "text": "The long-awaited difficulty spike probably falls tomorrow. Today was simple enough. The input is a comma-separated list like this:\nrn=1,cm-,qp=3,cm=2,qp-,pc=4,ot=9,ab=5,pc-,pc=6,ot=7\nPart1 simply asks you to evaluate each token with a custom hash function (called HASH, naturally). Part 2 reveals that each token is actually an instruction. There are 256 boxes, each of which can contain several labeled lenses. For each step, calling HASH on the label gives the index of the target box.\n{label}={num} means to replace the lens in the target box with {label} with one with focal length {num}. {label}- means to remove the lens with {label} from the target box. Once done, you apply a formula to a lens arrangement to get the answer.\nExecuting these instructions in code proved fiddly but straightforward.\nfunction arrange(lenses){ \n    let part1 = 0;\n    let length = 256;\n    let boxes =Array.from(Array(length), () =&gt; []); \n    let pattern = /([a-z]+)(-|=)(\\d+)?/\n\n    for(lens of lenses){ \n        part1 += HASH(lens);\n        let parts = lens.match(pattern);\n        let label = parts[1];\n        let box = HASH(label);\n\n        if (parts[2] == \"-\"){ \n            for(let i = 0; i &lt; boxes[box].length; i++){\n                //Do nothing if no lens with label in box\n                if (boxes[box][i][0] == label){ \n                    boxes[box].splice(i, 1);\n                }\n            }\n        }else{ \n            let done = false;\n            let item = [label, parseInt(parts[3])];\n            for(let i = 0; i &lt; boxes[box].length; i++){\n                //Do nothing if no lens with label in box\n                if (boxes[box][i][0] == label){ \n                    boxes[box][i] = item;\n                    done = true;\n                    break\n                }\n            }\n            if (!done){ \n                boxes[box].push(item);\n            }\n        }\n    }\n\n    return [part1, focusing_power(boxes)];\n}"
  },
  {
    "objectID": "posts/aoc-2023-16/index.html",
    "href": "posts/aoc-2023-16/index.html",
    "title": "Advent of Code 2023 Day 10",
    "section": "",
    "text": "Today’s puzzle, like the final fight of Enter the Dragon, takes place in a hall of mirrors. Using the lenses from yesterday, you shine a beam of light into the hall. Part 1 asks you to track how many tiles it traverses as it bounces off mirrors. Mirrors either redirect (/, \\) or split (|,-) the beam.\nSimulating the beam proved tricky but not truly difficult. Part 2 asks you to shine a beam from all 440 edge tiles of the input and find the greatest number of illuminated tiles. This took a long time; I should go back and optimize my solution. Nonetheless, the real difficulty spike has yet to come.\ndef simulate_beam(grid, start, initial):\n    # coord, direction\n    current = beam_dict()\n    current[start].add(initial)\n    traversed = set()\n    seen = set()\n\n    while current:\n        new = beam_dict()\n        for coord, directions in current.items():\n            for direction in directions:\n                new_coord = coord + direction\n                # Moving off grid\n                if new_coord not in grid:\n                    continue\n                traversed.add(new_coord)\n                new_directions = grid[new_coord][direction]\n\n                # Add to count of beam in relevant direction on tile if it exists, otherwise create new beam\n                new[new_coord].update(new_directions)\n        hashed = frozenset(\n            (coord, frozenset(d)) \n            for coord, d in new.items()\n        )\n        # print(hashed)\n        if hashed in seen:\n            break\n        seen.add(hashed)\n        current = new\n\n    return len(traversed)"
  },
  {
    "objectID": "posts/aoc-2023-18/index.html",
    "href": "posts/aoc-2023-18/index.html",
    "title": "Advent of Code 2023 Day 10",
    "section": "",
    "text": "Today sees the return of polygon area, a topic already covered in day 10. This time, the input consists of a series of instructions for digging a trench that encloses an area. Part 1 asks you to compute the total area. I tried using the shoelace formula but couldn’t get it to work. Like a lamb to the slaughter, I solved it instead using flood-fill.\nPart 2, of course, reveals that you misunderstood the instructions; in reality, the total area should be in the hundreds of trillions, not the tens of thousands. (Why is the true meaning of the elves’ instructions never to solve the problem on a smaller input than part 1)? Flood-fill would be beyond hopeless.\nI asked for help on Discord, and someone quickly pointed me to Pick’s theorem, an astoundingly simple formula to compute the area of an arbitrary polygon with integer coordinates. Using this technique, the answer came easily.\n\ndef shoelace(vertices):\n    return 0.5 * sum(\n        (left.real * right.imag) - (left.imag * right.real)\n        for left, right in zip(vertices, vertices[1:] + [vertices[0]])\n    )\n\n# Pick's theorem\ndef pick(interior, border):\n    return interior + border // 2 + 1\n\ndef area(border, vertices):\n    return int(pick(shoelace(vertices), border))"
  },
  {
    "objectID": "posts/aoc-2023-19/index.html",
    "href": "posts/aoc-2023-19/index.html",
    "title": "Advent of Code 2023 Day 19",
    "section": "",
    "text": "The last few days were brutal, so I’ve resorted to posting out of sequence as I complete them.\nToday’s puzzle, as ever, starts unassumingly. For part 1, you are given a number of records like this:\n{x=787,m=2655,a=1222,s=2876}\nYou also receive a table of rules, each of which assigns a record to a new rule, accepts it (A), or rejects it (R) based on application of Boolean tests:\npx{a&lt;2006:qkq,m&gt;2090:A,rfg}\nWriting a program to solve part 1 is easy enough. Part 2, as you should have learned to expect by now, massively increases the search space. Now, each field can take any value from 1 to 4000 inclusive, and you have to compute how many of the 4000^4 combinations are valid according to the rules.\nBrute force, since this is past day 15, is hopeless. Instead, you can use depth-first search to follow each possible path through the rules and record each valid combination of ranges. That leaves the problem of aggregating them. I struggled with this for awhile before I found a hint on the subreddit that each rule partitioned the space of acceptable intervals, so all the valid states were disjoint. That meant I could just compute the combinations for each record and add them together.\ndef dfs(workflows):\n    start = {f: Interval(1, 4000) for f in FIELDS}\n    valid = []\n\n    def inner(state, workflow):\n        state = dict(state)\n        workflow = workflows[workflow]\n\n        for rule in workflow:\n            if type(rule) == dict:\n                field = rule[\"field\"]\n                # Remember &lt; x means x-1 is max value\n                new = state[field].narrow(rule[\"value\"], rule[\"operator\"])\n                if new.valid:\n                    new_state = state | ({field: new})\n                    target = rule[\"target\"]\n                    # If invalid, reject here\n                    if target == \"A\":\n                        valid.append(new_state)\n                    elif target != \"R\":\n                        inner(new_state, target)\n\n                # Exclude any state caught by rule\n                if rule[\"operator\"] == \"&gt;\":\n                    state[field] = Interval(state[field].low, rule[\"value\"])\n                else:\n                    # &lt; case\n                    state[field] = Interval(rule[\"value\"], state[field].high)\n\n                if not state[field].valid:\n                    return\n            # Catchall rule, so break\n            elif rule == \"A\":\n                valid.append(state)\n                return\n            elif rule == \"R\":\n                return\n            else:\n                inner(state, rule)\n\n    inner(start, \"in\")\n    return sum(prod(r.high - r.low + 1 for r in v.values()) for v in valid)"
  },
  {
    "objectID": "posts/aoc-2023-12/index.html",
    "href": "posts/aoc-2023-12/index.html",
    "title": "Advent of Code 2023 Day 12",
    "section": "",
    "text": "The catch-up grinds on. Every year there’s one moderately hard puzzle I just don’t get, and this year’s was day 12. The puzzle involves validating sequences of characters. Consider this line:\n???.### 1,1,3\nThe numbers on the right mean there must be groups of 1, 1, and 3 # characters, separated by one or more .. Each ? can stand for either a . or a #. The challenge is to count how many arrangements in each line match the group lengths.\nI decided to try each possible permutation and validate it with a regex. None of the input lines were much longer than 15 characters, so an O(2^n) solution would be fine. Sure enough, it was.\nPart 2 revealed I had swallowed the bait. It asked the same counting question, but quintupled the length of each line.\nAs ever in Advent of Code, you shouldn’t count each distinct instance of something when you can group similar ones together. I wrote a function to use BFS to explore valid sequences, storing each as just the index and size of the current group being traversed.\n```{python}\n# .\n@cache\ndef decide_dot(group_index, group_size, n_groups, target_length):\n    # Not in group\n    if group_size == 0:\n        return (group_index, group_size)\n    # Exiting group\n    elif group_size == target_length:\n        return (group_index + 1, 0)\n\n\n# #\n@cache\ndef decide_hash(group_index, group_size, n_groups, target_length):\n    # Starting group\n    if group_size == 0 and group_index &lt; n_groups:\n        return (group_index, 1)\n    # Advance current group\n    elif group_size &lt; target_length:\n        return (group_index, group_size + 1)\n\n\ndef parse(line):\n    parts = line.split(\" \")\n    parts[1] = list_map(parts[1].split(\",\"), int)\n    return parts\n\n\ndef bfs(string, groups):\n    # group_index, group_size\n    start = (\n        0,\n        0,\n    )\n    n_groups = len(groups)\n    reference = groups + [0]\n    last = {start: 1}\n    choices = {\".\": (decide_dot,), \"#\": (decide_hash,), \"?\": (decide_dot, decide_hash)}\n\n    # Dict of states to add to count?\n    for char in string:\n        next = defaultdict(lambda: 0)\n        choosers = choices[char]\n\n        for state in list(last.keys()):\n            count = last.pop(state)\n            group_index, group_size = state\n            target_length = reference[group_index]\n\n            for chooser in choosers:\n                result = chooser(group_index, group_size, n_groups, target_length)\n                if result is not None:\n                    next[result] += count\n        last = next\n    correct = {(n_groups, 0), (n_groups - 1, reference[-2])}\n    return sum(v for k, v in last.items() if k in correct)\n\n\n# Multiply by recursive call to remaining string\ndef reparse(data, n=5):\n    return [[\"?\".join([l[0]] * n), l[1] * n] for l in data]\n\n\nraw = split_lines(\"inputs/day12.txt\")\nparsed = list_map(raw, parse)\npart1 = sum(bfs(line, nums) for line, nums in parsed)\nprint(part1)\n\nnew_parsed = reparse(parsed)\npart2 = sum(bfs(line, nums) for line, nums in new_parsed)\nprint(part2)\n```"
  },
  {
    "objectID": "posts/aoc-2023-23/index.html",
    "href": "posts/aoc-2023-23/index.html",
    "title": "Advent of Code 2023 Day 23",
    "section": "",
    "text": "Every year, there’s a puzzle past day 20 with an easy part 1 and brutal part 2. I always feel a vague sense of dread while solving part 1, which makes them psychologically interesting.\nThis year, it was day 23. The puzzle asks you to find the longest path through a maze that does not visit any tile twice. Some tiles are marked with a steep slope that limits travel (e.g., &gt; means you can only go right on that tile). Brute-forcing was pretty easy.\nPart 2, of course, changes that. Now you can go in any direction on sloped tiles. The astute among you have already noted that the longest path in a graph with cycles is an NP-hard problem. That meant brute force was hopeless.\nI wasted a lot of time with alternative approaches before I found a guide that recommended reducing the maze to a graph of intersections separated by distances. Once given the approach, implementing it was easy.\n```{python}\ndef node_paths(start, neighbors, targets):\n    queue = deque([[start]])\n\n    while queue:\n        current = queue.pop()\n        current_coord = current[-1]\n\n        for neighbor in neighbors(current_coord):\n            if neighbor not in current:\n                # New path found to node, so end here\n                if neighbor in targets and neighbor != current_coord:\n                    # Longer path found\n                    yield start, neighbor, len(current)\n                else:\n                    queue.append(current + [neighbor])\n\n\ndef reduce_graph(graph, start, goal, neighbors):\n    targets = {c for c  in graph.keys() if len(neighbors(c)) &gt; 2} | {start, goal}\n    result = defaultdict(dict)\n    for target in targets:\n        gen = node_paths(target, neighbors, targets)\n        # Exhaust results from generator\n        while True:\n            try:\n                this = next(gen)\n                if this is not None:\n                    start, dest, length = this\n                    result[start][dest] = max(result[start].get(dest, -inf), length)\n                    result[dest][start] = max(result[dest].get(start, -inf), length)\n            except StopIteration:\n                break\n    return result\n\n\ndef solve(graph, start, goal):\n    queue = deque([(set(), start,  0)])\n    result = -inf\n\n    while queue:\n        current, current_coord, current_dist = queue.pop()\n        if current_coord == goal:\n            result = max(result, current_dist)\n        else:\n            neighbors = graph[current_coord]\n            for neighbor, dist in neighbors.items():\n                if neighbor not in current:\n                    new_dist = current_dist + dist\n                    queue.append((current | {current_coord}, neighbor, new_dist))\n    return result\n```"
  },
  {
    "objectID": "posts/aoc-2023-20/index.html",
    "href": "posts/aoc-2023-20/index.html",
    "title": "Advent of Code 2023 Day 20",
    "section": "",
    "text": "Time for an esoteric one. Day 20 has you model the behavior of a circuit board. A broadcaster sends a low pulse along a wire toward several modules. Each is either a flip-flop (which toggle between off and on when they receive a low pulse) and conjunction modules (which track the most recent type of pulse received in each of their inputs, and sends a low pulse if all are high and a high pulse if all are low).\nPart 1 asks you to multiply the total numbers of low and high pulses sent after 1000 pulses are broadcast. This takes some tricky programming to model the network, but nothing too demanding.\nPart 2 instead asks how many pulses it will take before a single low pulse is sent to module rx. As you might expect, a general solution is hopeless; the answer is too big. Instead, as the subreddit revealed, the input contains several hidden bit counters. By examining the structure of each, you can read binary numbers equal to their period lengths. Multiplying these gives the answer.\n```{python}\nfrom collections import defaultdict\nfrom collections import deque\nfrom math import prod\n\nfrom utils.utils import split_lines\n\nclass Module():\n\n    def __init__(self, name, inputs, outputs):\n        self.name = name\n        self.inputs = inputs\n        self.outputs = outputs\n\nclass Broadcaster(Module):\n\n    def receive(self, _, pulse):\n        return pulse\n\nclass FlipFlop(Module):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.on = False\n\n    def receive(self, _,  pulse):\n        if pulse:\n            return\n        self.on = not self.on\n        return self.on\n\nclass Conjunction(Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.inputs = {i : False for i in self.inputs}\n        self.n_inputs = len(self.inputs)\n\n    def receive(self, name, pulse):\n        self.inputs[name] = pulse\n        return sum(self.inputs.values()) != self.n_inputs\n\n    def __repr__(self):\n        return str(self.inputs)\n\ndef parse(lines):\n    inputs = defaultdict(list)\n    data = []\n    end = None\n    for line in lines:\n        name, outputs = line.split(\" -&gt; \")\n        kind = FlipFlop if name[0] == \"%\" else Conjunction if name[0] == \"&\" else Broadcaster\n        name = name.lstrip(\"&%\")\n        outputs = outputs.split(\", \")\n        if outputs == [\"rx\"]:\n            end = name\n        for o in outputs:\n            inputs[o].append(name)\n        data.append((name, kind , outputs))\n    return {t[0] : t[1](t[0], inputs[t[0]], t[2]) for t in data}, end\n\ndef pulse(data, iterations):\n    low = iterations\n    high = 0\n\n    for _ in range(iterations):\n        # Only one set of pulses in queue at once\n        queue = deque([ (\"broadcaster\", \"\", False) ])\n        while queue:\n            new = deque()\n            while queue:\n                target, source, pulse = queue.popleft()\n                outputs = data[target].outputs\n                kind = data[target].receive(source, pulse)\n                if kind is not None:\n                    sent = len(outputs)\n                    if kind:\n                        high += sent\n                    else:\n                        low += sent\n\n                    for o in outputs:\n                        if o not in data:\n                            continue\n                        if o not in new:\n                            new.append((o, target,  kind))\n            queue = new\n    return low, high\n\ndef find_targets(data, end):\n    result = []\n    for t in  data[end].inputs.keys():\n        result.append(next(iter(data[t].inputs.keys())))\n    return result\n\ndef read_number(start, ends, data):\n    num = \"\"\n    current = start\n    while current is not None :\n        digit = \"0\"\n        next = None\n        for o in data[current].outputs:\n            if o in ends:\n                digit = \"1\"\n            else:\n                next = o\n        num += digit\n        current = next\n\n    return int(num[-1::-1], 2)\n\nraw_input = split_lines(\"inputs/day20.txt\")\ndata, end = parse(raw_input)\nlow, high = pulse(data, 1000)\nprint(low * high)\n\ntargets = set(find_targets(data, end))\nnums = [ read_number(num, targets, data) for num in data[\"broadcaster\"].outputs]\npart2 = prod(nums)\nprint(part2)\n```"
  },
  {
    "objectID": "posts/aoc-2023-21/index.html",
    "href": "posts/aoc-2023-21/index.html",
    "title": "Advent of Code 2023 Day 21",
    "section": "",
    "text": "We come at last to perhaps the hardest puzzle of the year. Part 1 presents a map with passable and impassible tiles. You have to calculate how many tiles can be reached by some path in exactly 64 steps. This can be achieved with simple BFS.\nPart 2 asks the same thing, but for 26501365 steps instead. Brute force is beyond hopeless, so what can be done? I spent a lot of time trying to compute the area geometrically, which can be done because the area of reachable tiles expands in a diamond pattern. I couldn’t get it to work. More research revealed a better way: compute the area for a few values, then use Lagrange interpolation to derive a quadratic function giving the number of reachable tiles in \\(n\\) steps. That meant I had to implement the algorithm myself, which proved a fun challenge.\n```{python}\nfrom collections import defaultdict\nimport heapq\nfrom math import inf\n\nimport utils.utils as ut\n\n\nclass ComparableComplex(complex):\n\n    def __lt__(self, other):\n        return abs(self) &gt; abs(other)\n\n# Standard quadratic approximation\ndef lagrange(X, Y):\n    degree = 3\n    denoms = ((X[0] - X[1]) *(X[0] - X[2]), \n              (X[1] - X[0]) * (X[1] - X[2]), \n              (X[2] - X[0]) * (X[2] - X[1])\n              )\n    quadratic = sum(Y[i] / denoms[i] for i in range(degree))\n    linear = (-X[1] - X[2],  -X[0] - X[2] , -X[0] - X[1])\n    linear = sum((Y[i] * linear[i]) /denoms[i] for i in range(degree))\n    constant =(X[1] * X[2] , X[0] * X[2] , X[0] * X[1])\n    constant = sum((Y[i] * constant[i]) /denoms[i] for i in range(degree))\n    return quadratic, linear, constant\n\n\ndef parse(lines):\n    result = {}\n    result.update(\n        {\n            complex(y, x): char == \"#\"\n            for y, line in enumerate(lines)\n            for x, char in enumerate(line)\n        }\n    )\n    return result\n\ndef dijkstra(start, graph, max_dists, neighbors):\n    queue = [(0, ComparableComplex(start))]\n    heapq.heapify(queue)\n    visited = {(0, ComparableComplex(start))}\n    greatest = max(max_dists)\n    result = defaultdict(set)\n    extrema = ut.extrema(graph)\n    xrange = extrema[\"xmax\"] - extrema[\"xmin\"] + 1\n    yrange = extrema[\"ymax\"] - extrema[\"ymin\"] + 1\n\n    while True:\n        try:\n            dist, current = heapq.heappop(queue)\n        except IndexError:\n            return result\n        if dist in max_dists:\n            result[dist].add(current)\n            if dist == greatest:\n                continue\n        dist += 1\n\n        for neighbor in neighbors(current):\n            clamped_neighbor = complex(neighbor.real % xrange, neighbor.imag % yrange)\n            new = (dist, ComparableComplex(neighbor))\n            if not (graph[clamped_neighbor] or new in visited):\n                visited.add(new)\n                heapq.heappush(queue, new)\n\ndef find_start(lines):\n    for y, line in enumerate(lines):\n        for x, char in enumerate(line):\n            if char == \"S\":\n                return complex(x, y)\n\n\nraw_input = ut.split_lines(\"inputs/day21.txt\")\ngraph = parse(raw_input)\nstart = find_start(raw_input)\nextrema = ut.extrema(graph)\nneighbors = ut.neighbors(-inf, inf, -inf, inf,  diag=False, combos=None)\n# Start has to be in dead center for trick to work\nextent = extrema[\"xmax\"] - extrema[\"xmin\"] + 1\nassert (\n    start\n    and start.real == extent // 2\n    and start.imag == (extrema[\"ymax\"] - extrema[\"ymin\"] + 1) // 2\n)\n\nhalf = extent // 2\ndata = dijkstra(start, graph, (half,            half + extent, half + extent * 2), neighbors)\ncoefs = lagrange(range(3), list(map(len, data.values())))\nfinal_dist = 26501365\nrepeats = final_dist // extent\npart2 = int((coefs[0] * (repeats ** 2)) + (coefs[1] * repeats) + coefs[2])\nprint(part2)\n```"
  },
  {
    "objectID": "posts/aoc-2023-22/index.html",
    "href": "posts/aoc-2023-22/index.html",
    "title": "Advent of Code 2023 Day 22",
    "section": "",
    "text": "Time for another 3-D coordinate puzzle. You are given the coordinates of a number of blocks hanging in three-dimensional space. Part 1 asks you to simulate the position of the blocks once they fall to the floor, then compute how many you can “safely” vaporize. A block can be “safely” vaporized so long as any block that rests on it also rests on at least one other block.\nAs is often the case, this task had some finicky implementation challenges, but nothing profoundly difficult.\nPart 2 reveals that it’s time for Tetris! You’re asked to sum, for each block, how many blocks would fall if that block were vaporized. Since there are only a few hundred blocks in total, nothing stops me from just calling the fall function used in part 1 on each block. For once, part 2 ends up easier.\n```{python}\nfrom functools import reduce\nfrom operator import itemgetter\nfrom itertools import chain\n\nfrom utils.utils import split_lines\n\nclass Coord(tuple):\n    def update(self, key, value):\n        return __class__(chain(self[:key], (value,), self[key + 1 :]))\n\n    def __sub__(self, value):\n        return __class__(self[:2] + (self[2] - value,))\n\n    def __isub__(self, value):\n        return self - value\n\n\ndef coord_range(start, end):\n    start, second = sorted([start, end])\n    if start == second:\n        return {start}\n    for field in range(len(start)):\n        if start[field] != end[field]:\n            axis = field\n            break\n    else:\n        raise ValueError\n    # Singleton cube\n\n    offset = start[axis]\n    assert start[axis] &lt;= end[axis]\n    return {start.update(axis, val) for val in range(offset, end[axis] + 1)}\n\ndef get_bricks(bricks):\n    return reduce( set.union, bricks)\n\n\ndef parse(line):\n    parts = line.split(\"~\")\n    return Coord(map(int, parts[0].split(\",\"))), Coord(map(int, parts[1].split(\",\")))\n\ndef lowest_z(coords):\n    return min(coords, key = itemgetter(-1))[-1]\n\n\ndef fall(bricks, contained):\n    result = {}\n\n    bricks = list(sorted(bricks.items(), key = lambda x: lowest_z(x[1])))\n    # Make each brick fall in ascending order\n    fallen = 0\n\n    for pair in bricks:\n        id, brick = pair\n        current= set(brick)\n        contained -= current\n        z_level = lowest_z(brick)\n        done = started = False\n\n        while z_level &gt; 1 and not done:\n            new = set()\n            for coord in current:\n                new_coord = coord - 1\n                if new_coord in contained:\n                    done = True\n                    break\n                new.add(new_coord)\n            else:\n                started = True\n                z_level -= 1\n                current = new\n\n        fallen += started\n        result[id] = current\n        contained |= current\n    return result, fallen\n\ndef count_not_supporting(bricks):\n    mapping = {}\n    alone = set()\n    for id, brick in bricks.items():\n        for coord in brick:\n            mapping[coord] = id\n    for id, brick in bricks.items():\n        below = set()\n        for coord in brick:\n            if coord[2] == 1:\n                break\n            new = coord - 1\n            supporting = mapping.get(new)\n            if supporting is not None and supporting != id:\n                below.add(supporting)\n        if len(below) == 1:\n            alone.update(below)\n    return len(bricks) - len(alone)\n\n\nraw_input = split_lines(\"inputs/day22.txt\")\nparsed = list(map(parse, raw_input))\ncontained = set()\nbricks = {i: coord_range(*line) for i, line in enumerate(parsed)}\ncontained = get_bricks(bricks.values())\nbricks, _ = fall(bricks, contained)\npart1 = count_not_supporting(bricks)\nprint(part1)\n\npart2 = 0\ncontained = get_bricks(bricks.values())\n\nfor id in list(bricks.keys()):\n    removed = bricks[id]\n    bricks.pop(id)\n    part2 += fall(bricks, contained - removed)[1]\n    bricks[id] = removed\nprint(part2)\n```"
  },
  {
    "objectID": "posts/aoc-2023-24/index.html",
    "href": "posts/aoc-2023-24/index.html",
    "title": "Advent of Code 2023 Day 24",
    "section": "",
    "text": "For the penultimate puzzle, we are given coordinates and velocities representing the positions of a few hundred hailstones. Given position vector \\(\\vec p_i\\) and velocity vector \\(\\vec v_i\\),\nthe position of hailstone \\(i\\) at time \\(t\\) can be calculated by \\(\\vec p_{ti} = \\vec {p_i} + t \\vec {v_i}\\).\nPart 1 asks how many of their paths will intersect at some point in the future, ignoring the \\(z\\). This is easy to brute-force by trying to solve for the intersection of every pair of lines and throwing out solutions where \\(t \\leq 0\\).\nPart 2 asks for something vastly harder: the sum of the position coordinates for the equation of a line that intersects all hailstones (not just the lines they trace) at some point in the future. Even assuming integer coordinates, brute force is out of the question. As with most things in life, the best solution comes through complicated linear algebra. This post explains the algorithm I used.\nLinear algebra rarely appears in Advent of Code, perhaps because floating-point error is a serious concern in matrix arithmetic, so it’s good to see it represented.\n\nsqtka find_intersection &lt;- function(pair, low, high) {     A_pos &lt;- pair[[1]][[1]][-3]     A_vel &lt;- pair[[1]][[2]][-3]     B_pos &lt;- pair[[2]][[1]][-3]     B_vel &lt;- pair[[2]][[2]][-3]     left &lt;- -A_vel     b &lt;- A_pos - B_pos     A &lt;- cbind(left, B_vel)      # Beta in OLS     solution &lt;- tryCatch(solve(A, b), error = function(e) {     })     if (is.null(solution) || (solution[[1]] &lt; 0) || (solution[[2]] &lt; 0)) {         return()     }     intersection &lt;- A_pos + A_vel * solution[[1]]     # print(intersection)     if (all((intersection &gt;= low) & (intersection &lt;= high))) {         intersection     } else {         return()     }     # if (!is.null(solution) && !all((A_pos[-3] + A_vel[-3] * solution[[1]]) == (B_pos[-3] + B_vel[-3] * solution[[2]]) ) stop()     # solution &lt;- (solve(t(A) %*% A) %*% t(A)) %*% b     # if (all(A %*% solution) == b) {     #     A_pos[-3] + A_vel[-3] * solution[[1]]     # } }  to_int &lt;- function(string) {     strsplit(string, \",\\\\s?\") |&gt;         unlist() |&gt;         as.numeric() }  parse &lt;- function(line) {     parts &lt;- strsplit(line, \"\\\\s@\\\\s\") |&gt;         unlist() |&gt;         lapply(to_int) }  # Y DX - X DY = x dy - y dx + Y dx + y DX - x DY - X dy # (dy'-dy) X + (dx-dx') Y + (y-y') DX + (x'-x) DY = x' dy' - y' dx' - x dy + y dx # See https://www.reddit.com/r/adventofcode/comments/18q40he/2023_day_24_part_2_a_straightforward_nonsolver/ make_row &lt;- function(pair, first, second) {     c1 &lt;- pair[[1]][[1]][[first]]     d1 &lt;- pair[[1]][[2]][[first]]     c1_prime &lt;- pair[[2]][[1]][[first]]     d1_prime &lt;- pair[[2]][[2]][[first]]      c2 &lt;- pair[[1]][[1]][[second]]     d2 &lt;- pair[[1]][[2]][[second]]     c2_prime &lt;- pair[[2]][[1]][[second]]     d2_prime &lt;- pair[[2]][[2]][[second]]      rhs &lt;- (c1_prime * d2_prime) - (c2_prime * d1_prime) - (c1 * d2) + (c2 * d1)     c(d2_prime - d2, d1 - d1_prime, c2 - c2_prime, c1_prime - c1, rhs) }  full_rank &lt;- function(X) {     tryCatch(         {             solve(t(X) %*% X)             TRUE         },         error = function(e) FALSE     ) }  find_coord &lt;- function(pairs, first, second) {     equations &lt;- c()     # browser()     for (pair in pairs) {         row &lt;- make_row(pair, first, second)         # print(row)         new &lt;- rbind(equations, row)         if (qr(new)[[\"rank\"]] == nrow(new)) {             equations &lt;- new         }         if (length(equations) && nrow(equations) == 4) break     }     # Solve for X, Y, ignore velocities     result &lt;- solve(equations[, -5], equations[, 5])     print(result)     result[1:2] }  solve_part2 &lt;- function(pairs) {     start &lt;- find_coord(pairs, 1, 2)     second &lt;- find_coord(rev(pairs), 1, 3)     c(start[1:2], second[[2]]) }  raw_input &lt;- readLines(\"inputs/day24.txt\") parsed &lt;- lapply(raw_input, parse) lower &lt;- 200000000000000 upper &lt;- 400000000000000 pairs &lt;- combn(parsed, m = 2, FUN = c, simplify = FALSE) part1 &lt;- lapply(pairs, find_intersection, low = lower, high = upper) |&gt;     vapply(Negate(is.null), FUN.VALUE = logical(1)) |&gt;     sum() print(part1) part2 &lt;- solve_part2(pairs) print(sum(part2))"
  },
  {
    "objectID": "posts/aoc-2023-25/index.html",
    "href": "posts/aoc-2023-25/index.html",
    "title": "Advent of Code 2023 Day 25",
    "section": "",
    "text": "The end at last! Day 25 of Advent of Code is traditionally a simple puzzle, since no one wants a tough problem on Christmas morning. This year’s kept with that tradition, though it was a bit tougher than usual.\nThe question is simple: find a way to remove three edges of a graph such that it splits into two components whose degrees make the largest product when multiplied. I foolishly tried brute force, but the graph has several thousand edges; that meant billions of possibilities to check, far too many.\nI found a hint on the subreddit: just start with any node, then scan the remaining nodes and add the one mostly densely connected to nodes already added. Once only three nodes connect the processed nodes to the unprocessed nodes, the problem is solved. I don’t know if this would work on all possible graphs, but it worked well enough on mine.\n```{python}\nfrom collections import defaultdict\nfrom math import inf\n\nfrom utils.utils import split_lines\n\n\ndef parse(lines):\n    result = defaultdict(set)\n    for line in lines:\n        source, dests = line.split(\": \")\n        dests = set(dests.split(\" \"))\n        result[source].update(dests)\n\n        for dest in dests:\n            result[dest].add(source)\n    return result\n\n\n# Algorithm \"borrowed\" from https://www.reddit.com/r/adventofcode/comments/18qbsxs/2023_day_25_solutions/\ndef partition(graph):\n    S = set(graph.keys())\n    n = len(S)\n\n    while True:\n        most = -inf\n        target = None\n        all_edges = set()\n        for node in S:\n            in_other = {n for n in graph[node] if n not in S}\n            found = len(in_other)\n            if found &gt; most:\n                target = node\n                most = found\n            all_edges |= {tuple(sorted((node, dest))) for dest in in_other}\n        if len(all_edges) == 3:\n            return len(S) * (n - len(S))\n        S.remove(target)\n\n\nraw_input = split_lines(\"inputs/day25.txt\")\ngraph = parse(raw_input)\npart1 = partition(graph)\nprint(part1)\n```"
  },
  {
    "objectID": "posts/aoc-2023-17/index.html",
    "href": "posts/aoc-2023-17/index.html",
    "title": "Advent of Code 2023 Day 17",
    "section": "",
    "text": "Time for a really tricky graph puzzle. Today’s puzzle involves maneuvering a crucible full of lava through a map. Each tile has a one-digit number indicating how much heat will be lost on entering that block. More interestingly, the crucible can only turn left or right or go straight, and it must turn after going three tiles in the same direction.\nThis amounts to a familiar challenge: graph pathfinding, while keeping track of some kind of state in addition to node position. After some tricky implementation, this part proved straightforward.\nPart 2 introduces “ultra crucibles.” These devices must go straight at least four and at most ten tiles before turning. This constraint makes the graph bigger and the implementation more complicated, but it isn’t too challenging.\n```{python}\nimport heapq\nfrom collections import defaultdict\nfrom functools import cache\nfrom math import inf\nfrom operator import attrgetter\n\nfrom utils.utils import split_lines\n\n\nclass State:\n    def __init__(self, coord, direction, remaining, traversed):\n        self.coord = coord\n        self.direction = direction\n        self.remaining = remaining\n        self.traversed = traversed\n\n    def __lt__(self, other):\n        return self.traversed &lt; other.traversed\n\n    def __hash__(self):\n        return hash((self.coord, self.direction, self.remaining))\n\n    def __repr__(self) -&gt; str:\n        return (self.coord, self.direction, self.remaining, self.traversed).__repr__()\n\n\ndef parse(lines):\n    return {\n        complex(i, j): int(char)\n        for j, line in enumerate(lines)\n        for i, char in enumerate(line)\n    }\n\n\n@cache\ndef manhattan(x, y):\n    return abs(x.real - y.real) + abs(x.imag - y.imag)\n\n\ndef A_star(graph, start, max_straight, min_turn_time):\n    xmin = ymin = 0\n    move_threshold = max_straight - min_turn_time\n    xmax = int(max(graph.keys(), key=attrgetter(\"real\")).real)\n    ymax = int(max(graph.keys(), key=attrgetter(\"imag\")).imag)\n    goal = complex(xmax, ymax)\n\n    dist = defaultdict(lambda: inf)\n    queue = [\n        State(start, 1, max_straight, 0),\n        State(\n            start,\n            1j,\n            max_straight,\n            0,\n        ),\n    ]\n    heapq.heapify(queue)\n    # Tip from Discord\n    answer = max(graph.values()) * (xmax + ymax)\n\n    def verify_coord(coord):\n        return xmin &lt;= coord.real &lt;= xmax and ymin &lt;= coord.imag &lt;= ymax\n\n    def add_neighbor(new_coord, direction, this_traversed, new_remaining):\n        heapq.heappush(\n            queue,\n            State(\n                new_coord,\n                direction,\n                new_remaining,\n                this_traversed ,\n            ),\n        )\n\n    directions = ((-1, 1), (-1j, 1j))\n    while True:\n        try:\n            current = heapq.heappop(queue)\n        except IndexError:\n            break\n        \n        new_coord = current.coord + current.direction\n        if not verify_coord(new_coord):\n            continue\n        this_traversed = current.traversed + graph[new_coord]\n        new_remaining = current.remaining - 1\n        key = (new_coord, current.direction, new_remaining)\n\n        if not (\n            new_remaining &gt; -1\n            and this_traversed &lt; dist[key]\n            and this_traversed &lt; answer\n        ):\n            continue\n        dist[key] = this_traversed\n\n        if new_coord == goal and new_remaining &lt;= move_threshold:\n            answer = this_traversed\n            continue\n\n        if new_remaining &lt;= move_threshold:\n            for dir in directions[bool(current.direction.real)]:\n                add_neighbor(new_coord, dir, this_traversed, max_straight)\n        add_neighbor(new_coord, current.direction, this_traversed, new_remaining)\n\n    return answer\n\n\nraw_input = split_lines(\"inputs/day17.txt\")\ngraph = parse(raw_input)\npart1 = A_star(graph, 0, 3, 0)\nprint(part1)\n\npart2 = A_star(parse(raw_input), 0, 10, 4)\nprint(part2)\n```"
  },
  {
    "objectID": "posts/aoc-2023-wrapup/index.html",
    "href": "posts/aoc-2023-wrapup/index.html",
    "title": "Completing Advent of Code",
    "section": "",
    "text": "Another year of Advent of Code draws to a close. This year, I made a post detailing my solution to each puzzle. Some were simple, some very hard, but all, as ever, were interesting problems.\nFrom the start, this year was harder than the last. Day 1’s puzzle set the stage. Part 1 is a straightforward regex problem, but part 2 requires you to read numbers written as both numerals (1) and words (one). This isn’t hard in itself, but the input contains overlapping matches like oneight. I figured this out quickly, but an inexperienced player might have been stuck for a long time.\nThe early puzzles kept up the difficulty, with several containing special cases to trip up the unwary. Day 10 in particular caused me a lot of trouble. Part 2 requires you to find the area enclosed by the pipes, allowing that you can “squeeze through” pipes (e.g., you can move horizontally along --- but not vertically). This rules out the obvious approach of flood-filling the interior area. I eventually figured out you could expand each tile into nine, marking the tiles blocked by pipes, compute the area, and divide by nine.\nI kept up until around day 20, when the toughest puzzles began to appear. I came back to these one by one in early January, often consulting the subreddit for help. In time, I solved them all.\nMy favorites included day 19 (requiring a tricky form of depth-first search) and day 24 (finally, a puzzle that emphasized matrix algebra). Overall, the puzzles were much tougher than last year’s, in what I suspect was an intentional difficulty spike. Veteran players should be kept on their toes, after all. I wonder what new twists we’ll see next year, which will be the tenth (!) edition of Advent of Code.\nI write this in February, when it’s hard to look forward to next winter. But it’s not all bad. Each November, I find myself wondering how the summer passed so quickly and knowing the cold, drizzly weather will only get colder. But soon I’ll have some puzzles to solve."
  }
]